{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9462ce-5c07-4155-a8fd-1eb09b22a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate, interpolate\n",
    "from scipy import optimize\n",
    "from torch.autograd import grad\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.integrate import solve_ivp, odeint\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3aee7ec-798b-4a29-bc78-1762a0bf764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_sys(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PINN_sys, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size), nn.Tanh()\n",
    "        )\n",
    "        self.params = nn.Parameter(torch.rand(3))  # Learnable parameters\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Training function\n",
    "def train_pinn_sys(model, \n",
    "                   t_train, \n",
    "                   G1_data,\n",
    "                   G2_data,\n",
    "                   P_data,\n",
    "                   D_data,\n",
    "                   zero_init,\n",
    "                   known_param,\n",
    "                   num_epochs, \n",
    "                   learning_rate\n",
    "                   ):\n",
    "    \n",
    "    # Define the loss function (MSE for the PINN)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Define the learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=50, factor=0.5, verbose=True)\n",
    "    \n",
    "    # write the loss and params to a list\n",
    "    epoch_list = []\n",
    "    loss_list = []\n",
    "    fg_list = []\n",
    "    fm_list = []\n",
    "    fp_list = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Convert to PyTorch tensors\n",
    "        t_train = torch.FloatTensor(t_train.reshape(-1, 1))\n",
    "        G1_tensor = torch.FloatTensor(G1_data.reshape(-1, 1))\n",
    "        G2_tensor = torch.FloatTensor(G2_data.reshape(-1, 1))\n",
    "        P_tensor = torch.FloatTensor(P_data.reshape(-1, 1))\n",
    "        D_tensor = torch.FloatTensor(D_data.reshape(-1, 1))\n",
    "        zero_init = torch.FloatTensor(zero_init.reshape(-1, 1))\n",
    "        known_param = torch.FloatTensor(known_param.reshape(-1, 1))\n",
    "        \n",
    "        t_train.requires_grad_()\n",
    "\n",
    "        # Forward pass\n",
    "        NN = model(t_train)\n",
    "        G1, G2, P, D = torch.split(NN, 1, dim=1)\n",
    "        \n",
    "        G1_ones = torch.ones_like(G1)\n",
    "        G2_ones = torch.ones_like(G2)\n",
    "        P_ones = torch.ones_like(P)\n",
    "        D_ones = torch.ones_like(D)\n",
    "\n",
    "        # Compute the gradients of the variables\n",
    "        G1_t = grad(G1, t_train, G1_ones, create_graph=True)[0]\n",
    "        G2_t = grad(G2, t_train, G2_ones, create_graph=True)[0]\n",
    "        P_t = grad(P, t_train, P_ones, create_graph=True)[0]\n",
    "        D_t = grad(D, t_train, D_ones, create_graph=True)[0]\n",
    "        \n",
    "        # ensure the model parameters are positive\n",
    "        param_reg = torch.abs(model.params)\n",
    "        fg = param_reg[0]\n",
    "        fm = param_reg[1]\n",
    "        fp = param_reg[2]\n",
    "        \n",
    "        k1 = known_param[0]\n",
    "        k2 = known_param[1]\n",
    "        k3 = known_param[2]\n",
    "        kd = known_param[3]\n",
    "        kd2 = known_param[4]\n",
    "        cGEM = known_param[5]\n",
    "        \n",
    "        # residual loss\n",
    "        l1 = criterion(G1_t, -k1*(1-fg)*G1 + 2*k2*(1-fm)*G2 - kd*G1)\n",
    "        l2 = criterion(G2_t, k1*(1-fg)*G1 - k2*(1-fm)*G2 - k3*(1-fp)*G2 - kd*cGEM*G2)\n",
    "        l3 = criterion(P_t, k3*(1-fp)*G2 - kd2*P)\n",
    "        l4 = criterion(D_t, kd*G1 + kd*cGEM*G2 + kd2*P)\n",
    "        \n",
    "        # data loss\n",
    "        sum_loss1 = criterion(G1, G1_tensor)\n",
    "        sum_loss2 = criterion(G2, G2_tensor)\n",
    "        sum_loss3 = criterion(P, P_tensor)\n",
    "        sum_loss4 = criterion(D, D_tensor)\n",
    "        \n",
    "        # initial conditions loss\n",
    "        G1_init_loss = criterion(G1[0], zero_init[0])\n",
    "        G2_init_loss = criterion(G2[0], zero_init[1])\n",
    "        P_init_loss = criterion(P[0], zero_init[2])\n",
    "        D_init_loss = criterion(D[0], zero_init[3])\n",
    "        \n",
    "       \n",
    "        # residual loss + data loss + initial conditions loss \n",
    "        total_loss = (l1 + l2 + l3 + l4 + \\\n",
    "                      sum_loss1 + sum_loss2 + sum_loss3 + sum_loss4 +  \\\n",
    "                      G1_init_loss + G2_init_loss + P_init_loss + D_init_loss)       \n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the learning rate based on the validation loss\n",
    "        scheduler.step(total_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            loss_item = round(total_loss.item(), 4)\n",
    "            param_item = param_reg.data.numpy()\n",
    "            epoch_list.append(epoch+1)\n",
    "            loss_list.append(loss_item)\n",
    "            fg_list.append(param_item[0])\n",
    "            fm_list.append(param_item[1])\n",
    "            fp_list.append(param_item[2])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]', \n",
    "                  f'Loss: {total_loss.item():.2e}', \n",
    "                  f'params: {param_reg.data.numpy()}')\n",
    "            \n",
    "        # save list containing loss and params to csv files\n",
    "        loss_param_df = pd.DataFrame(data={\"Num Epoch\": epoch_list,\n",
    "                                           \"loss\": loss_list,\n",
    "                                           \"fg\": fg_list,\n",
    "                                           \"fm\": fm_list,\n",
    "                                           \"fp\": fp_list\n",
    "                                           })\n",
    "        loss_param_df.to_csv(\"./loss_param_1c.csv\", sep=',',index=False)\n",
    "\n",
    "    print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188a9b35-b6cc-4237-8594-235bbdc290b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19.\n",
      " 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37.\n",
      " 38. 39.]\n"
     ]
    }
   ],
   "source": [
    "t = np.linspace(2,39,38)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc9713d-d475-4c8b-957a-70efa26c6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drug_dose_param(file,known_param,t):\n",
    "    #file = pd.read_csv('filename')\n",
    "    \n",
    "    file_D = file.iloc[0,:-1].to_numpy()\n",
    "    file_G1 = file.iloc[1,:-1].to_numpy()\n",
    "    file_G2 = file.iloc[2,:-1].to_numpy()\n",
    "    file_P = file.iloc[3,:-1].to_numpy()\n",
    "    \n",
    "    # scaling the data\n",
    "    max_file_G1 = np.max(file_G1)\n",
    "    max_file_G2 = np.max(file_G2)\n",
    "    max_file_P = np.max(file_P)\n",
    "    max_file_D = np.max(file_D)\n",
    "    \n",
    "    max_file = np.max([max_file_G1, max_file_G2, max_file_P, max_file_D])\n",
    "    ###########################################################################\n",
    "    \n",
    "    t_train = t\n",
    "    file_G1_data = file_G1 / max_file\n",
    "    file_G2_data = file_G2 / max_file\n",
    "    file_P_data = file_P / max_file\n",
    "    file_D_data = file_D / max_file\n",
    "    \n",
    "    # Set initial conditions for all variables\n",
    "    file_zero_init = np.array([file_G1_data[0], file_G2_data[0], file_P_data[0], file_D_data[0]])\n",
    "    \n",
    "    # Create and train the PINN model\n",
    "    input_size = 1; hidden_size = 32; output_size = 4; num_epochs =  6000; learning_rate = 1e-3\n",
    "    \n",
    "    model = PINN_sys(input_size, hidden_size, output_size)\n",
    "    train_pinn_sys(model,\n",
    "                   t_train, file_G1_data, file_G2_data, file_P_data,\n",
    "                   file_D_data, file_zero_init, known_param, num_epochs, learning_rate)\n",
    "    \n",
    "    # Test the trained model\n",
    "    t_test = t\n",
    "    t_test = torch.FloatTensor(t_test.reshape(-1, 1))\n",
    "    file_pred = model(t_test).detach().numpy()\n",
    "    \n",
    "    file_G1_pred = file_pred[:, 0] * max_file\n",
    "    file_G2_pred = file_pred[:, 1] * max_file\n",
    "    file_P_pred = file_pred[:, 2] * max_file\n",
    "    file_D_pred = file_pred[:, 3] * max_file\n",
    "    \n",
    "    return file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85a4fff-17a2-477a-bc65-cec458722a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cGEM_all = np.array([0, 3.125, 6.25, 12.5, 25, 50, 100, 200, 400, 800])\n",
    "\n",
    "kn_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656])\n",
    "kn_param_4N = np.array([0.04754016, 1.06978193, 0.02534762, 0.00800557, 0.00394785])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45d1c9cb-6488-4e73-ba77-9d1b10aa83eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 8.45e-03 params: [0.03093755 0.20537752 0.5282054 ]\n",
      "Epoch [400/6000] Loss: 4.84e-03 params: [2.8183684e-05 4.0129226e-01 7.2683585e-01]\n",
      "Epoch [600/6000] Loss: 4.16e-03 params: [4.3646332e-07 5.1755595e-01 8.5364008e-01]\n",
      "Epoch 00649: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [800/6000] Loss: 4.02e-03 params: [0.00981871 0.54702085 0.893408  ]\n",
      "Epoch [1000/6000] Loss: 3.88e-03 params: [0.03287798 0.56139594 0.91976   ]\n",
      "Epoch [1200/6000] Loss: 3.75e-03 params: [0.06410462 0.5746554  0.9501406 ]\n",
      "Epoch [1400/6000] Loss: 3.63e-03 params: [0.10155251 0.589183   0.98795784]\n",
      "Epoch [1600/6000] Loss: 3.53e-03 params: [0.14474168 0.60559857 1.0344056 ]\n",
      "Epoch [1800/6000] Loss: 3.43e-03 params: [0.19375919 0.6240503  1.0896554 ]\n",
      "Epoch [2000/6000] Loss: 3.34e-03 params: [0.24855456 0.6445395  1.153415  ]\n",
      "Epoch [2200/6000] Loss: 3.62e-03 params: [0.30882907 0.6670125  1.2251472 ]\n",
      "Epoch [2400/6000] Loss: 3.17e-03 params: [0.37425253 0.6911167  1.3041307 ]\n",
      "Epoch [2600/6000] Loss: 3.10e-03 params: [0.44398496 0.7165287  1.3898451 ]\n",
      "Epoch [2800/6000] Loss: 3.04e-03 params: [0.5170846  0.74277437 1.4816269 ]\n",
      "Epoch [3000/6000] Loss: 2.98e-03 params: [0.59187603 0.7694191  1.5789236 ]\n",
      "Epoch 03060: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [3200/6000] Loss: 2.95e-03 params: [0.6412112  0.78677547 1.6448158 ]\n",
      "Epoch [3400/6000] Loss: 2.92e-03 params: [0.6797863 0.8003542 1.6962701]\n",
      "Epoch [3600/6000] Loss: 2.89e-03 params: [0.7188628 0.8140187 1.7485754]\n",
      "Epoch [3800/6000] Loss: 2.87e-03 params: [0.7580877 0.8276455 1.8016278]\n",
      "Epoch [4000/6000] Loss: 2.84e-03 params: [0.7971295  0.84110373 1.8552387 ]\n",
      "Epoch [4200/6000] Loss: 2.82e-03 params: [0.8356627 0.8542678 1.9091841]\n",
      "Epoch [4400/6000] Loss: 2.80e-03 params: [0.87337214 0.8670202  1.9632204 ]\n",
      "Epoch [4600/6000] Loss: 2.78e-03 params: [0.9099359  0.87922734 2.0170543 ]\n",
      "Epoch [4800/6000] Loss: 2.77e-03 params: [0.94506276 0.89079654 2.070421  ]\n",
      "Epoch [5000/6000] Loss: 2.76e-03 params: [0.97849107 0.9016584  2.1230726 ]\n",
      "Epoch [5200/6000] Loss: 2.74e-03 params: [1.0099787 0.9117507 2.1747644]\n",
      "Epoch [5400/6000] Loss: 2.75e-03 params: [1.0393358  0.92103994 2.2252672 ]\n",
      "Epoch [5600/6000] Loss: 2.73e-03 params: [1.0663805  0.92949706 2.27437   ]\n",
      "Epoch [5800/6000] Loss: 2.72e-03 params: [1.0910151 0.9370748 2.3218687]\n",
      "Epoch [6000/6000] Loss: 2.71e-03 params: [1.1131539 0.9437791 2.3675585]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# A_3, cGEM=3.125\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 3.125])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/A_3.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43d3146-2748-4b91-8bf5-0aa3439fe79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=1.1131539\n",
      "fm=0.9437791\n",
      "fp=2.3675585\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7614a67-6478-432f-808a-a9ee1140f87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 6.40e-03 params: [0.04969702 0.39510772 0.93759817]\n",
      "Epoch [400/6000] Loss: 3.39e-03 params: [3.1594345e-06 4.4531733e-01 1.0181094e+00]\n",
      "Epoch [600/6000] Loss: 2.93e-03 params: [1.3170328e-05 4.5039997e-01 1.0603565e+00]\n",
      "Epoch [800/6000] Loss: 2.70e-03 params: [8.9328423e-06 4.4737911e-01 1.0936056e+00]\n",
      "Epoch 00923: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [1000/6000] Loss: 2.58e-03 params: [4.0574432e-06 4.4466433e-01 1.1170423e+00]\n",
      "Epoch [1200/6000] Loss: 2.48e-03 params: [9.5550777e-06 4.4285902e-01 1.1301521e+00]\n",
      "Epoch [1400/6000] Loss: 2.38e-03 params: [1.5437130e-05 4.4097072e-01 1.1424035e+00]\n",
      "Epoch [1600/6000] Loss: 2.29e-03 params: [1.9130648e-05 4.3930656e-01 1.1534941e+00]\n",
      "Epoch [1800/6000] Loss: 2.23e-03 params: [5.2211171e-06 4.3810454e-01 1.1632096e+00]\n",
      "Epoch [2000/6000] Loss: 2.19e-03 params: [4.1340783e-05 4.3712175e-01 1.1712940e+00]\n",
      "Epoch [2200/6000] Loss: 2.28e-03 params: [1.0170250e-04 4.3621662e-01 1.1778432e+00]\n",
      "Epoch 02221: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [2400/6000] Loss: 2.13e-03 params: [1.3544030e-05 4.3610284e-01 1.1811360e+00]\n",
      "Epoch [2600/6000] Loss: 2.11e-03 params: [1.6090144e-05 4.3567732e-01 1.1838595e+00]\n",
      "Epoch [2800/6000] Loss: 2.09e-03 params: [9.9970612e-06 4.3530652e-01 1.1866533e+00]\n",
      "Epoch [3000/6000] Loss: 2.07e-03 params: [1.8103023e-05 4.3499583e-01 1.1895723e+00]\n",
      "Epoch [3200/6000] Loss: 2.06e-03 params: [2.2376549e-05 4.3474424e-01 1.1927075e+00]\n",
      "Epoch [3400/6000] Loss: 2.04e-03 params: [1.8118291e-05 4.3456253e-01 1.1962105e+00]\n",
      "Epoch [3600/6000] Loss: 2.03e-03 params: [5.1407560e-06 4.3454948e-01 1.2002288e+00]\n",
      "Epoch [3800/6000] Loss: 2.01e-03 params: [3.5552512e-05 4.3462899e-01 1.2048213e+00]\n",
      "Epoch [4000/6000] Loss: 2.00e-03 params: [1.1497553e-05 4.3483785e-01 1.2100885e+00]\n",
      "Epoch [4200/6000] Loss: 1.99e-03 params: [1.1899609e-05 4.3509915e-01 1.2160593e+00]\n",
      "Epoch [4400/6000] Loss: 2.19e-03 params: [5.8106380e-06 4.3534735e-01 1.2226657e+00]\n",
      "Epoch [4600/6000] Loss: 1.97e-03 params: [1.0858328e-05 4.3571043e-01 1.2298567e+00]\n",
      "Epoch [4800/6000] Loss: 1.96e-03 params: [1.2961841e-05 4.3619537e-01 1.2374889e+00]\n",
      "Epoch [5000/6000] Loss: 2.49e-03 params: [8.5334519e-05 4.3648568e-01 1.2452918e+00]\n",
      "Epoch [5200/6000] Loss: 1.95e-03 params: [9.2476239e-06 4.3686953e-01 1.2530777e+00]\n",
      "Epoch [5400/6000] Loss: 1.94e-03 params: [1.7587397e-05 4.3726644e-01 1.2607288e+00]\n",
      "Epoch 05499: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [5600/6000] Loss: 1.94e-03 params: [1.12803264e-07 4.37616736e-01 1.26629221e+00]\n",
      "Epoch [5800/6000] Loss: 1.93e-03 params: [7.8172179e-06 4.3768877e-01 1.2699184e+00]\n",
      "Epoch [6000/6000] Loss: 1.93e-03 params: [1.0119130e-05 4.3780208e-01 1.2736831e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# B_3, cGEM=3.125\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 3.125])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/B_3.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9c6e3c5-7509-4a8f-b007-f95cc72ae41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=1.011913e-05\n",
      "fm=0.43780208\n",
      "fp=1.2736831\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec8fd1ec-5328-47bb-98dd-71a463db8bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 9.50e-03 params: [0.3630737  0.28052145 0.43820798]\n",
      "Epoch [400/6000] Loss: 4.76e-03 params: [0.21936528 0.4157651  0.6168723 ]\n",
      "Epoch [600/6000] Loss: 4.19e-03 params: [0.1434845 0.5092325 0.7659647]\n",
      "Epoch [800/6000] Loss: 4.05e-03 params: [0.13178627 0.5478024  0.8815972 ]\n",
      "Epoch [1000/6000] Loss: 3.71e-03 params: [0.15141067 0.5633621  0.9857707 ]\n",
      "Epoch [1200/6000] Loss: 3.59e-03 params: [0.18363847 0.5760376  1.0909581 ]\n",
      "Epoch [1400/6000] Loss: 3.48e-03 params: [0.22360004 0.58939075 1.1996249 ]\n",
      "Epoch [1600/6000] Loss: 3.37e-03 params: [0.26837406 0.60444903 1.3123947 ]\n",
      "Epoch [1800/6000] Loss: 3.29e-03 params: [0.31674916 0.62061155 1.4289838 ]\n",
      "Epoch [2000/6000] Loss: 3.20e-03 params: [0.36733714 0.6370172  1.5477507 ]\n",
      "Epoch [2200/6000] Loss: 3.13e-03 params: [0.41797695 0.6528426  1.6663611 ]\n",
      "Epoch [2400/6000] Loss: 3.08e-03 params: [0.4666333 0.6669031 1.7819306]\n",
      "Epoch [2600/6000] Loss: 3.03e-03 params: [0.5111994 0.6798583 1.8920488]\n",
      "Epoch [2800/6000] Loss: 3.02e-03 params: [0.55089283 0.691341   1.9943123 ]\n",
      "Epoch [3000/6000] Loss: 2.93e-03 params: [0.58539474 0.7015028  2.0861933 ]\n",
      "Epoch [3200/6000] Loss: 2.86e-03 params: [0.6154807 0.7109927 2.1665099]\n",
      "Epoch [3400/6000] Loss: 2.79e-03 params: [0.6425127 0.7201413 2.2340744]\n",
      "Epoch [3600/6000] Loss: 2.72e-03 params: [0.66604507 0.7280191  2.2883532 ]\n",
      "Epoch [3800/6000] Loss: 2.64e-03 params: [0.68397284 0.7341423  2.3289714 ]\n",
      "Epoch [4000/6000] Loss: 2.59e-03 params: [0.6959441 0.7392585 2.356136 ]\n",
      "Epoch [4200/6000] Loss: 2.57e-03 params: [0.70384496 0.7438701  2.3710356 ]\n",
      "Epoch [4400/6000] Loss: 2.42e-03 params: [0.709788   0.74858946 2.3760216 ]\n",
      "Epoch [4600/6000] Loss: 2.38e-03 params: [0.71500885 0.75310326 2.3743267 ]\n",
      "Epoch [4800/6000] Loss: 2.34e-03 params: [0.7197296 0.7569706 2.3696   ]\n",
      "Epoch [5000/6000] Loss: 2.33e-03 params: [0.7239901  0.75990254 2.365081  ]\n",
      "Epoch 05136: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [5200/6000] Loss: 2.32e-03 params: [0.7269045  0.76176137 2.3619342 ]\n",
      "Epoch [5400/6000] Loss: 2.32e-03 params: [0.7283375  0.76251566 2.3609462 ]\n",
      "Epoch [5600/6000] Loss: 2.31e-03 params: [0.7295367  0.76312214 2.3601723 ]\n",
      "Epoch [5800/6000] Loss: 2.31e-03 params: [0.73052824 0.7635924  2.3597748 ]\n",
      "Epoch [6000/6000] Loss: 2.31e-03 params: [0.73131996 0.76392233 2.359938  ]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# C_3, cGEM=3.125\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 3.125])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/C_3.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c8081cf-3826-42f1-aaf8-7d6a7151cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.73131996\n",
      "fm=0.76392233\n",
      "fp=2.359938\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3362a44-afad-478a-9bf1-86805a166af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 8.17e-03 params: [0.5561322  0.27917406 0.7729857 ]\n",
      "Epoch [400/6000] Loss: 3.33e-03 params: [0.42431924 0.39610144 0.95435196]\n",
      "Epoch [600/6000] Loss: 2.81e-03 params: [0.35980263 0.4371681  1.0926276 ]\n",
      "Epoch 00775: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [800/6000] Loss: 2.44e-03 params: [0.32978535 0.4327691  1.1974884 ]\n",
      "Epoch [1000/6000] Loss: 2.31e-03 params: [0.31619447 0.42496884 1.24963   ]\n",
      "Epoch [1200/6000] Loss: 2.20e-03 params: [0.3022287  0.41562095 1.300841  ]\n",
      "Epoch [1400/6000] Loss: 2.11e-03 params: [0.28768536 0.40586862 1.3500268 ]\n",
      "Epoch [1600/6000] Loss: 2.07e-03 params: [0.2729626 0.3964494 1.3961803]\n",
      "Epoch [1800/6000] Loss: 3.14e-03 params: [0.2585274  0.38785413 1.4384629 ]\n",
      "Epoch [2000/6000] Loss: 1.95e-03 params: [0.24513632 0.38040462 1.4763112 ]\n",
      "Epoch 02070: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [2200/6000] Loss: 1.93e-03 params: [0.23699684 0.37626386 1.4994624 ]\n",
      "Epoch [2400/6000] Loss: 1.91e-03 params: [0.23130584 0.37316737 1.51576   ]\n",
      "Epoch [2600/6000] Loss: 1.89e-03 params: [0.22595942 0.37028396 1.5314535 ]\n",
      "Epoch [2800/6000] Loss: 1.87e-03 params: [0.22109747 0.36773443 1.5464281 ]\n",
      "Epoch [3000/6000] Loss: 1.86e-03 params: [0.2168848  0.36560225 1.5606155 ]\n",
      "Epoch [3200/6000] Loss: 1.84e-03 params: [0.21347274 0.36394393 1.5740114 ]\n",
      "Epoch [3400/6000] Loss: 1.83e-03 params: [0.21103024 0.36295867 1.5867181 ]\n",
      "Epoch [3600/6000] Loss: 2.37e-03 params: [0.20960057 0.36235887 1.5987775 ]\n",
      "Epoch [3800/6000] Loss: 1.81e-03 params: [0.20917393 0.3623315  1.6104207 ]\n",
      "Epoch [4000/6000] Loss: 1.80e-03 params: [0.20967852 0.36273628 1.6218355 ]\n",
      "Epoch [4200/6000] Loss: 1.79e-03 params: [0.2110384  0.36355823 1.6332551 ]\n",
      "Epoch [4400/6000] Loss: 1.78e-03 params: [0.21312489 0.36472368 1.6448942 ]\n",
      "Epoch [4600/6000] Loss: 2.70e-03 params: [0.21577121 0.36588714 1.6568618 ]\n",
      "Epoch [4800/6000] Loss: 1.77e-03 params: [0.21894792 0.36751083 1.6694043 ]\n",
      "Epoch [5000/6000] Loss: 1.76e-03 params: [0.2224883 0.3692892 1.6825069]\n",
      "Epoch 05102: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [5200/6000] Loss: 1.76e-03 params: [0.22535379 0.37078094 1.6928414 ]\n",
      "Epoch [5400/6000] Loss: 1.76e-03 params: [0.22732253 0.37160453 1.7000579 ]\n",
      "Epoch [5600/6000] Loss: 1.75e-03 params: [0.22933267 0.37249097 1.7076993 ]\n",
      "Epoch [5800/6000] Loss: 1.75e-03 params: [0.23141702 0.3734413  1.715732  ]\n",
      "Epoch [6000/6000] Loss: 1.75e-03 params: [0.23360129 0.37445915 1.7241403 ]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# D_3, cGEM=3.125\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 3.125])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/D_3.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9ca8636-c88f-4116-8731-8ebdc895b165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.23360129\n",
      "fm=0.37445915\n",
      "fp=1.7241403\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9932e36c-87ce-43cd-8c8b-00ae0e779d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 1.28e-02 params: [0.46139723 0.8991579  1.071198  ]\n",
      "Epoch [400/6000] Loss: 7.78e-03 params: [0.368236  0.8787292 1.2634718]\n",
      "Epoch [600/6000] Loss: 6.81e-03 params: [0.25939497 0.85534406 1.4279714 ]\n",
      "Epoch [800/6000] Loss: 6.56e-03 params: [0.15378448 0.83442175 1.5575273 ]\n",
      "Epoch [1000/6000] Loss: 6.16e-03 params: [0.06041493 0.817208   1.6533712 ]\n",
      "Epoch [1200/6000] Loss: 6.02e-03 params: [3.9851991e-05 8.0502665e-01 1.7180035e+00]\n",
      "Epoch [1400/6000] Loss: 5.93e-03 params: [3.3850225e-05 8.0378509e-01 1.7666703e+00]\n",
      "Epoch [1600/6000] Loss: 7.69e-03 params: [1.5240337e-05 8.0228639e-01 1.8044522e+00]\n",
      "Epoch [1800/6000] Loss: 5.79e-03 params: [2.5471043e-05 8.0161101e-01 1.8323135e+00]\n",
      "Epoch 01912: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [2000/6000] Loss: 5.76e-03 params: [5.4331736e-05 8.0144000e-01 1.8486247e+00]\n",
      "Epoch [2200/6000] Loss: 5.73e-03 params: [6.9901922e-05 8.0110610e-01 1.8573673e+00]\n",
      "Epoch [2400/6000] Loss: 5.70e-03 params: [3.6662466e-05 8.0089372e-01 1.8653132e+00]\n",
      "Epoch [2600/6000] Loss: 5.67e-03 params: [1.5098686e-05 8.0076867e-01 1.8724872e+00]\n",
      "Epoch [2800/6000] Loss: 5.64e-03 params: [6.9201906e-07 8.0073112e-01 1.8790411e+00]\n",
      "Epoch [3000/6000] Loss: 5.60e-03 params: [2.4138624e-05 8.0079043e-01 1.8851783e+00]\n",
      "Epoch [3200/6000] Loss: 5.57e-03 params: [4.0845509e-05 8.0095810e-01 1.8911256e+00]\n",
      "Epoch [3400/6000] Loss: 5.56e-03 params: [7.2091629e-05 8.0122501e-01 1.8970809e+00]\n",
      "Epoch [3600/6000] Loss: 5.49e-03 params: [2.6774776e-05 8.0166978e-01 1.9031481e+00]\n",
      "Epoch [3800/6000] Loss: 5.46e-03 params: [1.8330647e-05 8.0217183e-01 1.9093696e+00]\n",
      "Epoch [4000/6000] Loss: 5.42e-03 params: [2.3677134e-05 8.0271786e-01 1.9158422e+00]\n",
      "Epoch [4200/6000] Loss: 5.39e-03 params: [1.7967643e-05 8.0317897e-01 1.9226458e+00]\n",
      "Epoch [4400/6000] Loss: 5.36e-03 params: [1.8606806e-05 8.0370694e-01 1.9299845e+00]\n",
      "Epoch [4600/6000] Loss: 5.38e-03 params: [1.1246115e-05 8.0408812e-01 1.9382426e+00]\n",
      "Epoch [4800/6000] Loss: 5.29e-03 params: [1.3779900e-06 8.0452794e-01 1.9477464e+00]\n",
      "Epoch [5000/6000] Loss: 5.26e-03 params: [3.3710352e-05 8.0496436e-01 1.9588377e+00]\n",
      "Epoch [5200/6000] Loss: 5.24e-03 params: [6.9438262e-05 8.0544162e-01 1.9718307e+00]\n",
      "Epoch [5400/6000] Loss: 5.17e-03 params: [1.6523905e-05 8.0583102e-01 1.9865681e+00]\n",
      "Epoch [5600/6000] Loss: 5.13e-03 params: [8.792362e-05 8.064076e-01 2.002414e+00]\n",
      "Epoch [5800/6000] Loss: 5.09e-03 params: [3.1512966e-05 8.0696249e-01 2.0178304e+00]\n",
      "Epoch [6000/6000] Loss: 5.08e-03 params: [2.3339730e-05 8.0744940e-01 2.0306816e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# A_4, cGEM=6.25\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 6.25])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/A_4.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e70a30e-3a61-480e-a555-2a857730717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=2.333973e-05\n",
      "fm=0.8074494\n",
      "fp=2.0306816\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b471e26-4e4f-4288-b5c4-357a5d4628d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 1.81e-02 params: [5.5527860e-05 8.0141133e-01 9.7534233e-01]\n",
      "Epoch [400/6000] Loss: 9.83e-03 params: [2.2734668e-05 8.4734988e-01 1.1487929e+00]\n",
      "Epoch [600/6000] Loss: 7.39e-03 params: [2.7677908e-05 8.3955842e-01 1.3021915e+00]\n",
      "Epoch [800/6000] Loss: 6.58e-03 params: [3.0039271e-05 8.3308357e-01 1.4449065e+00]\n",
      "Epoch [1000/6000] Loss: 6.08e-03 params: [3.9510694e-05 8.2903087e-01 1.5740237e+00]\n",
      "Epoch [1200/6000] Loss: 8.98e-03 params: [2.5127118e-05 8.2551032e-01 1.6878364e+00]\n",
      "Epoch 01240: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [1400/6000] Loss: 5.72e-03 params: [2.1763681e-05 8.2411140e-01 1.7488703e+00]\n",
      "Epoch [1600/6000] Loss: 5.61e-03 params: [3.6666752e-05 8.2244635e-01 1.7974147e+00]\n",
      "Epoch [1800/6000] Loss: 5.52e-03 params: [4.5474695e-05 8.2095683e-01 1.8434026e+00]\n",
      "Epoch [2000/6000] Loss: 5.45e-03 params: [3.8790549e-05 8.1961024e-01 1.8861654e+00]\n",
      "Epoch [2200/6000] Loss: 5.50e-03 params: [2.7448532e-05 8.1840265e-01 1.9251293e+00]\n",
      "Epoch 02202: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [2400/6000] Loss: 5.36e-03 params: [7.3594911e-06 8.1802154e-01 1.9436231e+00]\n",
      "Epoch [2600/6000] Loss: 5.33e-03 params: [6.6034090e-08 8.1745821e-01 1.9615148e+00]\n",
      "Epoch [2800/6000] Loss: 5.30e-03 params: [1.6359555e-06 8.1691837e-01 1.9788920e+00]\n",
      "Epoch [3000/6000] Loss: 5.28e-03 params: [3.3980679e-05 8.1640059e-01 1.9955523e+00]\n",
      "Epoch [3200/6000] Loss: 5.25e-03 params: [1.9749223e-06 8.1590837e-01 2.0113149e+00]\n",
      "Epoch [3400/6000] Loss: 5.23e-03 params: [1.1381137e-05 8.1545079e-01 2.0260222e+00]\n",
      "Epoch [3600/6000] Loss: 5.22e-03 params: [2.6730675e-05 8.1509310e-01 2.0394781e+00]\n",
      "Epoch [3800/6000] Loss: 5.21e-03 params: [1.7075909e-06 8.1475419e-01 2.0515981e+00]\n",
      "Epoch [4000/6000] Loss: 6.18e-03 params: [2.9442719e-05 8.1445122e-01 2.0623083e+00]\n",
      "Epoch [4200/6000] Loss: 5.19e-03 params: [3.2257529e-05 8.1422502e-01 2.0715246e+00]\n",
      "Epoch 04259: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [4400/6000] Loss: 5.19e-03 params: [1.3400766e-05 8.1412238e-01 2.0767469e+00]\n",
      "Epoch [4600/6000] Loss: 5.18e-03 params: [5.1100260e-06 8.1400859e-01 2.0805314e+00]\n",
      "Epoch [4800/6000] Loss: 5.18e-03 params: [1.4958558e-05 8.1390917e-01 2.0841012e+00]\n",
      "Epoch [5000/6000] Loss: 5.18e-03 params: [1.4025492e-05 8.1382126e-01 2.0874152e+00]\n",
      "Epoch [5200/6000] Loss: 5.17e-03 params: [2.3843213e-06 8.1374592e-01 2.0904415e+00]\n",
      "Epoch [5400/6000] Loss: 5.17e-03 params: [5.9455310e-06 8.1368458e-01 2.0931587e+00]\n",
      "Epoch [5600/6000] Loss: 5.17e-03 params: [1.3340643e-05 8.1363881e-01 2.0955560e+00]\n",
      "Epoch [5800/6000] Loss: 5.17e-03 params: [1.8340921e-05 8.1361115e-01 2.0976284e+00]\n",
      "Epoch [6000/6000] Loss: 5.43e-03 params: [1.0746952e-05 8.1360054e-01 2.0993881e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# B_4, cGEM=6.25\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 6.25])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/B_4.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91df96cb-8572-4ef1-9b57-e0aa942021aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=1.0746952e-05\n",
      "fm=0.81360054\n",
      "fp=2.0993881\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33d38cb0-6e14-455f-94c5-9ad20e543b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 2.46e-02 params: [0.5596991 0.725065  1.0454592]\n",
      "Epoch [400/6000] Loss: 8.82e-03 params: [0.4211323 0.865681  1.2326629]\n",
      "Epoch [600/6000] Loss: 7.51e-03 params: [0.35667208 0.9104649  1.366025  ]\n",
      "Epoch 00773: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [800/6000] Loss: 7.09e-03 params: [0.32102287 0.91383994 1.4708819 ]\n",
      "Epoch [1000/6000] Loss: 6.95e-03 params: [0.30226794 0.911648   1.525964  ]\n",
      "Epoch [1200/6000] Loss: 6.84e-03 params: [0.28158012 0.9084455  1.5819436 ]\n",
      "Epoch [1400/6000] Loss: 6.76e-03 params: [0.2586184 0.9049417 1.6377436]\n",
      "Epoch [1600/6000] Loss: 6.69e-03 params: [0.23337631 0.90133166 1.6923376 ]\n",
      "Epoch [1800/6000] Loss: 6.63e-03 params: [0.20593537 0.8977424  1.7447525 ]\n",
      "Epoch 01983: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [2000/6000] Loss: 6.59e-03 params: [0.17768897 0.8941919  1.7922704 ]\n",
      "Epoch [2200/6000] Loss: 6.55e-03 params: [0.16176198 0.8923284  1.81622   ]\n",
      "Epoch [2400/6000] Loss: 6.53e-03 params: [0.14457555 0.89025617 1.8400633 ]\n",
      "Epoch [2600/6000] Loss: 6.50e-03 params: [0.12606844 0.8880716  1.8635366 ]\n",
      "Epoch [2800/6000] Loss: 6.47e-03 params: [0.10623571 0.88578624 1.8863645 ]\n",
      "Epoch [3000/6000] Loss: 6.45e-03 params: [0.08508375 0.88340545 1.9082695 ]\n",
      "Epoch [3200/6000] Loss: 6.42e-03 params: [0.06262544 0.8809343  1.9289747 ]\n",
      "Epoch [3400/6000] Loss: 6.40e-03 params: [0.03887858 0.87840486 1.9481806 ]\n",
      "Epoch [3600/6000] Loss: 6.38e-03 params: [0.01388918 0.87578493 1.9655918 ]\n",
      "Epoch [3800/6000] Loss: 6.36e-03 params: [8.1810231e-06 8.7358391e-01 1.9812237e+00]\n",
      "Epoch [4000/6000] Loss: 6.35e-03 params: [4.1889431e-05 8.7318021e-01 1.9963392e+00]\n",
      "Epoch [4200/6000] Loss: 6.34e-03 params: [3.2715758e-05 8.7294739e-01 2.0104988e+00]\n",
      "Epoch 04333: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [4400/6000] Loss: 6.33e-03 params: [8.1481894e-06 8.7276495e-01 2.0214520e+00]\n",
      "Epoch [4600/6000] Loss: 6.33e-03 params: [6.5201716e-06 8.7265915e-01 2.0277958e+00]\n",
      "Epoch [4800/6000] Loss: 6.32e-03 params: [3.6913407e-06 8.7254918e-01 2.0340574e+00]\n",
      "Epoch [5000/6000] Loss: 6.31e-03 params: [1.8935963e-05 8.7244380e-01 2.0401723e+00]\n",
      "Epoch [5200/6000] Loss: 6.31e-03 params: [1.8517188e-05 8.7234777e-01 2.0460651e+00]\n",
      "Epoch [5400/6000] Loss: 6.30e-03 params: [1.8156154e-05 8.7226063e-01 2.0516624e+00]\n",
      "Epoch [5600/6000] Loss: 6.29e-03 params: [1.7859935e-05 8.7218428e-01 2.0568910e+00]\n",
      "Epoch [5800/6000] Loss: 6.28e-03 params: [1.7610981e-05 8.7212127e-01 2.0616813e+00]\n",
      "Epoch [6000/6000] Loss: 6.27e-03 params: [1.9328287e-05 8.7208277e-01 2.0659506e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# C_4, cGEM=6.25\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 6.25])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/C_4.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e98bca44-3cc3-4762-bbda-120dd39e58d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=1.9328287e-05\n",
      "fm=0.87208277\n",
      "fp=2.0659506\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4db59ca1-7730-4cb2-8efb-af23e62d020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 1.49e-02 params: [0.17585187 0.8072099  0.55448854]\n",
      "Epoch [400/6000] Loss: 5.29e-03 params: [0.07713408 0.7584627  0.7639463 ]\n",
      "Epoch [600/6000] Loss: 4.05e-03 params: [1.1148560e-04 7.2965145e-01 9.5220393e-01]\n",
      "Epoch [800/6000] Loss: 4.82e-03 params: [4.5064990e-06 7.2389954e-01 1.1247835e+00]\n",
      "Epoch [1000/6000] Loss: 3.35e-03 params: [1.20069235e-05 7.20419526e-01 1.28268766e+00]\n",
      "Epoch 01176: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [1200/6000] Loss: 3.22e-03 params: [1.7813723e-05 7.1719217e-01 1.4182633e+00]\n",
      "Epoch [1400/6000] Loss: 3.14e-03 params: [1.8161098e-05 7.1505380e-01 1.4862056e+00]\n",
      "Epoch [1600/6000] Loss: 3.08e-03 params: [1.8634337e-05 7.1320921e-01 1.5526733e+00]\n",
      "Epoch [1800/6000] Loss: 3.03e-03 params: [1.8978224e-05 7.1152723e-01 1.6170425e+00]\n",
      "Epoch [2000/6000] Loss: 2.99e-03 params: [1.9228224e-05 7.0998639e-01 1.6788336e+00]\n",
      "Epoch [2200/6000] Loss: 2.98e-03 params: [3.2631295e-05 7.0844018e-01 1.7375997e+00]\n",
      "Epoch 02216: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [2400/6000] Loss: 2.95e-03 params: [4.3073902e-05 7.0807165e-01 1.7683822e+00]\n",
      "Epoch [2600/6000] Loss: 2.94e-03 params: [1.1802553e-05 7.0739472e-01 1.7968165e+00]\n",
      "Epoch [2800/6000] Loss: 2.93e-03 params: [4.878425e-05 7.067453e-01 1.825121e+00]\n",
      "Epoch [3000/6000] Loss: 2.92e-03 params: [2.7035599e-05 7.0611733e-01 1.8531044e+00]\n",
      "Epoch [3200/6000] Loss: 2.91e-03 params: [2.6043625e-05 7.0551413e-01 1.8805844e+00]\n",
      "Epoch [3400/6000] Loss: 2.90e-03 params: [4.4974186e-06 7.0493931e-01 1.9073886e+00]\n",
      "Epoch [3600/6000] Loss: 3.04e-03 params: [4.9151313e-06 7.0437658e-01 1.9333447e+00]\n",
      "Epoch [3800/6000] Loss: 2.89e-03 params: [2.3422515e-05 7.0393056e-01 1.9582646e+00]\n",
      "Epoch [4000/6000] Loss: 2.88e-03 params: [5.5453304e-05 7.0350474e-01 1.9819939e+00]\n",
      "Epoch [4200/6000] Loss: 2.88e-03 params: [6.737702e-05 7.031290e-01 2.004373e+00]\n",
      "Epoch [4400/6000] Loss: 3.01e-03 params: [8.7297731e-06 7.0269066e-01 2.0252788e+00]\n",
      "Epoch 04405: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [4600/6000] Loss: 2.87e-03 params: [9.5921769e-07 7.0259804e-01 2.0355477e+00]\n",
      "Epoch [4800/6000] Loss: 2.87e-03 params: [7.3595111e-06 7.0241439e-01 2.0454886e+00]\n",
      "Epoch [5000/6000] Loss: 2.86e-03 params: [1.2363171e-05 7.0224255e-01 2.0553005e+00]\n",
      "Epoch [5200/6000] Loss: 2.86e-03 params: [1.1520673e-05 7.0207989e-01 2.0649004e+00]\n",
      "Epoch [5400/6000] Loss: 2.86e-03 params: [1.0982089e-05 7.0192844e-01 2.0741949e+00]\n",
      "Epoch [5600/6000] Loss: 2.85e-03 params: [7.9709571e-06 7.0179039e-01 2.0830932e+00]\n",
      "Epoch [5800/6000] Loss: 2.85e-03 params: [1.4253814e-05 7.0166761e-01 2.0915010e+00]\n",
      "Epoch [6000/6000] Loss: 2.84e-03 params: [1.0087875e-05 7.0156342e-01 2.0993211e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# D_4, cGEM=6.25\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 6.25])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/D_4.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "912fb5b0-eed0-4ee2-a4f7-fc4729a82c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=1.0087875e-05\n",
      "fm=0.7015634\n",
      "fp=2.0993211\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53e19c9c-f2c5-4f2e-ae54-924c01e05bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 7.72e-02 params: [0.39778936 0.4331471  0.9557597 ]\n",
      "Epoch [400/6000] Loss: 4.84e-02 params: [0.20636007 0.659946   1.1866872 ]\n",
      "Epoch [600/6000] Loss: 2.46e-02 params: [0.08185443 0.84318143 1.3926628 ]\n",
      "Epoch [800/6000] Loss: 1.73e-02 params: [0.0443233 0.9278079 1.5305123]\n",
      "Epoch [1000/6000] Loss: 1.58e-02 params: [0.0548047  0.95144874 1.625495  ]\n",
      "Epoch [1200/6000] Loss: 1.48e-02 params: [0.08181296 0.9580673  1.7077786 ]\n",
      "Epoch [1400/6000] Loss: 1.36e-02 params: [0.11558408 0.9618856  1.7864555 ]\n",
      "Epoch [1600/6000] Loss: 1.26e-02 params: [0.1533847  0.96560353 1.8626809 ]\n",
      "Epoch [1800/6000] Loss: 1.22e-02 params: [0.19416791 0.96941644 1.9346372 ]\n",
      "Epoch [2000/6000] Loss: 1.20e-02 params: [0.23799644 0.9730892  2.0001972 ]\n",
      "Epoch [2200/6000] Loss: 1.19e-02 params: [0.2841931 0.9765648 2.0588398]\n",
      "Epoch [2400/6000] Loss: 1.18e-02 params: [0.3317892 0.9800007 2.1107104]\n",
      "Epoch [2600/6000] Loss: 1.17e-02 params: [0.38000605 0.98337525 2.1561327 ]\n",
      "Epoch [2800/6000] Loss: 1.16e-02 params: [0.42784458 0.9868583  2.195874  ]\n",
      "Epoch [3000/6000] Loss: 1.15e-02 params: [0.47454998 0.9902172  2.230622  ]\n",
      "Epoch [3200/6000] Loss: 1.14e-02 params: [0.5193386 0.9935116 2.260918 ]\n",
      "Epoch [3400/6000] Loss: 1.14e-02 params: [0.5614571 0.9966756 2.2874107]\n",
      "Epoch [3600/6000] Loss: 1.29e-02 params: [0.6003449 0.9995399 2.3104882]\n",
      "Epoch 03624: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [3800/6000] Loss: 1.13e-02 params: [0.62084275 1.0010481  2.3221629 ]\n",
      "Epoch [4000/6000] Loss: 1.12e-02 params: [0.63876367 1.0023537  2.3322618 ]\n",
      "Epoch [4200/6000] Loss: 1.12e-02 params: [0.6561864 1.0036372 2.342046 ]\n",
      "Epoch [4400/6000] Loss: 1.12e-02 params: [0.67291075 1.0048598  2.3513627 ]\n",
      "Epoch [4600/6000] Loss: 1.11e-02 params: [0.6887217 1.0060072 2.36009  ]\n",
      "Epoch [4800/6000] Loss: 1.11e-02 params: [0.7034179 1.0070673 2.368119 ]\n",
      "Epoch [5000/6000] Loss: 1.11e-02 params: [0.7168222 1.0080327 2.375339 ]\n",
      "Epoch [5200/6000] Loss: 1.11e-02 params: [0.728788 1.008924 2.381653]\n",
      "Epoch [5400/6000] Loss: 1.10e-02 params: [0.73927283 1.0096766  2.3870165 ]\n",
      "Epoch [5600/6000] Loss: 1.10e-02 params: [0.7482123 1.0102988 2.3914828]\n",
      "Epoch [5800/6000] Loss: 1.10e-02 params: [0.75559735 1.0108194  2.395122  ]\n",
      "Epoch 05975: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [6000/6000] Loss: 1.09e-02 params: [0.76116383 1.0112497  2.3978627 ]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# A_5, cGEM=12.5\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 12.5])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/A_5.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b12eff5f-92bd-4010-b38e-07f4aadbeb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.76116383\n",
      "fm=1.0112497\n",
      "fp=2.3978627\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60af64e1-332e-49f2-97b2-09c65ee4cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 4.21e-02 params: [0.62650436 0.9360577  0.7517732 ]\n",
      "Epoch [400/6000] Loss: 2.30e-02 params: [0.53611654 1.0165156  0.9098535 ]\n",
      "Epoch [600/6000] Loss: 1.57e-02 params: [0.50748646 1.0265394  1.0503712 ]\n",
      "Epoch [800/6000] Loss: 1.46e-02 params: [0.49121773 1.0240817  1.1877354 ]\n",
      "Epoch [1000/6000] Loss: 1.38e-02 params: [0.4768379 1.0214692 1.3209949]\n",
      "Epoch [1200/6000] Loss: 1.31e-02 params: [0.4643745 1.0189342 1.4484941]\n",
      "Epoch [1400/6000] Loss: 1.27e-02 params: [0.45414302 1.0167451  1.5689822 ]\n",
      "Epoch [1600/6000] Loss: 1.25e-02 params: [0.44717327 1.0148127  1.680644  ]\n",
      "Epoch [1800/6000] Loss: 1.23e-02 params: [0.44444075 1.013001   1.7817459 ]\n",
      "Epoch [2000/6000] Loss: 1.22e-02 params: [0.44594136 1.011587   1.8716297 ]\n",
      "Epoch [2200/6000] Loss: 1.21e-02 params: [0.45136434 1.0106064  1.9501821 ]\n",
      "Epoch [2400/6000] Loss: 1.21e-02 params: [0.4601047 1.0100389 2.017673 ]\n",
      "Epoch [2600/6000] Loss: 1.22e-02 params: [0.47144052 1.009774   2.0745301 ]\n",
      "Epoch 02638: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [2800/6000] Loss: 1.20e-02 params: [0.47918922 1.0097877  2.1039715 ]\n",
      "Epoch [3000/6000] Loss: 1.20e-02 params: [0.48635456 1.009841   2.126957  ]\n",
      "Epoch [3200/6000] Loss: 1.20e-02 params: [0.49400166 1.0099628  2.1483297 ]\n",
      "Epoch [3400/6000] Loss: 1.19e-02 params: [0.5019934 1.0101337 2.167879 ]\n",
      "Epoch [3600/6000] Loss: 1.19e-02 params: [0.5101506 1.010338  2.185452 ]\n",
      "Epoch [3800/6000] Loss: 1.19e-02 params: [0.5182501 1.0105618 2.20096  ]\n",
      "Epoch [4000/6000] Loss: 1.18e-02 params: [0.5261073 1.0107908 2.214349 ]\n",
      "Epoch [4200/6000] Loss: 1.18e-02 params: [0.53353024 1.0110135  2.2256157 ]\n",
      "Epoch [4400/6000] Loss: 1.17e-02 params: [0.5404067 1.011221  2.234806 ]\n",
      "Epoch [4600/6000] Loss: 1.17e-02 params: [0.5465604 1.0114235 2.242016 ]\n",
      "Epoch [4800/6000] Loss: 1.17e-02 params: [0.55200076 1.0116231  2.2474332 ]\n",
      "Epoch [5000/6000] Loss: 1.16e-02 params: [0.55668426 1.0118048  2.2512958 ]\n",
      "Epoch [5200/6000] Loss: 1.17e-02 params: [0.5606345 1.0119666 2.253927 ]\n",
      "Epoch [5400/6000] Loss: 1.16e-02 params: [0.5638289 1.0121281 2.2556422]\n",
      "Epoch [5600/6000] Loss: 1.15e-02 params: [0.566401  1.0122167 2.2567222]\n",
      "Epoch [5800/6000] Loss: 1.17e-02 params: [0.5683603 1.0122867 2.2574544]\n",
      "Epoch [6000/6000] Loss: 1.15e-02 params: [0.56980366 1.0123497  2.257992  ]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# B_5, cGEM=12.5\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 12.5])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/B_5.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e717fce5-00cf-4027-97bf-8492586cbc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.56980366\n",
      "fm=1.0123497\n",
      "fp=2.257992\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "116c1149-5295-4369-917e-e3ca90313fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 9.35e-02 params: [0.25793162 0.5840618  0.67671365]\n",
      "Epoch [400/6000] Loss: 4.48e-02 params: [0.08041872 0.79319257 0.9043353 ]\n",
      "Epoch [600/6000] Loss: 2.54e-02 params: [2.0586331e-04 9.2651927e-01 1.0921046e+00]\n",
      "Epoch [800/6000] Loss: 2.18e-02 params: [7.3401643e-06 9.7633547e-01 1.2344171e+00]\n",
      "Epoch [1000/6000] Loss: 2.06e-02 params: [7.6968599e-06 9.8751760e-01 1.3568411e+00]\n",
      "Epoch [1200/6000] Loss: 1.94e-02 params: [1.4513485e-06 9.8784041e-01 1.4730966e+00]\n",
      "Epoch [1400/6000] Loss: 1.78e-02 params: [2.5204581e-06 9.8588467e-01 1.5870354e+00]\n",
      "Epoch [1600/6000] Loss: 1.70e-02 params: [3.3893348e-06 9.8403037e-01 1.6990211e+00]\n",
      "Epoch [1800/6000] Loss: 1.65e-02 params: [8.836345e-06 9.826602e-01 1.805774e+00]\n",
      "Epoch [2000/6000] Loss: 1.62e-02 params: [8.8769921e-06 9.8127824e-01 1.9051068e+00]\n",
      "Epoch [2200/6000] Loss: 1.60e-02 params: [4.6876103e-06 9.7995722e-01 1.9955913e+00]\n",
      "Epoch [2400/6000] Loss: 1.59e-02 params: [4.7657350e-06 9.7874528e-01 2.0761266e+00]\n",
      "Epoch [2600/6000] Loss: 1.58e-02 params: [4.8440888e-06 9.7755665e-01 2.1463172e+00]\n",
      "Epoch [2800/6000] Loss: 1.57e-02 params: [3.7758298e-06 9.7657865e-01 2.2053823e+00]\n",
      "Epoch 02930: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [3000/6000] Loss: 1.56e-02 params: [5.0643703e-06 9.7596604e-01 2.2462263e+00]\n",
      "Epoch [3200/6000] Loss: 1.56e-02 params: [2.2672157e-07 9.7552782e-01 2.2681260e+00]\n",
      "Epoch [3400/6000] Loss: 1.56e-02 params: [8.9139321e-06 9.7516567e-01 2.2884808e+00]\n",
      "Epoch [3600/6000] Loss: 1.56e-02 params: [2.4631406e-06 9.7485316e-01 2.3069062e+00]\n",
      "Epoch [3800/6000] Loss: 1.56e-02 params: [7.0546139e-07 9.7458714e-01 2.3231707e+00]\n",
      "Epoch [4000/6000] Loss: 1.55e-02 params: [3.4397294e-06 9.7436756e-01 2.3371410e+00]\n",
      "Epoch [4200/6000] Loss: 1.55e-02 params: [3.7145692e-06 9.7419494e-01 2.3487747e+00]\n",
      "Epoch [4400/6000] Loss: 1.55e-02 params: [4.0477858e-06 9.7406924e-01 2.3581190e+00]\n",
      "Epoch [4600/6000] Loss: 1.55e-02 params: [5.1078941e-06 9.7399098e-01 2.3652146e+00]\n",
      "Epoch 04682: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [4800/6000] Loss: 1.55e-02 params: [4.8289189e-07 9.7396660e-01 2.3690505e+00]\n",
      "Epoch [5000/6000] Loss: 1.54e-02 params: [6.8959844e-07 9.7394300e-01 2.3713481e+00]\n",
      "Epoch [5200/6000] Loss: 1.54e-02 params: [3.4824780e-08 9.7393721e-01 2.3733141e+00]\n",
      "Epoch [5400/6000] Loss: 1.54e-02 params: [5.4953457e-06 9.7394425e-01 2.3749003e+00]\n",
      "Epoch [5600/6000] Loss: 1.54e-02 params: [1.1873699e-06 9.7396410e-01 2.3760889e+00]\n",
      "Epoch [5800/6000] Loss: 1.54e-02 params: [1.2072271e-06 9.7399682e-01 2.3768733e+00]\n",
      "Epoch [6000/6000] Loss: 1.54e-02 params: [6.9864677e-06 9.7404373e-01 2.3772602e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# C_5, cGEM=12.5\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 12.5])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/C_5.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e7aac84-040b-4c36-bf00-6f577ee7844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=6.9864677e-06\n",
      "fm=0.9740437\n",
      "fp=2.3772602\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8a4c58e-2905-4503-9f99-7e27f6367ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 1.29e-01 params: [0.4866544 0.3590404 0.797906 ]\n",
      "Epoch [400/6000] Loss: 8.07e-02 params: [0.2836509 0.5990944 1.0416037]\n",
      "Epoch [600/6000] Loss: 3.96e-02 params: [0.12775575 0.81144446 1.2733322 ]\n",
      "Epoch [800/6000] Loss: 2.41e-02 params: [0.04681785 0.9317394  1.4384093 ]\n",
      "Epoch [1000/6000] Loss: 2.00e-02 params: [0.01969562 0.9721852  1.5463214 ]\n",
      "Epoch [1200/6000] Loss: 1.78e-02 params: [0.01303231 0.9824783  1.6342595 ]\n",
      "Epoch [1400/6000] Loss: 1.72e-02 params: [0.01362044 0.9853396  1.7151985 ]\n",
      "Epoch [1600/6000] Loss: 1.68e-02 params: [0.01807988 0.98550576 1.7884793 ]\n",
      "Epoch [1800/6000] Loss: 1.66e-02 params: [0.0247636 0.9851357 1.8543482]\n",
      "Epoch [2000/6000] Loss: 1.64e-02 params: [0.03322333 0.9848566  1.9125595 ]\n",
      "Epoch [2200/6000] Loss: 1.62e-02 params: [0.04336953 0.9847816  1.9629234 ]\n",
      "Epoch [2400/6000] Loss: 1.61e-02 params: [0.05516261 0.9848972  2.005465  ]\n",
      "Epoch [2600/6000] Loss: 1.59e-02 params: [0.06845497 0.9853527  2.0406475 ]\n",
      "Epoch [2800/6000] Loss: 1.58e-02 params: [0.08327466 0.9859352  2.069145  ]\n",
      "Epoch [3000/6000] Loss: 1.56e-02 params: [0.09939531 0.98679745 2.091966  ]\n",
      "Epoch [3200/6000] Loss: 1.55e-02 params: [0.11673204 0.9877941  2.1099641 ]\n",
      "Epoch [3400/6000] Loss: 1.54e-02 params: [0.13501664 0.98902655 2.124008  ]\n",
      "Epoch 03537: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [3600/6000] Loss: 1.53e-02 params: [0.15117753 0.9901023  2.1334448 ]\n",
      "Epoch [3800/6000] Loss: 1.53e-02 params: [0.16126636 0.99079067 2.1384428 ]\n",
      "Epoch [4000/6000] Loss: 1.52e-02 params: [0.1719421 0.991553  2.1432357]\n",
      "Epoch [4200/6000] Loss: 1.52e-02 params: [0.18318577 0.9923445  2.147857  ]\n",
      "Epoch [4400/6000] Loss: 1.52e-02 params: [0.19492447 0.9931575  2.1523747 ]\n",
      "Epoch [4600/6000] Loss: 1.52e-02 params: [0.20706598 0.99398124 2.1568432 ]\n",
      "Epoch [4800/6000] Loss: 1.52e-02 params: [0.21949701 0.99481016 2.1612778 ]\n",
      "Epoch 04974: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [5000/6000] Loss: 1.52e-02 params: [0.23131633 0.995583   2.1654143 ]\n",
      "Epoch [5200/6000] Loss: 1.51e-02 params: [0.23782182 0.9960002  2.1676726 ]\n",
      "Epoch [5400/6000] Loss: 1.51e-02 params: [0.2446189 0.9964406 2.1700158]\n",
      "Epoch [5600/6000] Loss: 1.51e-02 params: [0.25168705 0.9968973  2.172428  ]\n",
      "Epoch [5800/6000] Loss: 1.51e-02 params: [0.25898746 0.997367   2.1748986 ]\n",
      "Epoch [6000/6000] Loss: 1.51e-02 params: [0.2664721 0.9978474 2.1774185]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# D_5, cGEM=12.5\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 12.5])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/D_5.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99ac7fc0-bd88-4236-9fe2-3b26ce02a04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.2664721\n",
      "fm=0.9978474\n",
      "fp=2.1774185\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7102b1b9-b673-41fd-bb9b-ccac99b29785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 4.60e-02 params: [0.3090364 0.88425   0.376065 ]\n",
      "Epoch [400/6000] Loss: 2.86e-02 params: [0.2009408  1.0190747  0.58345455]\n",
      "Epoch [600/6000] Loss: 2.43e-02 params: [0.18398681 1.0615287  0.7729264 ]\n",
      "Epoch [800/6000] Loss: 2.34e-02 params: [0.20420808 1.0679985  0.95073235]\n",
      "Epoch [1000/6000] Loss: 2.28e-02 params: [0.23401646 1.0689396  1.1243774 ]\n",
      "Epoch [1200/6000] Loss: 2.20e-02 params: [0.2680423 1.0697316 1.2945166]\n",
      "Epoch [1400/6000] Loss: 2.15e-02 params: [0.3050404 1.0708919 1.4608335]\n",
      "Epoch [1600/6000] Loss: 2.11e-02 params: [0.34412017 1.0723326  1.6230348 ]\n",
      "Epoch [1800/6000] Loss: 2.03e-02 params: [0.38444683 1.073926   1.780792  ]\n",
      "Epoch [2000/6000] Loss: 1.99e-02 params: [0.42522115 1.075642   1.9338374 ]\n",
      "Epoch [2200/6000] Loss: 1.94e-02 params: [0.46556067 1.0773742  2.081888  ]\n",
      "Epoch [2400/6000] Loss: 1.90e-02 params: [0.5044249 1.0788277 2.2246606]\n",
      "Epoch [2600/6000] Loss: 1.87e-02 params: [0.5408549 1.080109  2.3619273]\n",
      "Epoch [2800/6000] Loss: 1.85e-02 params: [0.5739684 1.0810262 2.4934404]\n",
      "Epoch [3000/6000] Loss: 1.83e-02 params: [0.6034262 1.0816737 2.618914 ]\n",
      "Epoch [3200/6000] Loss: 1.82e-02 params: [0.6293814 1.0820326 2.7380054]\n",
      "Epoch [3400/6000] Loss: 1.81e-02 params: [0.65207183 1.0821923  2.8503082 ]\n",
      "Epoch [3600/6000] Loss: 1.80e-02 params: [0.6716932 1.0821912 2.9554763]\n",
      "Epoch 03677: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [3800/6000] Loss: 1.79e-02 params: [0.6839419 1.0821263 3.0243826]\n",
      "Epoch [4000/6000] Loss: 1.79e-02 params: [0.69233996 1.082052   3.0732265 ]\n",
      "Epoch [4200/6000] Loss: 1.79e-02 params: [0.700408  1.0819618 3.1215484]\n",
      "Epoch [4400/6000] Loss: 1.78e-02 params: [0.7081544 1.0818565 3.1690125]\n",
      "Epoch [4600/6000] Loss: 1.78e-02 params: [0.7155213 1.0817351 3.2152946]\n",
      "Epoch [4800/6000] Loss: 1.78e-02 params: [0.7224519 1.0815977 3.2600715]\n",
      "Epoch [5000/6000] Loss: 1.78e-02 params: [0.7289842 1.0814574 3.3030305]\n",
      "Epoch [5200/6000] Loss: 1.77e-02 params: [0.7350427 1.081311  3.3438654]\n",
      "Epoch [5400/6000] Loss: 1.77e-02 params: [0.7406021 1.0811661 3.3822885]\n",
      "Epoch [5600/6000] Loss: 1.77e-02 params: [0.74564844 1.0810292  3.4180212 ]\n",
      "Epoch [5800/6000] Loss: 1.77e-02 params: [0.7501734 1.0808831 3.4508367]\n",
      "Epoch 05905: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [6000/6000] Loss: 1.77e-02 params: [0.75329965 1.0807875  3.4738712 ]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# A_6, cGEM=25\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 25])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/A_6.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bae7524-5a77-42d9-89fe-46721eb37dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.75329965\n",
      "fm=1.0807875\n",
      "fp=3.4738712\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc1538e0-c511-48eb-be0c-5fb08a1fac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 4.48e-02 params: [0.65755016 0.9459417  0.9601673 ]\n",
      "Epoch [400/6000] Loss: 2.79e-02 params: [0.5274755 1.0451844 1.1506315]\n",
      "Epoch [600/6000] Loss: 2.20e-02 params: [0.4685985 1.077707  1.3304292]\n",
      "Epoch [800/6000] Loss: 2.04e-02 params: [0.4324136 1.077369  1.5008518]\n",
      "Epoch [1000/6000] Loss: 2.05e-02 params: [0.3979582 1.0728002 1.6668648]\n",
      "Epoch [1200/6000] Loss: 1.85e-02 params: [0.36326692 1.0678158  1.8282812 ]\n",
      "Epoch [1400/6000] Loss: 1.77e-02 params: [0.3299644 1.0634017 1.9847986]\n",
      "Epoch [1600/6000] Loss: 1.73e-02 params: [0.3005967 1.0592148 2.1353161]\n",
      "Epoch [1800/6000] Loss: 1.70e-02 params: [0.27677974 1.0553031  2.2785215 ]\n",
      "Epoch [2000/6000] Loss: 1.67e-02 params: [0.2589228 1.0519637 2.414299 ]\n",
      "Epoch [2200/6000] Loss: 1.90e-02 params: [0.24752717 1.0492272  2.5422342 ]\n",
      "Epoch [2400/6000] Loss: 1.62e-02 params: [0.24272594 1.0470241  2.6624649 ]\n",
      "Epoch [2600/6000] Loss: 1.61e-02 params: [0.24440734 1.0454718  2.774774  ]\n",
      "Epoch [2800/6000] Loss: 1.57e-02 params: [0.2522438 1.0441357 2.8790848]\n",
      "Epoch [3000/6000] Loss: 1.56e-02 params: [0.26476875 1.0433924  2.9754062 ]\n",
      "Epoch [3200/6000] Loss: 1.55e-02 params: [0.2804072 1.043085  3.0634084]\n",
      "Epoch [3400/6000] Loss: 1.54e-02 params: [0.29760185 1.0430931  3.1429834 ]\n",
      "Epoch [3600/6000] Loss: 1.53e-02 params: [0.31523296 1.0430812  3.2139456 ]\n",
      "Epoch [3800/6000] Loss: 1.54e-02 params: [0.33221313 1.0433006  3.2764292 ]\n",
      "Epoch [4000/6000] Loss: 1.61e-02 params: [0.34780946 1.0435835  3.3303082 ]\n",
      "Epoch 04023: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [4200/6000] Loss: 1.50e-02 params: [0.35582832 1.0438001  3.3570993 ]\n",
      "Epoch [4400/6000] Loss: 1.50e-02 params: [0.36270458 1.0439842  3.3796768 ]\n",
      "Epoch [4600/6000] Loss: 1.49e-02 params: [0.3692261 1.0441823 3.40086  ]\n",
      "Epoch [4800/6000] Loss: 1.49e-02 params: [0.37533155 1.0443848  3.420442  ]\n",
      "Epoch [5000/6000] Loss: 1.48e-02 params: [0.38096985 1.0445848  3.4382465 ]\n",
      "Epoch [5200/6000] Loss: 1.48e-02 params: [0.38610137 1.0447745  3.454141  ]\n",
      "Epoch [5400/6000] Loss: 1.48e-02 params: [0.3907577 1.044944  3.468075 ]\n",
      "Epoch 05528: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [5600/6000] Loss: 1.47e-02 params: [0.3941862 1.045054  3.4780257]\n",
      "Epoch [5800/6000] Loss: 1.47e-02 params: [0.3960848 1.0451266 3.4835677]\n",
      "Epoch [6000/6000] Loss: 1.47e-02 params: [0.39786568 1.0451822  3.488794  ]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# B_6, cGEM=25\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 25])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/B_6.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a70ef9ef-c246-43c8-9850-98459dbcbf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.39786568\n",
      "fm=1.0451822\n",
      "fp=3.488794\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b39834a-0660-4d98-9931-bfe87c45b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 8.49e-02 params: [0.5050558  0.39729923 0.5228013 ]\n",
      "Epoch [400/6000] Loss: 5.44e-02 params: [0.28866962 0.64838    0.7874279 ]\n",
      "Epoch [600/6000] Loss: 2.86e-02 params: [0.12326825 0.8718298  1.0541526 ]\n",
      "Epoch [800/6000] Loss: 2.06e-02 params: [0.03201599 0.99941725 1.2740092 ]\n",
      "Epoch [1000/6000] Loss: 1.92e-02 params: [3.3489587e-06 1.0425684e+00 1.4506532e+00]\n",
      "Epoch [1200/6000] Loss: 1.88e-02 params: [5.7395696e-06 1.0517249e+00 1.6102234e+00]\n",
      "Epoch [1400/6000] Loss: 1.84e-02 params: [6.6561875e-06 1.0515357e+00 1.7632649e+00]\n",
      "Epoch [1600/6000] Loss: 1.88e-02 params: [8.5718075e-06 1.0495172e+00 1.9118934e+00]\n",
      "Epoch [1800/6000] Loss: 1.79e-02 params: [7.1531995e-06 1.0472944e+00 2.0558414e+00]\n",
      "Epoch [2000/6000] Loss: 1.76e-02 params: [6.231159e-06 1.045121e+00 2.194561e+00]\n",
      "Epoch [2200/6000] Loss: 1.77e-02 params: [6.0027705e-06 1.0430385e+00 2.3274641e+00]\n",
      "Epoch [2400/6000] Loss: 1.73e-02 params: [6.9435914e-06 1.0410279e+00 2.4540184e+00]\n",
      "Epoch [2600/6000] Loss: 1.72e-02 params: [8.0145546e-06 1.0391148e+00 2.5737953e+00]\n",
      "Epoch [2800/6000] Loss: 1.71e-02 params: [1.2567547e-05 1.0373015e+00 2.6863554e+00]\n",
      "Epoch [3000/6000] Loss: 1.70e-02 params: [4.8273094e-05 1.0356042e+00 2.7912831e+00]\n",
      "Epoch [3200/6000] Loss: 1.72e-02 params: [1.3554378e-05 1.0340403e+00 2.8882086e+00]\n",
      "Epoch 03229: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [3400/6000] Loss: 1.69e-02 params: [2.4496745e-05 1.0331745e+00 2.9406817e+00]\n",
      "Epoch [3600/6000] Loss: 1.69e-02 params: [2.5273052e-05 1.0324448e+00 2.9856627e+00]\n",
      "Epoch [3800/6000] Loss: 1.69e-02 params: [2.6059159e-05 1.0317279e+00 3.0297937e+00]\n",
      "Epoch [4000/6000] Loss: 1.69e-02 params: [2.6906408e-05 1.0310287e+00 3.0727289e+00]\n",
      "Epoch [4200/6000] Loss: 1.69e-02 params: [2.7784577e-05 1.0303535e+00 3.1141264e+00]\n",
      "Epoch [4400/6000] Loss: 1.68e-02 params: [2.8699264e-05 1.0297099e+00 3.1536570e+00]\n",
      "Epoch [4600/6000] Loss: 1.68e-02 params: [2.9599569e-05 1.0291021e+00 3.1910100e+00]\n",
      "Epoch 04644: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [4800/6000] Loss: 1.68e-02 params: [2.0175694e-05 1.0287381e+00 3.2128689e+00]\n",
      "Epoch [5000/6000] Loss: 1.68e-02 params: [9.7830170e-06 1.0284520e+00 3.2306957e+00]\n",
      "Epoch [5200/6000] Loss: 1.68e-02 params: [2.0327266e-06 1.0281659e+00 3.2484245e+00]\n",
      "Epoch [5400/6000] Loss: 1.68e-02 params: [1.0216823e-05 1.0278798e+00 3.2659159e+00]\n",
      "Epoch [5600/6000] Loss: 1.68e-02 params: [1.0730424e-05 1.0276042e+00 3.2830174e+00]\n",
      "Epoch [5800/6000] Loss: 1.68e-02 params: [8.8504548e-06 1.0273391e+00 3.2995708e+00]\n",
      "Epoch 05986: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [6000/6000] Loss: 1.68e-02 params: [3.0922465e-06 1.0270900e+00 3.3149221e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# C_6, cGEM=25\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 25])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/C_6.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bd5574f-eae7-427e-ac2d-ef98eb375f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=3.0922465e-06\n",
      "fm=1.02709\n",
      "fp=3.314922\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b676e074-6607-40a4-8d88-40c9663b2d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 4.48e-02 params: [1.6728316e-04 7.1377987e-01 8.1500775e-01]\n",
      "Epoch [400/6000] Loss: 2.50e-02 params: [1.1544832e-04 9.0491277e-01 1.0427412e+00]\n",
      "Epoch [600/6000] Loss: 1.87e-02 params: [1.2795936e-05 1.0070107e+00 1.2373745e+00]\n",
      "Epoch [800/6000] Loss: 1.77e-02 params: [2.6355970e-05 1.0395043e+00 1.4057231e+00]\n",
      "Epoch [1000/6000] Loss: 1.69e-02 params: [1.1460307e-05 1.0446266e+00 1.5641309e+00]\n",
      "Epoch [1200/6000] Loss: 1.65e-02 params: [3.1087086e-05 1.0433371e+00 1.7186959e+00]\n",
      "Epoch [1400/6000] Loss: 1.62e-02 params: [1.1080165e-06 1.0411147e+00 1.8698295e+00]\n",
      "Epoch [1600/6000] Loss: 1.59e-02 params: [6.0590737e-06 1.0388160e+00 2.0167942e+00]\n",
      "Epoch [1800/6000] Loss: 1.57e-02 params: [5.2086827e-05 1.0366575e+00 2.1588781e+00]\n",
      "Epoch [2000/6000] Loss: 1.55e-02 params: [1.8706564e-06 1.0345603e+00 2.2954056e+00]\n",
      "Epoch [2200/6000] Loss: 1.53e-02 params: [7.4416967e-05 1.0325186e+00 2.4258394e+00]\n",
      "Epoch [2400/6000] Loss: 1.52e-02 params: [2.1860156e-05 1.0306063e+00 2.5497246e+00]\n",
      "Epoch [2600/6000] Loss: 1.51e-02 params: [1.9197581e-05 1.0287549e+00 2.6665580e+00]\n",
      "Epoch [2800/6000] Loss: 1.50e-02 params: [7.0794922e-05 1.0270635e+00 2.7759435e+00]\n",
      "Epoch [3000/6000] Loss: 1.49e-02 params: [3.3662582e-05 1.0254841e+00 2.8774464e+00]\n",
      "Epoch [3200/6000] Loss: 1.48e-02 params: [2.6056390e-05 1.0239966e+00 2.9706843e+00]\n",
      "Epoch [3400/6000] Loss: 1.47e-02 params: [2.1120173e-05 1.0226719e+00 3.0553422e+00]\n",
      "Epoch [3600/6000] Loss: 1.49e-02 params: [2.3781151e-05 1.0214661e+00 3.1311336e+00]\n",
      "Epoch [3800/6000] Loss: 1.47e-02 params: [3.9806422e-05 1.0204164e+00 3.1979167e+00]\n",
      "Epoch 03978: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [4000/6000] Loss: 1.46e-02 params: [1.3880172e-06 1.0195203e+00 3.2528322e+00]\n",
      "Epoch [4200/6000] Loss: 1.46e-02 params: [1.2095043e-06 1.0191467e+00 3.2791190e+00]\n",
      "Epoch [4400/6000] Loss: 1.46e-02 params: [4.0232871e-06 1.0187566e+00 3.3041482e+00]\n",
      "Epoch [4600/6000] Loss: 1.46e-02 params: [5.3213444e-07 1.0183971e+00 3.3276508e+00]\n",
      "Epoch [4800/6000] Loss: 1.46e-02 params: [6.1937317e-06 1.0180688e+00 3.3493741e+00]\n",
      "Epoch [5000/6000] Loss: 1.45e-02 params: [1.0572829e-05 1.0177724e+00 3.3691061e+00]\n",
      "Epoch [5200/6000] Loss: 1.45e-02 params: [1.3966137e-05 1.0175134e+00 3.3866839e+00]\n",
      "Epoch [5400/6000] Loss: 1.45e-02 params: [5.0256131e-07 1.0172864e+00 3.4020069e+00]\n",
      "Epoch [5600/6000] Loss: 1.46e-02 params: [1.0457979e-05 1.0171021e+00 3.4150438e+00]\n",
      "Epoch [5800/6000] Loss: 1.45e-02 params: [4.5615561e-06 1.0169532e+00 3.4258466e+00]\n",
      "Epoch [6000/6000] Loss: 1.45e-02 params: [2.3379729e-05 1.0168365e+00 3.4345367e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# D_6, cGEM=25\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 25])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/D_6.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52c093c8-95e6-433d-85f2-505649249c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=2.337973e-05\n",
      "fm=1.0168365\n",
      "fp=3.4345367\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "643f3eae-9156-493b-8041-99abaaddf3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 4.58e-02 params: [0.28231752 0.8803173  0.635285  ]\n",
      "Epoch [400/6000] Loss: 3.62e-02 params: [0.16487119 1.0341028  0.85014075]\n",
      "Epoch [600/6000] Loss: 3.17e-02 params: [0.16939445 1.1178463  1.0722492 ]\n",
      "Epoch [800/6000] Loss: 2.89e-02 params: [0.24322028 1.1464118  1.2832983 ]\n",
      "Epoch [1000/6000] Loss: 2.80e-02 params: [0.33965203 1.1700962  1.4854585 ]\n",
      "Epoch [1200/6000] Loss: 2.75e-02 params: [0.4406459 1.1903943 1.6820647]\n",
      "Epoch [1400/6000] Loss: 2.72e-02 params: [0.53454006 1.2068667  1.8744886 ]\n",
      "Epoch [1600/6000] Loss: 2.69e-02 params: [0.6164073 1.2201387 2.063457 ]\n",
      "Epoch [1800/6000] Loss: 2.66e-02 params: [0.6843642 1.2302531 2.2493243]\n",
      "Epoch [2000/6000] Loss: 2.64e-02 params: [0.7382651 1.2374345 2.432251 ]\n",
      "Epoch [2200/6000] Loss: 2.62e-02 params: [0.7793311 1.2420828 2.612287 ]\n",
      "Epoch [2400/6000] Loss: 2.60e-02 params: [0.8095381 1.2446824 2.7894268]\n",
      "Epoch [2600/6000] Loss: 2.58e-02 params: [0.8313026 1.2456174 2.9636283]\n",
      "Epoch [2800/6000] Loss: 2.56e-02 params: [0.8473857 1.244894  3.1347864]\n",
      "Epoch [3000/6000] Loss: 2.86e-02 params: [0.859897  1.2452967 3.3030107]\n",
      "Epoch [3200/6000] Loss: 2.52e-02 params: [0.86930376 1.2442565  3.468156  ]\n",
      "Epoch 03257: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [3400/6000] Loss: 2.51e-02 params: [0.87397474 1.2433004  3.5733497 ]\n",
      "Epoch [3600/6000] Loss: 2.50e-02 params: [0.878444  1.2428089 3.6551917]\n",
      "Epoch [3800/6000] Loss: 2.49e-02 params: [0.8820943 1.2421963 3.7373798]\n",
      "Epoch [4000/6000] Loss: 2.48e-02 params: [0.8849204 1.2415072 3.81972  ]\n",
      "Epoch [4200/6000] Loss: 2.47e-02 params: [0.88645023 1.2402648  3.9020467 ]\n",
      "Epoch [4400/6000] Loss: 2.50e-02 params: [0.8862783 1.2394897 3.9842353]\n",
      "Epoch [4600/6000] Loss: 2.46e-02 params: [0.8825246 1.2381026 4.0660696]\n",
      "Epoch [4800/6000] Loss: 2.37e-02 params: [0.8678859 1.2351325 4.147259 ]\n",
      "Epoch [5000/6000] Loss: 2.32e-02 params: [0.822526  1.2257553 4.2272487]\n",
      "Epoch [5200/6000] Loss: 2.29e-02 params: [0.769123  1.2136432 4.3059115]\n",
      "Epoch [5400/6000] Loss: 2.28e-02 params: [0.7377113 1.2056854 4.383687 ]\n",
      "Epoch [5600/6000] Loss: 2.27e-02 params: [0.7230926 1.2009972 4.4608946]\n",
      "Epoch [5800/6000] Loss: 2.26e-02 params: [0.717499  1.1981925 4.5376534]\n",
      "Epoch [6000/6000] Loss: 2.25e-02 params: [0.7162723 1.1961753 4.6139936]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# A_7, cGEM=50\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 50])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/A_7.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "666ac280-ef33-4ba8-b1b6-dd263697585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.7162723\n",
      "fm=1.1961753\n",
      "fp=4.6139936\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2ca6ae2-0af2-4194-9c9e-4dd2963e19b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 2.71e-02 params: [0.2605994  0.73631614 0.8144944 ]\n",
      "Epoch [400/6000] Loss: 2.14e-02 params: [0.09890895 0.92506933 1.0501057 ]\n",
      "Epoch [600/6000] Loss: 1.87e-02 params: [0.01353815 1.0254611  1.2641239 ]\n",
      "Epoch [800/6000] Loss: 1.73e-02 params: [2.5941745e-05 1.0588657e+00 1.4596648e+00]\n",
      "Epoch [1000/6000] Loss: 1.69e-02 params: [1.1090158e-06 1.0680144e+00 1.6483228e+00]\n",
      "Epoch [1200/6000] Loss: 2.05e-02 params: [1.6254202e-05 1.0688635e+00 1.8344663e+00]\n",
      "Epoch [1400/6000] Loss: 1.63e-02 params: [1.0582497e-06 1.0676007e+00 2.0187645e+00]\n",
      "Epoch [1600/6000] Loss: 1.69e-02 params: [8.5844964e-05 1.0658585e+00 2.2007790e+00]\n",
      "Epoch [1800/6000] Loss: 1.59e-02 params: [7.1451977e-05 1.0637051e+00 2.3799086e+00]\n",
      "Epoch [2000/6000] Loss: 1.57e-02 params: [4.8519909e-05 1.0616038e+00 2.5558939e+00]\n",
      "Epoch [2200/6000] Loss: 1.55e-02 params: [1.8857257e-04 1.0596741e+00 2.7287812e+00]\n",
      "Epoch [2400/6000] Loss: 1.54e-02 params: [2.4291538e-03 1.0577782e+00 2.8984351e+00]\n",
      "Epoch 02403: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [2600/6000] Loss: 1.53e-02 params: [0.00525202 1.0577009  2.9842153 ]\n",
      "Epoch [2800/6000] Loss: 1.52e-02 params: [0.00940357 1.0573411  3.0687866 ]\n",
      "Epoch [3000/6000] Loss: 1.52e-02 params: [0.01505372 1.0572585  3.1536894 ]\n",
      "Epoch [3200/6000] Loss: 1.51e-02 params: [0.02236702 1.0574918  3.2387776 ]\n",
      "Epoch [3400/6000] Loss: 1.50e-02 params: [0.03148726 1.058082   3.3239408 ]\n",
      "Epoch [3600/6000] Loss: 1.49e-02 params: [0.04265077 1.0589733  3.4090981 ]\n",
      "Epoch [3800/6000] Loss: 1.49e-02 params: [0.05573517 1.0602199  3.4941604 ]\n",
      "Epoch [4000/6000] Loss: 1.48e-02 params: [0.07051125 1.0618863  3.579057  ]\n",
      "Epoch [4200/6000] Loss: 1.47e-02 params: [0.08658192 1.0636345  3.663655  ]\n",
      "Epoch [4400/6000] Loss: 1.47e-02 params: [0.10448137 1.0657272  3.7480803 ]\n",
      "Epoch [4600/6000] Loss: 1.46e-02 params: [0.12381745 1.0679915  3.8322518 ]\n",
      "Epoch [4800/6000] Loss: 1.49e-02 params: [0.14302434 1.0704466  3.9159918 ]\n",
      "Epoch 04838: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [5000/6000] Loss: 1.45e-02 params: [0.15565966 1.071901   3.9661157 ]\n",
      "Epoch [5200/6000] Loss: 1.45e-02 params: [0.16613203 1.0732754  4.008472  ]\n",
      "Epoch [5400/6000] Loss: 1.45e-02 params: [0.17727417 1.0747423  4.051242  ]\n",
      "Epoch [5600/6000] Loss: 1.45e-02 params: [0.18898083 1.0763183  4.0943494 ]\n",
      "Epoch [5800/6000] Loss: 1.44e-02 params: [0.20116493 1.0779935  4.137734  ]\n",
      "Epoch [6000/6000] Loss: 1.44e-02 params: [0.21371973 1.0797468  4.1813307 ]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# B_7, cGEM=50\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 50])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/B_7.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56932aee-1ce6-4aac-aa8e-4a21a5c77ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.21371973\n",
      "fm=1.0797468\n",
      "fp=4.1813307\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f618c6c-d8ac-4a47-a9ac-3bea473ea1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 4.81e-02 params: [4.5267014e-05 9.2534548e-01 8.3996713e-01]\n",
      "Epoch [400/6000] Loss: 4.18e-02 params: [4.5648430e-05 1.0721710e+00 1.0709579e+00]\n",
      "Epoch [600/6000] Loss: 4.00e-02 params: [9.9648343e-05 1.1181811e+00 1.2747936e+00]\n",
      "Epoch [800/6000] Loss: 3.92e-02 params: [8.9004629e-05 1.1245725e+00 1.4682995e+00]\n",
      "Epoch [1000/6000] Loss: 3.87e-02 params: [3.3993630e-05 1.1240798e+00 1.6578362e+00]\n",
      "Epoch [1200/6000] Loss: 3.84e-02 params: [8.8652698e-05 1.1223423e+00 1.8446091e+00]\n",
      "Epoch [1400/6000] Loss: 3.81e-02 params: [7.5531309e-05 1.1199628e+00 2.0288970e+00]\n",
      "Epoch [1600/6000] Loss: 3.78e-02 params: [5.2844520e-05 1.1175499e+00 2.2108102e+00]\n",
      "Epoch [1800/6000] Loss: 3.76e-02 params: [7.8460260e-05 1.1150513e+00 2.3902857e+00]\n",
      "Epoch [2000/6000] Loss: 3.73e-02 params: [1.1856794e-04 1.1125671e+00 2.5672662e+00]\n",
      "Epoch [2200/6000] Loss: 3.73e-02 params: [7.7021730e-05 1.1099268e+00 2.7416835e+00]\n",
      "Epoch [2400/6000] Loss: 3.69e-02 params: [1.1264216e-04 1.1074125e+00 2.9135072e+00]\n",
      "Epoch [2600/6000] Loss: 3.69e-02 params: [6.6652719e-05 1.1049594e+00 3.0826554e+00]\n",
      "Epoch [2800/6000] Loss: 3.65e-02 params: [5.0889037e-05 1.1024715e+00 3.2490911e+00]\n",
      "Epoch [3000/6000] Loss: 3.64e-02 params: [4.2411506e-05 1.1000044e+00 3.4127350e+00]\n",
      "Epoch [3200/6000] Loss: 3.62e-02 params: [3.8490925e-05 1.0975307e+00 3.5735054e+00]\n",
      "Epoch [3400/6000] Loss: 3.68e-02 params: [4.9561197e-05 1.0951972e+00 3.7313197e+00]\n",
      "Epoch [3600/6000] Loss: 3.59e-02 params: [3.7612834e-05 1.0928397e+00 3.8860481e+00]\n",
      "Epoch [3800/6000] Loss: 3.58e-02 params: [3.4534147e-05 1.0903730e+00 4.0375643e+00]\n",
      "Epoch [4000/6000] Loss: 3.57e-02 params: [4.1784326e-05 1.0882405e+00 4.1857085e+00]\n",
      "Epoch [4200/6000] Loss: 3.55e-02 params: [4.6563408e-05 1.0860609e+00 4.3303280e+00]\n",
      "Epoch [4400/6000] Loss: 3.56e-02 params: [4.9619681e-05 1.0838329e+00 4.4712200e+00]\n",
      "Epoch [4600/6000] Loss: 3.54e-02 params: [4.6664012e-05 1.0816721e+00 4.6081815e+00]\n",
      "Epoch [4800/6000] Loss: 3.53e-02 params: [4.9697905e-05 1.0793552e+00 4.7409472e+00]\n",
      "Epoch [5000/6000] Loss: 3.63e-02 params: [2.1572701e-05 1.0777198e+00 4.8692713e+00]\n",
      "Epoch [5200/6000] Loss: 3.52e-02 params: [5.7751349e-05 1.0754135e+00 4.9928617e+00]\n",
      "Epoch 05216: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [5400/6000] Loss: 3.51e-02 params: [5.3796666e-05 1.0746789e+00 5.0584650e+00]\n",
      "Epoch [5600/6000] Loss: 3.51e-02 params: [4.7298799e-05 1.0737774e+00 5.1193261e+00]\n",
      "Epoch [5800/6000] Loss: 3.50e-02 params: [4.2766693e-05 1.0728699e+00 5.1803613e+00]\n",
      "Epoch [6000/6000] Loss: 3.50e-02 params: [3.9627961e-05 1.0719625e+00 5.2413144e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# C_7, cGEM=50\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 50])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/C_7.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68c87bad-4ae3-4c5a-88ee-8126991a0608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=3.962796e-05\n",
      "fm=1.0719625\n",
      "fp=5.2413144\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "981161f6-b3b1-4d86-8897-ad7e18bb86bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 4.25e-02 params: [0.07820171 0.8522696  0.9233523 ]\n",
      "Epoch [400/6000] Loss: 3.66e-02 params: [1.4644917e-04 9.7390127e-01 1.1185228e+00]\n",
      "Epoch [600/6000] Loss: 3.21e-02 params: [1.7809698e-04 1.0565631e+00 1.3189914e+00]\n",
      "Epoch [800/6000] Loss: 3.00e-02 params: [4.5290239e-05 1.0906216e+00 1.5173054e+00]\n",
      "Epoch [1000/6000] Loss: 2.91e-02 params: [3.0501360e-05 1.0954953e+00 1.7088870e+00]\n",
      "Epoch [1200/6000] Loss: 2.84e-02 params: [7.4825017e-05 1.0949583e+00 1.8972691e+00]\n",
      "Epoch [1400/6000] Loss: 2.78e-02 params: [9.4232164e-06 1.0940598e+00 2.0836623e+00]\n",
      "Epoch [1600/6000] Loss: 2.75e-02 params: [1.6702308e-05 1.0927895e+00 2.2678761e+00]\n",
      "Epoch [1800/6000] Loss: 2.72e-02 params: [2.2948831e-05 1.0909787e+00 2.4490020e+00]\n",
      "Epoch [2000/6000] Loss: 2.70e-02 params: [5.3093263e-06 1.0889142e+00 2.6267238e+00]\n",
      "Epoch [2200/6000] Loss: 2.68e-02 params: [8.3074738e-06 1.0867822e+00 2.8010757e+00]\n",
      "Epoch [2400/6000] Loss: 2.66e-02 params: [1.3332961e-05 1.0846399e+00 2.9721251e+00]\n",
      "Epoch [2600/6000] Loss: 2.73e-02 params: [5.2354408e-05 1.0826907e+00 3.1398144e+00]\n",
      "Epoch [2800/6000] Loss: 2.62e-02 params: [4.8139973e-06 1.0805686e+00 3.3042676e+00]\n",
      "Epoch [3000/6000] Loss: 2.61e-02 params: [5.1472296e-07 1.0784898e+00 3.4654160e+00]\n",
      "Epoch [3200/6000] Loss: 2.60e-02 params: [1.2300292e-03 1.0765902e+00 3.6231992e+00]\n",
      "Epoch [3400/6000] Loss: 2.58e-02 params: [0.00791152 1.0755674  3.777678  ]\n",
      "Epoch 03462: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [3600/6000] Loss: 2.57e-02 params: [0.01593051 1.0751818  3.8778963 ]\n",
      "Epoch [3800/6000] Loss: 2.57e-02 params: [0.02359384 1.075312   3.9546294 ]\n",
      "Epoch [4000/6000] Loss: 2.56e-02 params: [0.03293843 1.075651   4.031915  ]\n",
      "Epoch [4200/6000] Loss: 2.55e-02 params: [0.04382484 1.0761815  4.1096015 ]\n",
      "Epoch [4400/6000] Loss: 2.55e-02 params: [0.05601642 1.0768692  4.1875486 ]\n",
      "Epoch [4600/6000] Loss: 2.54e-02 params: [0.0692514 1.0776275 4.265644 ]\n",
      "Epoch [4800/6000] Loss: 2.53e-02 params: [0.08286657 1.0784628  4.343755  ]\n",
      "Epoch [5000/6000] Loss: 2.52e-02 params: [0.09673069 1.0792677  4.421756  ]\n",
      "Epoch [5200/6000] Loss: 2.52e-02 params: [0.1104916 1.080042  4.4995036]\n",
      "Epoch [5400/6000] Loss: 2.51e-02 params: [0.12396022 1.0807883  4.576861  ]\n",
      "Epoch [5600/6000] Loss: 2.53e-02 params: [0.13691568 1.0815392  4.6537004 ]\n",
      "Epoch [5800/6000] Loss: 2.49e-02 params: [0.14919591 1.0821706  4.7299123 ]\n",
      "Epoch [6000/6000] Loss: 2.49e-02 params: [0.16095254 1.0826944  4.805375  ]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# D_7, cGEM=50\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 50])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/D_7.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cad6e7eb-c529-474c-8998-9521414f2983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.16095254\n",
      "fm=1.0826944\n",
      "fp=4.805375\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e18f934a-f59f-46e2-a307-1a433ce72564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 3.74e-02 params: [0.6043511  0.32344198 0.6468745 ]\n",
      "Epoch [400/6000] Loss: 3.28e-02 params: [0.43261075 0.3487982  0.686183  ]\n",
      "Epoch [600/6000] Loss: 3.09e-02 params: [0.27822757 0.3844706  0.7422412 ]\n",
      "Epoch [800/6000] Loss: 2.93e-02 params: [0.14688946 0.42814782 0.8160681 ]\n",
      "Epoch [1000/6000] Loss: 2.74e-02 params: [0.04366217 0.47808263 0.9104142 ]\n",
      "Epoch [1200/6000] Loss: 2.64e-02 params: [1.2907520e-05 5.2314425e-01 1.0208669e+00]\n",
      "Epoch [1400/6000] Loss: 2.61e-02 params: [8.1086910e-07 5.6331956e-01 1.1432781e+00]\n",
      "Epoch [1600/6000] Loss: 2.59e-02 params: [6.2742838e-06 6.0324591e-01 1.2779883e+00]\n",
      "Epoch [1800/6000] Loss: 2.57e-02 params: [1.9307811e-06 6.4361244e-01 1.4232562e+00]\n",
      "Epoch [2000/6000] Loss: 2.56e-02 params: [0.01014536 0.6843445  1.5773246 ]\n",
      "Epoch [2200/6000] Loss: 2.54e-02 params: [0.04501729 0.72919285 1.7393323 ]\n",
      "Epoch [2400/6000] Loss: 2.53e-02 params: [0.10061579 0.7835278  1.9091048 ]\n",
      "Epoch [2600/6000] Loss: 2.51e-02 params: [0.17637913 0.85233927 2.0867078 ]\n",
      "Epoch [2800/6000] Loss: 2.49e-02 params: [0.27438924 0.94034237 2.2722967 ]\n",
      "Epoch [3000/6000] Loss: 2.50e-02 params: [0.39710444 1.0509021  2.465495  ]\n",
      "Epoch [3200/6000] Loss: 2.42e-02 params: [0.542782  1.1836805 2.6655748]\n",
      "Epoch [3400/6000] Loss: 2.38e-02 params: [0.7023763 1.3307542 2.8692884]\n",
      "Epoch [3600/6000] Loss: 2.35e-02 params: [0.85946256 1.4773561  3.071871  ]\n",
      "Epoch [3800/6000] Loss: 2.32e-02 params: [0.9978889 1.6088107 3.2685475]\n",
      "Epoch [4000/6000] Loss: 2.30e-02 params: [1.1086849 1.7156596 3.4573913]\n",
      "Epoch [4200/6000] Loss: 2.29e-02 params: [1.1916296 1.7965106 3.6385725]\n",
      "Epoch [4400/6000] Loss: 2.28e-02 params: [1.2502135 1.8541772 3.813787 ]\n",
      "Epoch [4600/6000] Loss: 2.59e-02 params: [1.2900876 1.8936733 3.9850152]\n",
      "Epoch 04632: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [4800/6000] Loss: 2.26e-02 params: [1.3066499 1.9085748 4.084181 ]\n",
      "Epoch [5000/6000] Loss: 2.26e-02 params: [1.3184724 1.9197633 4.169881 ]\n",
      "Epoch [5200/6000] Loss: 2.26e-02 params: [1.3283235 1.9282318 4.2562895]\n",
      "Epoch [5400/6000] Loss: 2.26e-02 params: [1.336022  1.9342159 4.3434286]\n",
      "Epoch [5600/6000] Loss: 2.25e-02 params: [1.3417337 1.9379903 4.4313   ]\n",
      "Epoch [5800/6000] Loss: 2.25e-02 params: [1.3456874 1.9397553 4.5198855]\n",
      "Epoch [6000/6000] Loss: 2.24e-02 params: [1.3481606 1.9399306 4.6091604]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# A_8, cGEM=100\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 100])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/A_8.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11780849-29f6-409d-8aac-c24a31d402e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=1.3481606\n",
      "fm=1.9399306\n",
      "fp=4.6091604\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3dfca1d7-d3ef-4929-ac3c-02bcc50fa8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 3.64e-02 params: [0.71418285 0.90425605 0.6922052 ]\n",
      "Epoch [400/6000] Loss: 3.23e-02 params: [0.5564974 1.1029782 0.924205 ]\n",
      "Epoch [600/6000] Loss: 3.04e-02 params: [0.46488833 1.2102067  1.1363939 ]\n",
      "Epoch [800/6000] Loss: 2.99e-02 params: [0.41762838 1.2535058  1.3369912 ]\n",
      "Epoch [1000/6000] Loss: 2.97e-02 params: [0.3941039 1.263306  1.5353714]\n",
      "Epoch [1200/6000] Loss: 2.95e-02 params: [0.3785048 1.2582526 1.7316681]\n",
      "Epoch [1400/6000] Loss: 2.93e-02 params: [0.363824  1.2499299 1.9259313]\n",
      "Epoch [1600/6000] Loss: 2.92e-02 params: [0.3493239 1.2421064 2.118414 ]\n",
      "Epoch [1800/6000] Loss: 2.90e-02 params: [0.33610722 1.2353823  2.3093739 ]\n",
      "Epoch [2000/6000] Loss: 2.88e-02 params: [0.3252038 1.2298285 2.4992485]\n",
      "Epoch [2200/6000] Loss: 2.87e-02 params: [0.31693283 1.2250463  2.6884403 ]\n",
      "Epoch [2400/6000] Loss: 2.86e-02 params: [0.31093076 1.2207614  2.8770626 ]\n",
      "Epoch [2600/6000] Loss: 2.85e-02 params: [0.3066906 1.2169273 3.0650942]\n",
      "Epoch [2800/6000] Loss: 2.84e-02 params: [0.30361074 1.2135419  3.252461  ]\n",
      "Epoch [3000/6000] Loss: 2.82e-02 params: [0.3014676 1.210917  3.439088 ]\n",
      "Epoch [3200/6000] Loss: 2.79e-02 params: [0.2989626 1.2093192 3.6248794]\n",
      "Epoch [3400/6000] Loss: 2.74e-02 params: [0.29262367 1.2095615  3.8094702 ]\n",
      "Epoch [3600/6000] Loss: 2.69e-02 params: [0.2734698 1.2080469 3.9922185]\n",
      "Epoch [3800/6000] Loss: 2.66e-02 params: [0.23279949 1.2006439  4.1728096 ]\n",
      "Epoch [4000/6000] Loss: 2.64e-02 params: [0.18182923 1.1784893  4.3512154 ]\n",
      "Epoch [4200/6000] Loss: 2.63e-02 params: [0.12934883 1.1534332  4.527638  ]\n",
      "Epoch [4400/6000] Loss: 2.61e-02 params: [0.08108037 1.1328807  4.7024717 ]\n",
      "Epoch [4600/6000] Loss: 2.59e-02 params: [0.04020676 1.1139376  4.875688  ]\n",
      "Epoch [4800/6000] Loss: 2.56e-02 params: [0.00786845 1.0989962  5.047765  ]\n",
      "Epoch [5000/6000] Loss: 2.59e-02 params: [4.7677438e-04 1.0926118e+00 5.2190423e+00]\n",
      "Epoch [5200/6000] Loss: 2.49e-02 params: [8.3479135e-06 1.0920067e+00 5.3906078e+00]\n",
      "Epoch [5400/6000] Loss: 2.47e-02 params: [4.9336348e-04 1.0914074e+00 5.5618329e+00]\n",
      "Epoch [5600/6000] Loss: 2.46e-02 params: [7.8689918e-06 1.0899014e+00 5.7327738e+00]\n",
      "Epoch 05741: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [5800/6000] Loss: 2.45e-02 params: [1.0234127e-03 1.0894411e+00 5.8785105e+00]\n",
      "Epoch [6000/6000] Loss: 2.45e-02 params: [1.0613452e-06 1.0878615e+00 5.9641261e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# B_8, cGEM=100\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 100])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/B_8.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51496a01-d8a3-4d16-8e38-50cf4e5f774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=1.0613452e-06\n",
      "fm=1.0878615\n",
      "fp=5.964126\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bb65882-2431-4cad-bdaf-47acd68da728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 4.49e-02 params: [0.72083026 0.90503883 0.96565694]\n",
      "Epoch [400/6000] Loss: 4.04e-02 params: [0.54167503 1.0545685  1.1624138 ]\n",
      "Epoch [600/6000] Loss: 3.90e-02 params: [0.41653588 1.1556623  1.3644805 ]\n",
      "Epoch [800/6000] Loss: 3.84e-02 params: [0.33109653 1.1924857  1.5612818 ]\n",
      "Epoch [1000/6000] Loss: 3.79e-02 params: [0.26311314 1.1859812  1.7538481 ]\n",
      "Epoch [1200/6000] Loss: 3.77e-02 params: [0.1960043 1.1600897 1.9447306]\n",
      "Epoch [1400/6000] Loss: 3.74e-02 params: [0.12249796 1.1265777  2.1345887 ]\n",
      "Epoch [1600/6000] Loss: 3.68e-02 params: [0.04048982 1.0892147  2.3232756 ]\n",
      "Epoch [1800/6000] Loss: 3.64e-02 params: [6.8466587e-05 1.0554086e+00 2.5111766e+00]\n",
      "Epoch [2000/6000] Loss: 3.60e-02 params: [9.9589713e-05 1.0453039e+00 2.6997955e+00]\n",
      "Epoch [2200/6000] Loss: 3.54e-02 params: [1.2371573e-04 1.0405143e+00 2.8884823e+00]\n",
      "Epoch [2400/6000] Loss: 3.48e-02 params: [1.8190891e-05 1.0349081e+00 3.0768495e+00]\n",
      "Epoch [2600/6000] Loss: 3.40e-02 params: [1.9819759e-05 1.0313709e+00 3.2650247e+00]\n",
      "Epoch [2800/6000] Loss: 3.34e-02 params: [6.4147258e-05 1.0306298e+00 3.4526725e+00]\n",
      "Epoch [3000/6000] Loss: 3.30e-02 params: [2.4600254e-04 1.0321914e+00 3.6398156e+00]\n",
      "Epoch [3200/6000] Loss: 3.28e-02 params: [3.5750208e-04 1.0330306e+00 3.8261042e+00]\n",
      "Epoch [3400/6000] Loss: 3.26e-02 params: [1.8503953e-04 1.0331706e+00 4.0113521e+00]\n",
      "Epoch [3600/6000] Loss: 3.24e-02 params: [5.9255035e-06 1.0329509e+00 4.1955647e+00]\n",
      "Epoch [3800/6000] Loss: 3.23e-02 params: [2.6463292e-04 1.0327864e+00 4.3787608e+00]\n",
      "Epoch [4000/6000] Loss: 3.21e-02 params: [1.8741957e-04 1.0324413e+00 4.5609436e+00]\n",
      "Epoch [4200/6000] Loss: 3.19e-02 params: [2.3154172e-04 1.0332406e+00 4.7421103e+00]\n",
      "Epoch [4400/6000] Loss: 3.17e-02 params: [9.0857793e-06 1.0342549e+00 4.9222813e+00]\n",
      "Epoch [4600/6000] Loss: 3.14e-02 params: [1.8150957e-05 1.0342791e+00 5.1014452e+00]\n",
      "Epoch [4800/6000] Loss: 3.12e-02 params: [5.8609927e-05 1.0346894e+00 5.2796330e+00]\n",
      "Epoch [5000/6000] Loss: 3.10e-02 params: [3.9058577e-05 1.0333622e+00 5.4568677e+00]\n",
      "Epoch [5200/6000] Loss: 3.06e-02 params: [3.6483954e-05 1.0337855e+00 5.6331377e+00]\n",
      "Epoch 05299: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [5400/6000] Loss: 3.04e-02 params: [4.8220150e-05 1.0329204e+00 5.7648954e+00]\n",
      "Epoch [5600/6000] Loss: 3.01e-02 params: [7.0518239e-05 1.0342274e+00 5.8532586e+00]\n",
      "Epoch [5800/6000] Loss: 2.98e-02 params: [1.3015002e-05 1.0342669e+00 5.9422359e+00]\n",
      "Epoch [6000/6000] Loss: 2.93e-02 params: [8.1247686e-05 1.0347699e+00 6.0317597e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# C_8, cGEM=100\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 100])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/C_8.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acd803d1-4001-4e04-aa9c-fb6a9c374d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=8.124769e-05\n",
      "fm=1.0347699\n",
      "fp=6.0317597\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e79cbf94-a241-438c-b2d4-ff2b716f73a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 4.73e-02 params: [0.68351567 0.7464161  0.6835755 ]\n",
      "Epoch [400/6000] Loss: 4.33e-02 params: [0.5024245  0.92828584 0.90245575]\n",
      "Epoch [600/6000] Loss: 4.10e-02 params: [0.37149906 1.0566998  1.1173292 ]\n",
      "Epoch [800/6000] Loss: 3.97e-02 params: [0.2847126 1.1221287 1.3234001]\n",
      "Epoch [1000/6000] Loss: 3.92e-02 params: [0.22658415 1.139551   1.5229172 ]\n",
      "Epoch [1200/6000] Loss: 3.89e-02 params: [0.17987637 1.1321906  1.7193387 ]\n",
      "Epoch [1400/6000] Loss: 3.87e-02 params: [0.1350523 1.1152338 1.9143351]\n",
      "Epoch [1600/6000] Loss: 3.85e-02 params: [0.08876246 1.0952141  2.1086087 ]\n",
      "Epoch [1800/6000] Loss: 3.83e-02 params: [0.04039805 1.0736406  2.3018265 ]\n",
      "Epoch [2000/6000] Loss: 3.80e-02 params: [1.0134383e-05 1.0512170e+00 2.4936280e+00]\n",
      "Epoch [2200/6000] Loss: 3.79e-02 params: [6.8149951e-05 1.0444387e+00 2.6847892e+00]\n",
      "Epoch [2400/6000] Loss: 3.88e-02 params: [1.2272191e-05 1.0440553e+00 2.8749630e+00]\n",
      "Epoch [2600/6000] Loss: 3.74e-02 params: [1.8945851e-05 1.0433867e+00 3.0638297e+00]\n",
      "Epoch 02697: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [2800/6000] Loss: 3.72e-02 params: [3.5897276e-05 1.0424173e+00 3.2037973e+00]\n",
      "Epoch [3000/6000] Loss: 3.71e-02 params: [1.6202306e-05 1.0421039e+00 3.2978113e+00]\n",
      "Epoch [3200/6000] Loss: 3.70e-02 params: [5.0720810e-05 1.0416903e+00 3.3920393e+00]\n",
      "Epoch [3400/6000] Loss: 3.68e-02 params: [6.6358494e-05 1.0410064e+00 3.4864440e+00]\n",
      "Epoch [3600/6000] Loss: 3.67e-02 params: [6.7643676e-05 1.0400634e+00 3.5809910e+00]\n",
      "Epoch [3800/6000] Loss: 3.66e-02 params: [1.4394418e-05 1.0388975e+00 3.6756604e+00]\n",
      "Epoch [4000/6000] Loss: 3.65e-02 params: [7.8537414e-06 1.0377293e+00 3.7704089e+00]\n",
      "Epoch [4200/6000] Loss: 3.65e-02 params: [3.4110493e-05 1.0366539e+00 3.8652010e+00]\n",
      "Epoch [4400/6000] Loss: 3.65e-02 params: [5.0554412e-05 1.0358777e+00 3.9600277e+00]\n",
      "Epoch [4600/6000] Loss: 3.64e-02 params: [1.9890002e-05 1.0343310e+00 4.0548081e+00]\n",
      "Epoch [4800/6000] Loss: 3.63e-02 params: [8.5165848e-06 1.0333077e+00 4.1495838e+00]\n",
      "Epoch 04995: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [5000/6000] Loss: 3.63e-02 params: [6.1436194e-06 1.0327014e+00 4.2433610e+00]\n",
      "Epoch [5200/6000] Loss: 3.63e-02 params: [1.8377754e-05 1.0315968e+00 4.2907920e+00]\n",
      "Epoch [5400/6000] Loss: 3.63e-02 params: [1.3561108e-05 1.0309826e+00 4.3384213e+00]\n",
      "Epoch [5600/6000] Loss: 3.62e-02 params: [1.5255060e-05 1.0304153e+00 4.3862143e+00]\n",
      "Epoch [5800/6000] Loss: 3.62e-02 params: [1.2896124e-05 1.0298461e+00 4.4341369e+00]\n",
      "Epoch [6000/6000] Loss: 3.62e-02 params: [1.1923095e-05 1.0292679e+00 4.4821653e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# D_8, cGEM=100\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 100])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/D_8.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e708b96-37cc-4fdc-ae50-bc5fea8a6377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=1.1923095e-05\n",
      "fm=1.0292679\n",
      "fp=4.4821653\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0dd97f4-477f-4193-888c-a99cee175dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 3.91e-02 params: [0.29420072 0.41367862 0.7536161 ]\n",
      "Epoch [400/6000] Loss: 3.25e-02 params: [0.28704786 0.4040827  0.77482384]\n",
      "Epoch [600/6000] Loss: 3.07e-02 params: [0.2871682  0.39984936 0.80585146]\n",
      "Epoch [800/6000] Loss: 2.95e-02 params: [0.29287073 0.39596477 0.8458658 ]\n",
      "Epoch [1000/6000] Loss: 2.81e-02 params: [0.29844272 0.39446396 0.89604   ]\n",
      "Epoch [1200/6000] Loss: 2.57e-02 params: [0.29935634 0.39682034 0.9570732 ]\n",
      "Epoch [1400/6000] Loss: 2.44e-02 params: [0.30071887 0.4035265  1.0298967 ]\n",
      "Epoch [1600/6000] Loss: 2.39e-02 params: [0.30562443 0.41473395 1.1152645 ]\n",
      "Epoch [1800/6000] Loss: 2.37e-02 params: [0.3127513  0.42909265 1.2114143 ]\n",
      "Epoch [2000/6000] Loss: 2.35e-02 params: [0.32169917 0.4480433  1.3177191 ]\n",
      "Epoch [2200/6000] Loss: 2.33e-02 params: [0.3332449 0.4731169 1.4336466]\n",
      "Epoch [2400/6000] Loss: 2.31e-02 params: [0.34830284 0.5060514  1.5589966 ]\n",
      "Epoch [2600/6000] Loss: 2.30e-02 params: [0.36808106 0.5489343  1.6934915 ]\n",
      "Epoch [2800/6000] Loss: 2.28e-02 params: [0.3940832  0.60492474 1.8372127 ]\n",
      "Epoch [3000/6000] Loss: 2.27e-02 params: [0.42811486 0.6781246  1.9903586 ]\n",
      "Epoch [3200/6000] Loss: 2.25e-02 params: [0.47221467 0.7736558  2.1534345 ]\n",
      "Epoch [3400/6000] Loss: 2.24e-02 params: [0.5283731  0.89616317 2.3261745 ]\n",
      "Epoch [3600/6000] Loss: 2.21e-02 params: [0.59752434 1.050136   2.5092282 ]\n",
      "Epoch [3800/6000] Loss: 2.19e-02 params: [0.6790858 1.235224  2.701929 ]\n",
      "Epoch [4000/6000] Loss: 2.16e-02 params: [0.7672677 1.4458867 2.9050167]\n",
      "Epoch [4200/6000] Loss: 2.13e-02 params: [0.8567654 1.6658703 3.1125278]\n",
      "Epoch [4400/6000] Loss: 2.11e-02 params: [0.93908423 1.8820045  3.3224292 ]\n",
      "Epoch [4600/6000] Loss: 2.62e-02 params: [1.0109603 2.0829008 3.5298107]\n",
      "Epoch [4800/6000] Loss: 2.06e-02 params: [1.071455  2.2617218 3.7333581]\n",
      "Epoch [5000/6000] Loss: 2.05e-02 params: [1.1224838 2.4203107 3.928146 ]\n",
      "Epoch [5200/6000] Loss: 2.04e-02 params: [1.1646746 2.5572538 4.1163335]\n",
      "Epoch [5400/6000] Loss: 2.03e-02 params: [1.1979271 2.6720605 4.2992926]\n",
      "Epoch [5600/6000] Loss: 2.02e-02 params: [1.226152  2.7725031 4.4759545]\n",
      "Epoch 05650: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [5800/6000] Loss: 2.02e-02 params: [1.2417309 2.8268528 4.5852365]\n",
      "Epoch [6000/6000] Loss: 2.01e-02 params: [1.2524132 2.867674  4.6720433]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# A_9, cGEM=200\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 200])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/A_9.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca138f8a-00a3-4384-93cb-ca2150ef3918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=1.2524132\n",
      "fm=2.867674\n",
      "fp=4.6720433\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e461812b-be6a-4887-b66c-9eac1f8ccc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 5.58e-02 params: [0.22805281 0.8269156  0.6977759 ]\n",
      "Epoch [400/6000] Loss: 3.78e-02 params: [0.18094525 0.81935054 0.70855266]\n",
      "Epoch [600/6000] Loss: 3.61e-02 params: [0.1461774 0.8093156 0.7234109]\n",
      "Epoch [800/6000] Loss: 3.46e-02 params: [0.12576893 0.7996408  0.74254346]\n",
      "Epoch [1000/6000] Loss: 3.31e-02 params: [0.12125962 0.792245   0.76581776]\n",
      "Epoch [1200/6000] Loss: 3.26e-02 params: [0.1250319  0.78792274 0.7930968 ]\n",
      "Epoch [1400/6000] Loss: 3.24e-02 params: [0.12467505 0.78383154 0.824344  ]\n",
      "Epoch [1600/6000] Loss: 3.24e-02 params: [0.12094213 0.77886313 0.859898  ]\n",
      "Epoch [1800/6000] Loss: 3.23e-02 params: [0.11613525 0.7733632  0.90017176]\n",
      "Epoch [2000/6000] Loss: 3.23e-02 params: [0.110992   0.76745516 0.9454927 ]\n",
      "Epoch [2200/6000] Loss: 3.22e-02 params: [0.1056527 0.7610654 0.996128 ]\n",
      "Epoch [2400/6000] Loss: 3.22e-02 params: [0.09998088 0.754034   1.0523206 ]\n",
      "Epoch 02518: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [2600/6000] Loss: 3.22e-02 params: [0.0951454  0.74781734 1.101402  ]\n",
      "Epoch 02608: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 02662: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 02713: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 02764: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch [2800/6000] Loss: 3.22e-02 params: [0.09420552 0.7465921  1.1109871 ]\n",
      "Epoch 02815: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 02866: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 02917: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 02968: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch [3000/6000] Loss: 3.22e-02 params: [0.09413397 0.74650145 1.1117026 ]\n",
      "Epoch 03019: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 03070: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 03121: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch 03172: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch [3200/6000] Loss: 3.22e-02 params: [0.09412886 0.746496   1.1117533 ]\n",
      "Epoch 03223: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch 03274: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch 03325: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Epoch [3400/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [3600/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [3800/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [4000/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [4200/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [4400/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [4600/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [4800/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [5000/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [5200/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [5400/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [5600/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [5800/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Epoch [6000/6000] Loss: 3.22e-02 params: [0.09412868 0.746496   1.1117533 ]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# B_9, cGEM=200\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 200])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/B_9.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12e855f9-6789-43ef-9b83-fa26f41ddd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.09412868\n",
      "fm=0.746496\n",
      "fp=1.1117533\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b030347-bd85-425e-8ac6-0e7f90ff660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 4.79e-02 params: [0.6564876  0.37302002 0.92355984]\n",
      "Epoch [400/6000] Loss: 4.35e-02 params: [0.46760243 0.44553512 1.1061772 ]\n",
      "Epoch [600/6000] Loss: 4.25e-02 params: [0.32548738 0.50054014 1.3021395 ]\n",
      "Epoch [800/6000] Loss: 4.19e-02 params: [0.21801956 0.49823233 1.5001209 ]\n",
      "Epoch [1000/6000] Loss: 4.13e-02 params: [0.13104206 0.4422909  1.6967347 ]\n",
      "Epoch [1200/6000] Loss: 4.19e-02 params: [0.04921328 0.34551865 1.891296  ]\n",
      "Epoch [1400/6000] Loss: 4.06e-02 params: [4.5345025e-04 2.2638616e-01 2.0840962e+00]\n",
      "Epoch [1600/6000] Loss: 4.05e-02 params: [6.8579189e-05 1.3693522e-01 2.2747157e+00]\n",
      "Epoch [1800/6000] Loss: 4.03e-02 params: [1.2693010e-05 7.9012394e-02 2.4645643e+00]\n",
      "Epoch [2000/6000] Loss: 4.03e-02 params: [9.4115094e-05 4.1012809e-02 2.6535532e+00]\n",
      "Epoch [2200/6000] Loss: 3.99e-02 params: [7.0837967e-05 2.0858584e-02 2.8429863e+00]\n",
      "Epoch 02258: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [2400/6000] Loss: 3.98e-02 params: [8.5060623e-05 1.6488846e-02 2.9663620e+00]\n",
      "Epoch [2600/6000] Loss: 3.97e-02 params: [2.2871434e-05 1.1750814e-02 3.0613084e+00]\n",
      "Epoch [2800/6000] Loss: 3.96e-02 params: [1.2361597e-06 9.7111342e-03 3.1568654e+00]\n",
      "Epoch [3000/6000] Loss: 3.95e-02 params: [6.4432665e-05 1.0348302e-02 3.2529283e+00]\n",
      "Epoch [3200/6000] Loss: 3.94e-02 params: [1.09222765e-05 1.34438500e-02 3.34944081e+00]\n",
      "Epoch [3400/6000] Loss: 3.92e-02 params: [9.3114417e-05 1.8653553e-02 3.4464028e+00]\n",
      "Epoch [3600/6000] Loss: 3.89e-02 params: [4.3577693e-05 2.7849913e-02 3.5441866e+00]\n",
      "Epoch [3800/6000] Loss: 3.85e-02 params: [6.1486142e-05 3.6795419e-02 3.6424673e+00]\n",
      "Epoch [4000/6000] Loss: 3.79e-02 params: [6.1895422e-05 5.2360982e-02 3.7421288e+00]\n",
      "Epoch [4200/6000] Loss: 3.81e-02 params: [3.3620483e-05 7.2064579e-02 3.8423271e+00]\n",
      "Epoch [4400/6000] Loss: 3.64e-02 params: [1.6707668e-04 1.0335967e-01 3.9433296e+00]\n",
      "Epoch [4600/6000] Loss: 3.52e-02 params: [3.6822417e-05 1.6215412e-01 4.0457182e+00]\n",
      "Epoch [4800/6000] Loss: 3.34e-02 params: [3.3634835e-05 2.6208284e-01 4.1473312e+00]\n",
      "Epoch [5000/6000] Loss: 3.21e-02 params: [6.4754175e-05 3.9054450e-01 4.2484522e+00]\n",
      "Epoch [5200/6000] Loss: 3.19e-02 params: [3.5586811e-05 4.9495840e-01 4.3490644e+00]\n",
      "Epoch [5400/6000] Loss: 3.17e-02 params: [8.0680329e-06 5.7232642e-01 4.4491634e+00]\n",
      "Epoch [5600/6000] Loss: 3.17e-02 params: [2.013833e-05 6.235525e-01 4.547816e+00]\n",
      "Epoch [5800/6000] Loss: 3.16e-02 params: [1.9068288e-05 6.5854216e-01 4.6457500e+00]\n",
      "Epoch 05849: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [6000/6000] Loss: 3.15e-02 params: [3.5310359e-06 6.7372054e-01 4.7066464e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# C_9, cGEM=200\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 200])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/C_9.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a01902f7-75b7-4313-bf21-c0b7d55527b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=3.5310359e-06\n",
      "fm=0.67372054\n",
      "fp=4.7066464\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5db7e82-c467-47cb-a0fd-6a01a12150b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 5.16e-02 params: [4.2835949e-05 2.7379885e-01 1.0415989e+00]\n",
      "Epoch [400/6000] Loss: 3.91e-02 params: [3.3537402e-05 2.5271040e-01 1.1187583e+00]\n",
      "Epoch [600/6000] Loss: 3.77e-02 params: [7.2671362e-05 2.3985708e-01 1.2187096e+00]\n",
      "Epoch [800/6000] Loss: 3.69e-02 params: [2.5995505e-05 2.2945167e-01 1.3371648e+00]\n",
      "Epoch [1000/6000] Loss: 3.61e-02 params: [1.5411121e-04 2.1462111e-01 1.4690057e+00]\n",
      "Epoch [1200/6000] Loss: 3.53e-02 params: [6.9194561e-05 1.9700786e-01 1.6107094e+00]\n",
      "Epoch [1400/6000] Loss: 3.44e-02 params: [1.5092734e-04 1.8434271e-01 1.7618666e+00]\n",
      "Epoch [1600/6000] Loss: 3.36e-02 params: [1.5242358e-04 1.7774621e-01 1.9192408e+00]\n",
      "Epoch [1800/6000] Loss: 3.31e-02 params: [1.3424155e-04 1.8208010e-01 2.0822740e+00]\n",
      "Epoch [2000/6000] Loss: 3.26e-02 params: [2.7407455e-05 1.8884973e-01 2.2476707e+00]\n",
      "Epoch [2200/6000] Loss: 3.22e-02 params: [1.1332741e-06 1.9881140e-01 2.4169238e+00]\n",
      "Epoch [2400/6000] Loss: 3.21e-02 params: [1.18053074e-04 2.12567165e-01 2.59076047e+00]\n",
      "Epoch [2600/6000] Loss: 3.13e-02 params: [9.9967059e-05 2.2319381e-01 2.7641783e+00]\n",
      "Epoch 02684: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [2800/6000] Loss: 3.12e-02 params: [4.3480548e-05 2.3734252e-01 2.8910933e+00]\n",
      "Epoch [3000/6000] Loss: 3.11e-02 params: [2.2130982e-05 2.4080189e-01 2.9787357e+00]\n",
      "Epoch [3200/6000] Loss: 3.10e-02 params: [6.1801373e-05 2.4444672e-01 3.0676556e+00]\n",
      "Epoch [3400/6000] Loss: 3.10e-02 params: [6.9014903e-05 2.4813049e-01 3.1576674e+00]\n",
      "Epoch [3600/6000] Loss: 3.09e-02 params: [7.1354501e-05 2.5176400e-01 3.2485976e+00]\n",
      "Epoch 03729: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [3800/6000] Loss: 3.09e-02 params: [3.0006988e-05 2.5503460e-01 3.3243053e+00]\n",
      "Epoch [4000/6000] Loss: 3.09e-02 params: [1.5610907e-05 2.5675011e-01 3.3706026e+00]\n",
      "Epoch [4200/6000] Loss: 3.09e-02 params: [1.20693985e-05 2.58508235e-01 3.41733599e+00]\n",
      "Epoch [4400/6000] Loss: 3.09e-02 params: [2.9843310e-05 2.6025862e-01 3.4644303e+00]\n",
      "Epoch [4600/6000] Loss: 3.08e-02 params: [1.8460869e-05 2.6196986e-01 3.5118225e+00]\n",
      "Epoch [4800/6000] Loss: 3.08e-02 params: [1.3941478e-05 2.6360157e-01 3.5594606e+00]\n",
      "Epoch [5000/6000] Loss: 3.08e-02 params: [3.0332540e-05 2.6510209e-01 3.6072986e+00]\n",
      "Epoch [5200/6000] Loss: 3.08e-02 params: [4.6795009e-05 2.6655453e-01 3.6553125e+00]\n",
      "Epoch 05268: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 05377: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch [5400/6000] Loss: 3.08e-02 params: [1.1654190e-05 2.6742929e-01 3.6863697e+00]\n",
      "Epoch 05469: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 05520: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 05571: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch [5600/6000] Loss: 3.08e-02 params: [4.9666767e-08 2.6756951e-01 3.6931212e+00]\n",
      "Epoch 05622: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 05673: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 05724: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 05775: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch [5800/6000] Loss: 3.08e-02 params: [1.8210393e-07 2.6758054e-01 3.6936486e+00]\n",
      "Epoch 05826: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch 05877: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch 05928: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch 05979: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch [6000/6000] Loss: 3.08e-02 params: [2.6652747e-09 2.6758054e-01 3.6936858e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# D_9, cGEM=200\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 200])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/D_9.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d97b8d7-b084-4ef2-b531-a8d13f6c8c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=2.6652747e-09\n",
      "fm=0.26758054\n",
      "fp=3.6936858\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d837b7d3-b51d-4518-828c-6ec23999a9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 5.53e-02 params: [0.6211792  0.9541699  0.80004954]\n",
      "Epoch [400/6000] Loss: 2.61e-02 params: [0.65996915 0.9454044  0.81204677]\n",
      "Epoch [600/6000] Loss: 2.52e-02 params: [0.6934031  0.942508   0.83154726]\n",
      "Epoch [800/6000] Loss: 2.48e-02 params: [0.713085  0.9417965 0.8564496]\n",
      "Epoch [1000/6000] Loss: 2.45e-02 params: [0.7236202  0.94333446 0.88692373]\n",
      "Epoch [1200/6000] Loss: 2.42e-02 params: [0.729478  0.9462702 0.9228288]\n",
      "Epoch [1400/6000] Loss: 2.42e-02 params: [0.7331982  0.9508036  0.96455014]\n",
      "Epoch [1600/6000] Loss: 2.37e-02 params: [0.73576415 0.9594624  1.014197  ]\n",
      "Epoch [1800/6000] Loss: 2.52e-02 params: [0.7376353 0.971547  1.0706433]\n",
      "Epoch [2000/6000] Loss: 2.36e-02 params: [0.73926693 0.98999685 1.1364013 ]\n",
      "Epoch [2200/6000] Loss: 2.29e-02 params: [0.7411771 1.0072819 1.2038621]\n",
      "Epoch [2400/6000] Loss: 2.27e-02 params: [0.7432766 1.0331638 1.2829492]\n",
      "Epoch [2600/6000] Loss: 2.25e-02 params: [0.74612296 1.0687711  1.3750373 ]\n",
      "Epoch [2800/6000] Loss: 2.24e-02 params: [0.74872893 1.0957426  1.463641  ]\n",
      "Epoch [3000/6000] Loss: 2.25e-02 params: [0.7517794 1.1352675 1.5692908]\n",
      "Epoch [3200/6000] Loss: 2.23e-02 params: [0.7563782 1.1717298 1.6756968]\n",
      "Epoch 03249: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [3400/6000] Loss: 2.22e-02 params: [0.7598379 1.2039467 1.7527546]\n",
      "Epoch [3600/6000] Loss: 2.22e-02 params: [0.7628444 1.2263662 1.8129752]\n",
      "Epoch [3800/6000] Loss: 2.22e-02 params: [0.76602435 1.251755   1.8773427 ]\n",
      "Epoch [4000/6000] Loss: 2.21e-02 params: [0.7696723 1.2807132 1.9457215]\n",
      "Epoch [4200/6000] Loss: 2.21e-02 params: [0.7739504 1.3139746 2.0179327]\n",
      "Epoch [4400/6000] Loss: 2.20e-02 params: [0.7790635 1.3525476 2.0938559]\n",
      "Epoch [4600/6000] Loss: 2.20e-02 params: [0.78523356 1.3977532  2.1733916 ]\n",
      "Epoch [4800/6000] Loss: 2.21e-02 params: [0.7926676 1.4505713 2.2563024]\n",
      "Epoch 04822: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [5000/6000] Loss: 2.19e-02 params: [0.797721  1.4845638 2.3041854]\n",
      "Epoch [5200/6000] Loss: 2.18e-02 params: [0.8025574 1.5169162 2.3475008]\n",
      "Epoch [5400/6000] Loss: 2.18e-02 params: [0.80783856 1.5523065  2.3918629 ]\n",
      "Epoch [5600/6000] Loss: 2.18e-02 params: [0.81356615 1.5907681  2.437137  ]\n",
      "Epoch [5800/6000] Loss: 2.17e-02 params: [0.81967944 1.6321871  2.4832072 ]\n",
      "Epoch [6000/6000] Loss: 2.18e-02 params: [0.8260633 1.6763363 2.5299864]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# A_1_0, cGEM=400\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 400])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/A_1_0.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9755940e-4905-4a27-b6e5-7cce5157fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.8260633\n",
      "fm=1.6763363\n",
      "fp=2.5299864\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79d428e2-0920-44e0-9790-6dd60d06d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GEM 2N\n",
    "# # B_1_0, cGEM=400\n",
    "\n",
    "# known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 400])\n",
    "# file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/B_1_0.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f346ca9-ee87-460c-84d5-209d50584b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PINN params\n",
    "# loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "# fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "# fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "# print('fg='+str(fg_param))\n",
    "# print('fm='+str(fm_param))\n",
    "# print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e1d78ff7-3019-42cc-a1d3-bcac9212e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 9.38e-02 params: [0.47478104 0.21631947 0.2992181 ]\n",
      "Epoch [400/6000] Loss: 4.86e-02 params: [0.4551506  0.21449915 0.30135778]\n",
      "Epoch [600/6000] Loss: 4.52e-02 params: [0.4282787  0.21326819 0.3049015 ]\n",
      "Epoch [800/6000] Loss: 4.42e-02 params: [0.40111268 0.21236305 0.30967474]\n",
      "Epoch [1000/6000] Loss: 4.33e-02 params: [0.37776372 0.21106382 0.31533742]\n",
      "Epoch [1200/6000] Loss: 4.26e-02 params: [0.3579247 0.208962  0.3216777]\n",
      "Epoch [1400/6000] Loss: 4.21e-02 params: [0.34193528 0.20598856 0.32868373]\n",
      "Epoch [1600/6000] Loss: 4.21e-02 params: [0.32982808 0.20223887 0.33649418]\n",
      "Epoch [1800/6000] Loss: 4.14e-02 params: [0.32114878 0.19779795 0.3452572 ]\n",
      "Epoch [2000/6000] Loss: 4.10e-02 params: [0.31524917 0.19274737 0.35508487]\n",
      "Epoch [2200/6000] Loss: 4.07e-02 params: [0.31133917 0.1871512  0.36608207]\n",
      "Epoch [2400/6000] Loss: 4.02e-02 params: [0.3083956  0.18107827 0.37835738]\n",
      "Epoch 02502: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [2600/6000] Loss: 3.99e-02 params: [0.30605158 0.17618325 0.3886338 ]\n",
      "Epoch [2800/6000] Loss: 3.96e-02 params: [0.30457634 0.1726231  0.39624476]\n",
      "Epoch [3000/6000] Loss: 3.93e-02 params: [0.30294964 0.16874018 0.4047257 ]\n",
      "Epoch [3200/6000] Loss: 3.91e-02 params: [0.3012655  0.1645786  0.41419056]\n",
      "Epoch [3400/6000] Loss: 3.88e-02 params: [0.29968536 0.16017558 0.42474645]\n",
      "Epoch [3600/6000] Loss: 3.87e-02 params: [0.29837707 0.15551199 0.43649152]\n",
      "Epoch [3800/6000] Loss: 3.85e-02 params: [0.29721576 0.15059899 0.44955072]\n",
      "Epoch 03969: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [4000/6000] Loss: 3.84e-02 params: [0.2961637  0.14582519 0.46291855]\n",
      "Epoch [4200/6000] Loss: 3.84e-02 params: [0.29568064 0.14305939 0.4709418 ]\n",
      "Epoch [4400/6000] Loss: 3.83e-02 params: [0.29513934 0.14006823 0.4798106 ]\n",
      "Epoch [4600/6000] Loss: 3.83e-02 params: [0.29453197 0.13685176 0.4896148 ]\n",
      "Epoch [4800/6000] Loss: 3.82e-02 params: [0.29389107 0.13340396 0.500441  ]\n",
      "Epoch [5000/6000] Loss: 3.82e-02 params: [0.2932337  0.12971617 0.5123783 ]\n",
      "Epoch [5200/6000] Loss: 4.02e-02 params: [0.2925583  0.12577836 0.52551997]\n",
      "Epoch 05221: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [5400/6000] Loss: 3.81e-02 params: [0.29213753 0.12344097 0.53350323]\n",
      "Epoch [5600/6000] Loss: 3.81e-02 params: [0.2917413 0.1211438 0.5414177]\n",
      "Epoch [5800/6000] Loss: 3.81e-02 params: [0.29129124 0.11864956 0.550078  ]\n",
      "Epoch [6000/6000] Loss: 3.80e-02 params: [0.2907903  0.11594417 0.55953336]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# C_1_0, cGEM=400\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 400])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/C_1_0.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2cd0df00-ffb4-4c1b-aad4-14563764604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.2907903\n",
      "fm=0.11594417\n",
      "fp=0.55953336\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be963e34-2c87-4141-a2d5-04e572151098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 1.03e-01 params: [0.683825   0.44247836 0.15808861]\n",
      "Epoch [400/6000] Loss: 4.62e-02 params: [0.63582784 0.4417698  0.16135718]\n",
      "Epoch [600/6000] Loss: 4.51e-02 params: [0.5854944  0.43899152 0.16508399]\n",
      "Epoch [800/6000] Loss: 4.46e-02 params: [0.547416   0.43571362 0.17039311]\n",
      "Epoch [1000/6000] Loss: 4.44e-02 params: [0.5223242  0.43211284 0.17745592]\n",
      "Epoch [1200/6000] Loss: 4.43e-02 params: [0.5075929  0.42811874 0.18623784]\n",
      "Epoch [1400/6000] Loss: 4.41e-02 params: [0.4991695  0.42349374 0.19657831]\n",
      "Epoch [1600/6000] Loss: 4.39e-02 params: [0.49386722 0.41820177 0.20844336]\n",
      "Epoch [1800/6000] Loss: 4.37e-02 params: [0.48954678 0.41237658 0.22189888]\n",
      "Epoch [2000/6000] Loss: 4.35e-02 params: [0.48562342 0.40607023 0.23702523]\n",
      "Epoch [2200/6000] Loss: 4.33e-02 params: [0.48251072 0.39970872 0.25403485]\n",
      "Epoch [2400/6000] Loss: 4.30e-02 params: [0.4800993  0.39386395 0.27313775]\n",
      "Epoch [2600/6000] Loss: 4.26e-02 params: [0.47767287 0.3887978  0.2943671 ]\n",
      "Epoch [2800/6000] Loss: 4.20e-02 params: [0.4750244  0.38449067 0.31763035]\n",
      "Epoch [3000/6000] Loss: 4.14e-02 params: [0.47276738 0.3819129  0.34388188]\n",
      "Epoch [3200/6000] Loss: 4.10e-02 params: [0.4699583  0.38143575 0.3735909 ]\n",
      "Epoch [3400/6000] Loss: 4.04e-02 params: [0.46639737 0.3796575  0.40499774]\n",
      "Epoch [3600/6000] Loss: 4.01e-02 params: [0.4632284  0.38141653 0.4440689 ]\n",
      "Epoch [3800/6000] Loss: 4.03e-02 params: [0.4589123  0.38348034 0.48750782]\n",
      "Epoch 03811: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [4000/6000] Loss: 3.97e-02 params: [0.45547885 0.38245028 0.5107812 ]\n",
      "Epoch [4200/6000] Loss: 3.95e-02 params: [0.45202005 0.3808065  0.53455716]\n",
      "Epoch [4400/6000] Loss: 3.94e-02 params: [0.4477704 0.3788434 0.5606852]\n",
      "Epoch [4600/6000] Loss: 3.92e-02 params: [0.44261405 0.37647668 0.58924675]\n",
      "Epoch [4800/6000] Loss: 3.90e-02 params: [0.43653727 0.37362203 0.6202725 ]\n",
      "Epoch [5000/6000] Loss: 3.90e-02 params: [0.4296684 0.3703081 0.6538199]\n",
      "Epoch [5200/6000] Loss: 3.87e-02 params: [0.4221194  0.36698276 0.69040936]\n",
      "Epoch [5400/6000] Loss: 3.85e-02 params: [0.41351578 0.36313802 0.7295048 ]\n",
      "Epoch [5600/6000] Loss: 3.83e-02 params: [0.40332144 0.35819554 0.76997554]\n",
      "Epoch [5800/6000] Loss: 3.82e-02 params: [0.39189878 0.3547017  0.8129016 ]\n",
      "Epoch [6000/6000] Loss: 3.83e-02 params: [0.3802185  0.3504016  0.85492235]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# D_1_0, cGEM=400\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 400])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/D_1_0.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6cc7e485-4dd2-4776-8983-d10d59d8f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.3802185\n",
      "fm=0.3504016\n",
      "fp=0.85492235\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2763b3aa-6903-460c-955d-04eac3f39fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 9.68e-02 params: [0.21735013 0.73309547 0.4018894 ]\n",
      "Epoch [400/6000] Loss: 3.78e-02 params: [0.3649252  0.7313171  0.40224242]\n",
      "Epoch [600/6000] Loss: 3.48e-02 params: [0.5022093  0.72917295 0.4025821 ]\n",
      "Epoch [800/6000] Loss: 3.25e-02 params: [0.6147469  0.7270237  0.40315446]\n",
      "Epoch [1000/6000] Loss: 3.08e-02 params: [0.7042914  0.72524947 0.4042224 ]\n",
      "Epoch [1200/6000] Loss: 2.94e-02 params: [0.7723617  0.72375864 0.40566435]\n",
      "Epoch [1400/6000] Loss: 2.86e-02 params: [0.82035285 0.722468   0.4071992 ]\n",
      "Epoch [1600/6000] Loss: 2.81e-02 params: [0.8498804  0.72148174 0.4088288 ]\n",
      "Epoch [1800/6000] Loss: 2.77e-02 params: [0.86494166 0.7207512  0.41058198]\n",
      "Epoch [2000/6000] Loss: 2.73e-02 params: [0.87078416 0.72022015 0.4125047 ]\n",
      "Epoch [2200/6000] Loss: 2.69e-02 params: [0.8720334 0.719873  0.4146657]\n",
      "Epoch [2400/6000] Loss: 2.66e-02 params: [0.87209034 0.7197296  0.41715497]\n",
      "Epoch [2600/6000] Loss: 2.63e-02 params: [0.87216526 0.7199638  0.4200685 ]\n",
      "Epoch [2800/6000] Loss: 2.60e-02 params: [0.8719026  0.7208024  0.42342603]\n",
      "Epoch [3000/6000] Loss: 2.57e-02 params: [0.8713038  0.7224497  0.42732942]\n",
      "Epoch [3200/6000] Loss: 2.56e-02 params: [0.87044734 0.72484565 0.43173006]\n",
      "Epoch [3400/6000] Loss: 2.54e-02 params: [0.86950934 0.7280648  0.43676162]\n",
      "Epoch [3600/6000] Loss: 2.53e-02 params: [0.8686327  0.7321179  0.44247863]\n",
      "Epoch [3800/6000] Loss: 2.60e-02 params: [0.8678151 0.737057  0.448958 ]\n",
      "Epoch 03935: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [4000/6000] Loss: 2.52e-02 params: [0.8672648  0.7420458  0.45513415]\n",
      "Epoch [4200/6000] Loss: 2.51e-02 params: [0.86672986 0.744591   0.4583621 ]\n",
      "Epoch [4400/6000] Loss: 2.51e-02 params: [0.8661308  0.7475564  0.46192855]\n",
      "Epoch [4600/6000] Loss: 2.50e-02 params: [0.8654258  0.7509859  0.46586397]\n",
      "Epoch [4800/6000] Loss: 2.49e-02 params: [0.8645735  0.7549488  0.47020638]\n",
      "Epoch [5000/6000] Loss: 2.49e-02 params: [0.86357534 0.7595238  0.47499943]\n",
      "Epoch [5200/6000] Loss: 2.48e-02 params: [0.86254776 0.7648192  0.4802999 ]\n",
      "Epoch [5400/6000] Loss: 2.47e-02 params: [0.86147225 0.7709858  0.48617336]\n",
      "Epoch 05539: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [5600/6000] Loss: 2.46e-02 params: [0.8606159  0.77737904 0.4920041 ]\n",
      "Epoch [5800/6000] Loss: 2.46e-02 params: [0.8600011  0.7814824  0.49557045]\n",
      "Epoch [6000/6000] Loss: 2.45e-02 params: [0.8592125  0.7861831  0.49950662]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# A_1_1, cGEM=800\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 800])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/A_1_1.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91d4dc57-6dd2-4745-8962-e6a712669d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.8592125\n",
      "fm=0.7861831\n",
      "fp=0.49950662\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "989d3246-3072-4d8c-b828-efa13b7eafb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 5.27e-02 params: [0.3758092  0.2949192  0.98760754]\n",
      "Epoch [400/6000] Loss: 3.35e-02 params: [0.43238807 0.27878323 0.9954401 ]\n",
      "Epoch [600/6000] Loss: 3.28e-02 params: [0.48435923 0.2610051  1.0106716 ]\n",
      "Epoch [800/6000] Loss: 3.24e-02 params: [0.5251186  0.24541634 1.0320812 ]\n",
      "Epoch [1000/6000] Loss: 3.22e-02 params: [0.5539131  0.23183152 1.0589483 ]\n",
      "Epoch [1200/6000] Loss: 3.43e-02 params: [0.5717651  0.21819669 1.088627  ]\n",
      "Epoch [1400/6000] Loss: 3.17e-02 params: [0.58183783 0.20770477 1.1236801 ]\n",
      "Epoch [1600/6000] Loss: 3.16e-02 params: [0.58694077 0.20013122 1.1637663 ]\n",
      "Epoch [1800/6000] Loss: 3.16e-02 params: [0.5887112  0.19820428 1.2122976 ]\n",
      "Epoch [2000/6000] Loss: 3.14e-02 params: [0.5892628  0.18535537 1.2545645 ]\n",
      "Epoch [2200/6000] Loss: 3.14e-02 params: [0.58787704 0.18423635 1.3134601 ]\n",
      "Epoch [2400/6000] Loss: 3.12e-02 params: [0.5869669  0.17029081 1.3656666 ]\n",
      "Epoch 02564: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [2600/6000] Loss: 3.12e-02 params: [0.5851934 0.1755947 1.4353762]\n",
      "Epoch [2800/6000] Loss: 3.11e-02 params: [0.5848609 0.1678216 1.4664959]\n",
      "Epoch [3000/6000] Loss: 3.11e-02 params: [0.5842076  0.16006035 1.5012332 ]\n",
      "Epoch [3200/6000] Loss: 3.11e-02 params: [0.58330464 0.15244098 1.5397869 ]\n",
      "Epoch [3400/6000] Loss: 3.10e-02 params: [0.5822175 0.1451722 1.5823739]\n",
      "Epoch [3600/6000] Loss: 3.10e-02 params: [0.5810245  0.13856049 1.629247  ]\n",
      "Epoch [3800/6000] Loss: 3.09e-02 params: [0.5798071  0.13300392 1.6806463 ]\n",
      "Epoch [4000/6000] Loss: 3.11e-02 params: [0.57859135 0.13136688 1.7386667 ]\n",
      "Epoch [4200/6000] Loss: 3.08e-02 params: [0.577879   0.12976506 1.799605  ]\n",
      "Epoch [4400/6000] Loss: 3.08e-02 params: [0.5774623  0.13512765 1.8682086 ]\n",
      "Epoch [4600/6000] Loss: 3.08e-02 params: [0.57711756 0.14177476 1.9396979 ]\n",
      "Epoch [4800/6000] Loss: 3.06e-02 params: [0.577347   0.14557502 2.0107293 ]\n",
      "Epoch [5000/6000] Loss: 3.06e-02 params: [0.577588   0.15646829 2.08805   ]\n",
      "Epoch [5200/6000] Loss: 3.09e-02 params: [0.5781844  0.16676188 2.1661587 ]\n",
      "Epoch 05377: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [5400/6000] Loss: 3.04e-02 params: [0.57914186 0.18433125 2.2455716 ]\n",
      "Epoch [5600/6000] Loss: 3.04e-02 params: [0.57995695 0.18985741 2.2859836 ]\n",
      "Epoch [5800/6000] Loss: 3.04e-02 params: [0.58085704 0.19683295 2.32784   ]\n",
      "Epoch [6000/6000] Loss: 3.04e-02 params: [0.58187705 0.20523748 2.3709762 ]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# B_1_1, cGEM=800\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 800])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/B_1_1.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8579ca06-6509-4e8b-aaf9-dd5b7ead6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.58187705\n",
      "fm=0.20523748\n",
      "fp=2.3709762\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f742fa18-18b4-404b-9a86-761c7e428ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 1.08e-01 params: [0.8260135  0.41114295 0.9430528 ]\n",
      "Epoch [400/6000] Loss: 5.21e-02 params: [0.7928977 0.4067505 0.9509587]\n",
      "Epoch [600/6000] Loss: 4.99e-02 params: [0.74853504 0.39511976 0.9597332 ]\n",
      "Epoch [800/6000] Loss: 4.91e-02 params: [0.7189876  0.37873325 0.9732042 ]\n",
      "Epoch [1000/6000] Loss: 4.86e-02 params: [0.70457983 0.36089984 0.9924309 ]\n",
      "Epoch [1200/6000] Loss: 4.81e-02 params: [0.70095867 0.34214687 1.0177875 ]\n",
      "Epoch [1400/6000] Loss: 4.79e-02 params: [0.70087934 0.32222325 1.0483556 ]\n",
      "Epoch [1600/6000] Loss: 4.73e-02 params: [0.700748   0.30260643 1.0846024 ]\n",
      "Epoch [1800/6000] Loss: 4.65e-02 params: [0.6993882  0.28438222 1.1269125 ]\n",
      "Epoch [2000/6000] Loss: 4.55e-02 params: [0.6962084  0.25596493 1.1641132 ]\n",
      "Epoch [2200/6000] Loss: 4.47e-02 params: [0.6907563  0.22843999 1.2084165 ]\n",
      "Epoch [2400/6000] Loss: 4.47e-02 params: [0.6849756  0.20311849 1.2629135 ]\n",
      "Epoch 02571: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [2600/6000] Loss: 4.42e-02 params: [0.68079525 0.17352298 1.3158137 ]\n",
      "Epoch [2800/6000] Loss: 4.42e-02 params: [0.6789032  0.15258203 1.3443466 ]\n",
      "Epoch [3000/6000] Loss: 4.41e-02 params: [0.67674065 0.12980972 1.3762294 ]\n",
      "Epoch [3200/6000] Loss: 4.40e-02 params: [0.6743195  0.10530662 1.4117477 ]\n",
      "Epoch [3400/6000] Loss: 4.40e-02 params: [0.6716705  0.07923768 1.4512004 ]\n",
      "Epoch [3600/6000] Loss: 4.39e-02 params: [0.668825   0.05185227 1.4948859 ]\n",
      "Epoch [3800/6000] Loss: 4.38e-02 params: [0.66591483 0.02653893 1.5456144 ]\n",
      "Epoch 03939: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [4000/6000] Loss: 4.38e-02 params: [0.6637111  0.00596053 1.5934272 ]\n",
      "Epoch [4200/6000] Loss: 4.38e-02 params: [6.6255784e-01 5.4746279e-06 1.6215661e+00]\n",
      "Epoch [4400/6000] Loss: 4.37e-02 params: [6.6160727e-01 1.4377756e-05 1.6519310e+00]\n",
      "Epoch [4600/6000] Loss: 4.37e-02 params: [6.6084474e-01 7.5223261e-07 1.6845483e+00]\n",
      "Epoch [4800/6000] Loss: 4.37e-02 params: [6.6021842e-01 1.8277951e-06 1.7194099e+00]\n",
      "Epoch [5000/6000] Loss: 4.36e-02 params: [6.5968192e-01 2.1934345e-06 1.7564657e+00]\n",
      "Epoch [5200/6000] Loss: 4.36e-02 params: [6.5919399e-01 4.6758591e-06 1.7956212e+00]\n",
      "Epoch 05258: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 05349: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch [5400/6000] Loss: 4.36e-02 params: [6.5892202e-01 6.3332459e-07 1.8195592e+00]\n",
      "Epoch 05418: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 05469: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 05520: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 05571: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch [5600/6000] Loss: 4.36e-02 params: [6.588840e-01 9.231837e-08 1.823004e+00]\n",
      "Epoch 05622: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 05673: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 05724: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 05775: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch [5800/6000] Loss: 4.36e-02 params: [6.5888262e-01 3.2757200e-08 1.8232385e+00]\n",
      "Epoch 05826: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch 05877: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch 05928: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch 05979: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Epoch [6000/6000] Loss: 4.36e-02 params: [6.588826e-01 2.060718e-09 1.823251e+00]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# C_1_1, cGEM=800\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 800])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/C_1_1.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99cfe4ee-f60f-48e9-b8b2-4bbc938187d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.6588826\n",
      "fm=2.060718e-09\n",
      "fp=1.823251\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "984480d0-5739-46dc-b4a7-41e3f2ac2fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/6000] Loss: 1.08e-01 params: [0.83922964 0.8409495  0.1655791 ]\n",
      "Epoch [400/6000] Loss: 3.80e-02 params: [0.832471   0.8402496  0.16655658]\n",
      "Epoch [600/6000] Loss: 3.41e-02 params: [0.8149891  0.8396711  0.16737406]\n",
      "Epoch [800/6000] Loss: 3.32e-02 params: [0.7944714  0.8402788  0.16885851]\n",
      "Epoch [1000/6000] Loss: 3.29e-02 params: [0.7740873  0.84109503 0.17083126]\n",
      "Epoch [1200/6000] Loss: 3.26e-02 params: [0.7549761  0.8419451  0.17324951]\n",
      "Epoch [1400/6000] Loss: 3.23e-02 params: [0.73773944 0.8426714  0.17601547]\n",
      "Epoch [1600/6000] Loss: 3.20e-02 params: [0.7227678  0.8432558  0.17908596]\n",
      "Epoch [1800/6000] Loss: 3.16e-02 params: [0.7100441  0.84382534 0.18248016]\n",
      "Epoch [2000/6000] Loss: 3.12e-02 params: [0.69929117 0.8445283  0.18623422]\n",
      "Epoch [2200/6000] Loss: 3.07e-02 params: [0.69009113 0.8454459  0.19040276]\n",
      "Epoch [2400/6000] Loss: 3.03e-02 params: [0.68213284 0.84658796 0.1950583 ]\n",
      "Epoch [2600/6000] Loss: 3.01e-02 params: [0.67563444 0.8479851  0.20030825]\n",
      "Epoch [2800/6000] Loss: 3.00e-02 params: [0.6708829  0.8496851  0.20625988]\n",
      "Epoch [3000/6000] Loss: 2.98e-02 params: [0.66783756 0.8516901  0.21300818]\n",
      "Epoch 03068: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [3200/6000] Loss: 2.98e-02 params: [0.6665599  0.85321856 0.21808942]\n",
      "Epoch [3400/6000] Loss: 2.98e-02 params: [0.66603744 0.8544988  0.22234371]\n",
      "Epoch [3600/6000] Loss: 2.97e-02 params: [0.66570663 0.85594755 0.22711153]\n",
      "Epoch [3800/6000] Loss: 2.97e-02 params: [0.6655297  0.8576055  0.23244983]\n",
      "Epoch [4000/6000] Loss: 2.96e-02 params: [0.66547143 0.8595228  0.23841634]\n",
      "Epoch [4200/6000] Loss: 3.01e-02 params: [0.6654915  0.8617597  0.24507403]\n",
      "Epoch 04228: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [4400/6000] Loss: 2.95e-02 params: [0.665515   0.8632373  0.24929215]\n",
      "Epoch [4600/6000] Loss: 2.95e-02 params: [0.66557544 0.86471564 0.25340042]\n",
      "Epoch [4800/6000] Loss: 2.95e-02 params: [0.6656511  0.86640257 0.25796086]\n",
      "Epoch [5000/6000] Loss: 2.95e-02 params: [0.66574067 0.8683322  0.26302344]\n",
      "Epoch [5200/6000] Loss: 2.94e-02 params: [0.6658491  0.8705436  0.26864377]\n",
      "Epoch [5400/6000] Loss: 2.94e-02 params: [0.6659827  0.8730805  0.27488336]\n",
      "Epoch [5600/6000] Loss: 2.94e-02 params: [0.6661459  0.8759923  0.28181168]\n",
      "Epoch [5800/6000] Loss: 2.94e-02 params: [0.66635257 0.87932926 0.28950262]\n",
      "Epoch [6000/6000] Loss: 2.93e-02 params: [0.66661245 0.88314515 0.29803842]\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# GEM 2N\n",
    "# D_1_1, cGEM=800\n",
    "\n",
    "known_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656, 800])\n",
    "file_G1, file_G2, file_P, file_D, file_G1_pred, file_G2_pred, file_P_pred, file_D_pred = drug_dose_param(pd.read_csv('csvs/D_1_1.csv'),known_param_2N,t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "594eb186-432b-4331-b0bf-e46ebf4e825f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg=0.66661245\n",
      "fm=0.88314515\n",
      "fp=0.29803842\n"
     ]
    }
   ],
   "source": [
    "# PINN params\n",
    "loss_param = pd.read_csv('loss_param_1c.csv')\n",
    "fg = loss_param['fg']; fm = loss_param['fm']; fp = loss_param['fp']; \n",
    "fg_param = fg[len(fg)-1]; fm_param = fm[len(fm)-1]; fp_param = fp[len(fp)-1];\n",
    "\n",
    "print('fg='+str(fg_param))\n",
    "print('fm='+str(fm_param))\n",
    "print('fp='+str(fp_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "efc3a166-a843-4f3c-8681-6bfe34e9fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Matrix:\n",
      "[[1.1131539e+00]\n",
      " [1.0119130e-05]\n",
      " [7.3131996e-01]\n",
      " [2.3360129e-01]\n",
      " [2.3339730e-05]\n",
      " [1.0746952e-05]\n",
      " [1.9328287e-05]\n",
      " [1.0087875e-05]\n",
      " [7.6116383e-01]\n",
      " [5.6980366e-01]\n",
      " [6.9864677e-06]\n",
      " [2.6647210e-01]\n",
      " [7.5329965e-01]\n",
      " [3.9786568e-01]\n",
      " [3.0922465e-06]\n",
      " [2.3379730e-05]\n",
      " [7.1627230e-01]\n",
      " [2.1371973e-01]\n",
      " [3.9627960e-05]\n",
      " [1.6095254e-01]\n",
      " [1.3481606e+00]\n",
      " [1.0613452e-06]\n",
      " [8.1247690e-05]\n",
      " [1.1923095e-05]\n",
      " [1.2524132e+00]\n",
      " [9.4128680e-02]\n",
      " [3.5310359e-06]\n",
      " [2.6652747e-09]\n",
      " [8.2606330e-01]\n",
      " [2.9079030e-01]\n",
      " [3.8021850e-01]\n",
      " [8.5921250e-01]\n",
      " [5.8187705e-01]\n",
      " [6.5888260e-01]\n",
      " [6.6661245e-01]]\n",
      "\n",
      "FM Matrix:\n",
      "[[9.4377910e-01]\n",
      " [4.3780208e-01]\n",
      " [7.6392233e-01]\n",
      " [3.7445915e-01]\n",
      " [8.0744940e-01]\n",
      " [8.1360054e-01]\n",
      " [8.7208277e-01]\n",
      " [7.0156340e-01]\n",
      " [1.0112497e+00]\n",
      " [1.0123497e+00]\n",
      " [9.7404370e-01]\n",
      " [9.9784740e-01]\n",
      " [1.0807875e+00]\n",
      " [1.0451822e+00]\n",
      " [1.0270900e+00]\n",
      " [1.0168365e+00]\n",
      " [1.1961753e+00]\n",
      " [1.0797468e+00]\n",
      " [1.0719625e+00]\n",
      " [1.0826944e+00]\n",
      " [1.9399306e+00]\n",
      " [1.0878615e+00]\n",
      " [1.0347699e+00]\n",
      " [1.0292679e+00]\n",
      " [2.8676740e+00]\n",
      " [7.4649600e-01]\n",
      " [6.7372054e-01]\n",
      " [2.6758054e-01]\n",
      " [1.6763363e+00]\n",
      " [1.1594417e-01]\n",
      " [3.5040160e-01]\n",
      " [7.8618310e-01]\n",
      " [2.0523748e-01]\n",
      " [2.0607180e-09]\n",
      " [8.8314515e-01]]\n",
      "\n",
      "FP Matrix:\n",
      "[[2.3675585 ]\n",
      " [1.2736831 ]\n",
      " [2.359938  ]\n",
      " [1.7241403 ]\n",
      " [2.0306816 ]\n",
      " [2.0993881 ]\n",
      " [2.0659506 ]\n",
      " [2.0993211 ]\n",
      " [2.3978627 ]\n",
      " [2.257992  ]\n",
      " [2.3772602 ]\n",
      " [2.1774185 ]\n",
      " [3.4738712 ]\n",
      " [3.488794  ]\n",
      " [3.314922  ]\n",
      " [3.4345367 ]\n",
      " [4.6139936 ]\n",
      " [4.1813307 ]\n",
      " [5.2413144 ]\n",
      " [4.805375  ]\n",
      " [4.6091604 ]\n",
      " [5.964126  ]\n",
      " [6.0317597 ]\n",
      " [4.4821653 ]\n",
      " [4.6720433 ]\n",
      " [1.1117533 ]\n",
      " [4.7066464 ]\n",
      " [3.6936858 ]\n",
      " [2.5299864 ]\n",
      " [0.55953336]\n",
      " [0.85492235]\n",
      " [0.49950662]\n",
      " [2.3709762 ]\n",
      " [1.823251  ]\n",
      " [0.29803842]]\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Path to the current notebook\n",
    "notebook_path = \"GEM_n.ipynb\"  # Replace with the actual filename of your notebook\n",
    "\n",
    "# Lists to store the extracted values\n",
    "fg_values, fm_values, fp_values = [], [], []\n",
    "\n",
    "# Load the notebook\n",
    "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Iterate over each cell in the notebook\n",
    "for cell in notebook.cells:\n",
    "    # Check if the cell contains output\n",
    "    if cell.cell_type == \"code\" and \"outputs\" in cell:\n",
    "        for output in cell.outputs:\n",
    "            # Ensure the output is text type and check for pattern\n",
    "            if output.output_type == \"stream\" and \"text\" in output:\n",
    "                output_text = output[\"text\"]\n",
    "\n",
    "                # Use regex to extract fg, fm, and fp values\n",
    "                fg_match = re.search(r\"fg=([\\d.eE+-]+)\", output_text)\n",
    "                fm_match = re.search(r\"fm=([\\d.eE+-]+)\", output_text)\n",
    "                fp_match = re.search(r\"fp=([\\d.eE+-]+)\", output_text)\n",
    "\n",
    "                # Append values if all three are found\n",
    "                if fg_match:\n",
    "                    fg_values.append(float(fg_match.group(1)))\n",
    "                if fm_match:\n",
    "                    fm_values.append(float(fm_match.group(1)))\n",
    "                if fp_match:\n",
    "                    fp_values.append(float(fp_match.group(1)))\n",
    "\n",
    "# Convert lists to separate matrices\n",
    "fg_matrix = np.array(fg_values).reshape(-1, 1)\n",
    "fm_matrix = np.array(fm_values).reshape(-1, 1)\n",
    "fp_matrix = np.array(fp_values).reshape(-1, 1)\n",
    "\n",
    "# Display the matrices\n",
    "print(\"FG Matrix:\")\n",
    "print(fg_matrix)\n",
    "print(\"\\nFM Matrix:\")\n",
    "print(fm_matrix)\n",
    "print(\"\\nFP Matrix:\")\n",
    "print(fp_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5b4a735d-9e96-4e1f-850c-d5edbd67fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_matrix_n = np.insert(fg_matrix, 28, fg_matrix[28])\n",
    "fm_matrix_n = np.insert(fm_matrix, 28, fm_matrix[28])\n",
    "fp_matrix_n = np.insert(fp_matrix, 28, fp_matrix[28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d95e9b0-5803-459c-b0bc-62dc8f516bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_matrix_2d = fg_matrix_n.reshape(9,4)\n",
    "fg_matrix_2d_T = fg_matrix_2d.T\n",
    "m_fg_matrix_2d_T = np.insert(fg_matrix_2d_T, 0, 0, axis=1)\n",
    "\n",
    "fm_matrix_2d = fm_matrix_n.reshape(9,4)\n",
    "fm_matrix_2d_T = fm_matrix_2d.T\n",
    "m_fm_matrix_2d_T = np.insert(fm_matrix_2d_T, 0, 0, axis=1)\n",
    "\n",
    "fp_matrix_2d = fp_matrix_n.reshape(9,4)\n",
    "fp_matrix_2d_T = fp_matrix_2d.T\n",
    "m_fp_matrix_2d_T = np.insert(fp_matrix_2d_T, 0, 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "54af24ac-4565-4777-b15b-c3aa73a114bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 1.1131539e+00 2.3339730e-05 7.6116383e-01 7.5329965e-01\n",
      "  7.1627230e-01 1.3481606e+00 1.2524132e+00 8.2606330e-01 8.5921250e-01]\n",
      " [0.0000000e+00 1.0119130e-05 1.0746952e-05 5.6980366e-01 3.9786568e-01\n",
      "  2.1371973e-01 1.0613452e-06 9.4128680e-02 8.2606330e-01 5.8187705e-01]\n",
      " [0.0000000e+00 7.3131996e-01 1.9328287e-05 6.9864677e-06 3.0922465e-06\n",
      "  3.9627960e-05 8.1247690e-05 3.5310359e-06 2.9079030e-01 6.5888260e-01]\n",
      " [0.0000000e+00 2.3360129e-01 1.0087875e-05 2.6647210e-01 2.3379730e-05\n",
      "  1.6095254e-01 1.1923095e-05 2.6652747e-09 3.8021850e-01 6.6661245e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(m_fg_matrix_2d_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2485f18a-2e5a-497d-a284-0fc086f57336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 9.4377910e-01 8.0744940e-01 1.0112497e+00 1.0807875e+00\n",
      "  1.1961753e+00 1.9399306e+00 2.8676740e+00 1.6763363e+00 7.8618310e-01]\n",
      " [0.0000000e+00 4.3780208e-01 8.1360054e-01 1.0123497e+00 1.0451822e+00\n",
      "  1.0797468e+00 1.0878615e+00 7.4649600e-01 1.6763363e+00 2.0523748e-01]\n",
      " [0.0000000e+00 7.6392233e-01 8.7208277e-01 9.7404370e-01 1.0270900e+00\n",
      "  1.0719625e+00 1.0347699e+00 6.7372054e-01 1.1594417e-01 2.0607180e-09]\n",
      " [0.0000000e+00 3.7445915e-01 7.0156340e-01 9.9784740e-01 1.0168365e+00\n",
      "  1.0826944e+00 1.0292679e+00 2.6758054e-01 3.5040160e-01 8.8314515e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(m_fm_matrix_2d_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "36762356-d4ed-437a-ac29-f06d7006f0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         2.3675585  2.0306816  2.3978627  3.4738712  4.6139936\n",
      "  4.6091604  4.6720433  2.5299864  0.49950662]\n",
      " [0.         1.2736831  2.0993881  2.257992   3.488794   4.1813307\n",
      "  5.964126   1.1117533  2.5299864  2.3709762 ]\n",
      " [0.         2.359938   2.0659506  2.3772602  3.314922   5.2413144\n",
      "  6.0317597  4.7066464  0.55953336 1.823251  ]\n",
      " [0.         1.7241403  2.0993211  2.1774185  3.4345367  4.805375\n",
      "  4.4821653  3.6936858  0.85492235 0.29803842]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(m_fp_matrix_2d_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f6f65-a773-45e0-8491-0c5c15545ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "119a7df3-ce05-4101-8d3e-c60e8bc385bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kd\n",
    "\n",
    "cGEM_all = np.array([0, 3.125, 6.25, 12.5, 25, 50, 100, 200, 400, 800])\n",
    "\n",
    "kn_param_2N = np.array([0.06542416, 0.65263357, 0.05263728, 0.01360368, 0.00667656])\n",
    "kn_param_4N = np.array([0.04754016, 1.06978193, 0.02534762, 0.00800557, 0.00394785])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9bcccfeb-ef46-4bad-9116-b2da7546edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kd\n",
    "\n",
    "kd_2N = kn_param_2N[3]*cGEM_all\n",
    "kd_4N = kn_param_4N[3]*cGEM_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ea109-2657-4274-9f08-58fa6fc4e2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d0418f7d-a207-4cee-b47e-485ea0b6fbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         0.0425115  0.085023   0.170046   0.340092   0.680184\n",
      "  1.360368   2.720736   5.441472  10.882944 ]\n"
     ]
    }
   ],
   "source": [
    "print(kd_2N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "04275aad-b69c-43b0-963a-894351a9b6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAJuCAYAAADGhrrQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOeUlEQVR4nO3de1xUdeLG8WcYZfDCxQsoCipFVzUtNcskdXNt29YsolqzQq3czAqysuyXtW2Z2bYmu2b30m2ztjXc1kpr19TIcjU1KyuTskQFJQswL6DD9/eHy+TIgAjDnDmcz/v14pWcc2bO43TqPHMu3+MyxhgBAABHibA6AAAACD0KAAAADkQBAADAgSgAAAA4EAUAAAAHogAAAOBAFAAAAByIAgAAgANRAAAAcCAKAAAADkQBAADAgSgAAHz279+v5s2by+VyaerUqVbHAdCIKAAAfD777DMdPHhQktS7d29rwwBoVBQAAD7r16/3/ZkCADRtFAAAPh9//LEkKT4+Xp07d7Y2DIBGRQEA4FNVAHr16mVtEACNjgIAQJJkjNEnn3wiicP/gBNQAABIkjZv3qyysjJJNReArVu3qn///nK5XIqKitJzzz0XwoQAgqmZ1QEAhIeqw/9S4AKwdOlSXXHFFSouLlanTp2Um5ur/v37hy4ggKDiCAAAST/fARAVFaWTTz7Zb96f/vQn/fKXv1RxcbEGDBigNWvWsPMHbI4CAEDSz0cAevToIbfbLUnas2ePrrjiCt1+++3yer0aN26cli5dqo4dO1qYFEAwcAoAgKSfjwBUHf7ftGmTLrnkEm3YsEGRkZGaNWuWrr/+egsTAggmjgAAUElJib777jtJhwrAv/71L/Xr108bNmxQYmKili1b1qCd/zPPPCOXy6W5c+cGKzKABuIIAAC/CwDffPNNLV68WMYYnX322XrttdeUmJjYoPdft26dJOmMM85o0PsACB4KAAC/IYAXLVokSfrlL3+pN954Q5GRkQ1+/3Xr1snj8eiUU05p8HsBCA5OAQDwHQFISUlRnz59JEkffPCBvvjiiwa/d2VlpT755BP17NlTzZrxnQMIFxQAAL4C0K9fP73++utKTEzUnj17dNFFF2nHjh11eo99+/bp/vvv1wknnKCoqCj16NFDr732mjZu3Ki9e/dy+B8IMxQAwOEOHDigzz//XNKhZwB07txZr7/+uqKiorRlyxZdfPHFKi8vr/U9du/erXPPPVe///3v1alTJ2VlZenEE0/U5ZdfrunTp0uSTj/99Eb/uwCoOwoA4HBffPGFKioqJP38EKB+/frp+eeflyStXLlS1157ba3vkZmZqTVr1uj555/X8uXLNX36dOXm5urhhx/2XflPAQDCCwUAcLjD7wA4/CmAI0eO1P/93/9Jkl566SVNnTo14Ov//e9/a8GCBRo7dqzGjBnjN++6666TJLndbp122mlBTg6gISgAgMNV3QHQtm1bJSUl+c174IEHdMkll0iSpkyZotzc3Gqvnz17tiTpjjvuqDYvLi5OEREROvnkk9WiRYtgRwfQABQAwOGqjgAc/u2/isvl0osvvqjevXvLGKOrr77ad09/lSVLlui4447TSSedVO313333nSorKzn8D4QhCgDgcFVHAAIVAElq1aqVXn/9dXXo0EF79+7VRRddpMLCQkmHRhDcvXu3unbtGvC1CxYskMQAQEA44qZcwOG+//77oy7TpUsXFRUVVZvevHlzSdLOnTurzdu3b58eeeQRSVwACIQjjgAAqLdWrVqpS5cu2rBhgz744APf9AMHDmjcuHG+0lD1gCEA4cNljDFWhwBgX7NmzdLNN9+sVq1aaeTIkYqOjtabb76p3bt3q7S0VB07dtTXX39tdUwAR+AUAIAGmTBhgkpLS/Xkk09q7ty56tChg4YNG6Ybb7xRffv25fA/EKY4AgAAgANxDQAAAA5EAQAAwIHC7hqAyspKbd++XdHR0XK5XFbHAQDANowx2r17tzp16qSIiNq/44ddAdi+fbuSk5OtjgEAgG0VFBRUG9r7SGFXAKKjoyUdCh8TE2NxGgAA7KOsrEzJycm+fWltwq4AVB32j4mJoQAAAFAPdTmFzkWAAAA4EAUAAAAHogAAAOBAYXcNQF15vV4dOHDA6hiWi4yMPOqtHgAAHMl2BcAYo6KiIpWUlFgdJSxEREQoJSVFkZGRVkcBANiI7QpA1c4/ISFBLVu2dPRgQVWDJhUWFqpLly6O/iwAAMfGVgXA6/X6dv7t2rWzOk5YiI+P1/bt23Xw4EE1b97c6jgAAJuw1cnjqnP+LVu2tDhJ+Kg69O/1ei1OAgCwE1sVgCoc6v4ZnwUAoD5sWQAAAEDDUAAAAHAgxxYAr9erZcuW6eWXX9ayZcsa/Rz6tGnT1K9fP0VHRyshIUEXX3yxNm7c6LdMt27d5HK5tHLlSr/p2dnZGjx4cKPmAwA4iyMLQG5urrp166YhQ4boyiuv1JAhQ9StWzfl5uY22jqXL1+uCRMmaOXKlfr3v/+tAwcOaNiwYdqzZ4/fclFRUbrzzjsbLQcAAJIDC0Bubq4yMjK0detWv+nbtm1TRkZGo5WAxYsXa/To0erevbt69eqlOXPmaMuWLVqzZo3fcuPGjdPKlSv11ltvNUoOAID1Qn0UOhBHFQCv16usrCwZY6rNq5qWnZ0dkn8RpaWlkqS2bdv6TU9JSdENN9ygyZMnq7KystFzAABCy4qj0IE4qgDk5eVV++Z/OGOMCgoKlJeX16g5KisrlZ2drXPOOUc9evSoNv+ee+7R5s2b9dJLLzVqDgBAaFl1FDoQRxWAwsLCoC5XXxMmTNBnn32mV155JeD8+Ph43X777br33ntVUVHRqFkAAKERTkehJYcVgMTExKAuVx833XST3njjDS1dulRJSUk1Ljdx4kTt27dPs2fPbrQsAIDQCZej0FUcVQDS0tKUlJRU4+h5LpdLycnJSktLC/q6jTG66aabtGDBAr377rtKSUmpdfnWrVtrypQpmjp1qnbv3h30PACA0AqXo9BVHFUA3G63cnJyJFUfQrfq95kzZ8rtdgd93RMmTNDf/vY3zZs3T9HR0SoqKlJRUZH27dtX42vGjRun2NhYzZs3L+h5AAChFQ5HoQ/nqAIgSenp6Zo/f746d+7sNz0pKUnz589Xenp6o6z3iSeeUGlpqQYPHqzExETfz9///vcaX9O8eXM98MAD2r9/f6NkAgCEjpVHoQOuzwS6GsFCZWVlio2NVWlpqWJiYvzm7d+/X5s3b1ZKSoqioqIatB6v16u8vDwVFhYqMTFRaWlpjfLNv7EF8zMBADSuqrsAJPldDFhVChr6RbS2feiRmtV7LTbndrsZXhcAEFJVR6GzsrL8LghMSkrSzJkzG+0odCCOLQAAAFghPT1dI0aMsPwoNAUAAIAQC4ej0I67CBAAAFAAAABwJAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4ECOLQDGa/Tjsh+14+Ud+nHZjzLe0I2I/PDDD8vlcik7O9s3rVu3bnK5XFq5cqXfstnZ2ZbfKwoAaHocORBQcW6x8rPyVb613DfNk+RRak6q4tPjG3Xdq1ev1lNPPaXTTjut2ryoqCjdeeedWr58eaNmAADAcUcAinOLtSFjg9/OX5LKt5VrQ8YGFecWN9q6f/rpJ40aNUrPPPOM2rRpU23+uHHjtHLlSr311luNlgEAAMlhBcB4jfKz8qVAR/v/Ny0/O7/RTgdMmDBBF154oYYOHRpwfkpKim644QZNnjxZlZWVjZIBAADJYQWgJK+k2jd/P0YqLyhXSV5J0Nf9yiuvaO3atZo2bVqty91zzz3avHmzXnrppaBnAACgiqMKQEVhRVCXq6uCggJlZWXppZdeUlRUVK3LxsfH6/bbb9e9996riorg5gAAoIqjCkBkYmRQl6urNWvWaOfOnTrjjDPUrFkzNWvWTMuXL9ef//xnNWvWTF6v12/5iRMnat++fZo9e3ZQcwAAUMVRBSAuLU6eJI/kqmEBl+RJ9iguLS6o6z3vvPP06aef6uOPP/b99O3bV6NGjdLHH39c7RnQrVu31pQpUzR16lTt3r07qFkAAJAcVgBcbpdSc1L/98uRMw/9I3VmqlzumhpC/URHR6tHjx5+P61atVK7du3Uo0ePgK8ZN26cYmNjNW/evKBmAQBAclgBkKT49Hh1n99dns4ev+meJI+6z+/e6OMA1FXz5s31wAMPaP/+/VZHAQA0QS5jTOiGwKuDsrIyxcbGqrS0VDExMX7z9u/fr82bNyslJeWoF9MdjfEaleSVqKKwQpGJkYpLiwv6N/9QCOZnAgCwt9r2oUdy5EiA0qHTAW0GVx+MBwAAJ3DcKQAAAEABAADAkSgAAAA4kC0LQJhdt2gpPgsAQH0ccwF47733NHz4cHXq1Ekul0v//Oc//eYbY3TvvfcqMTFRLVq00NChQ7Vp06aghG3evLkkae/evUF5v6agarjgIwcTAgCgNsd8F8CePXvUq1cvjR07Vunp6dXmP/LII/rzn/+suXPnKiUlRVOmTNH555+vzz//vMG3qbndbsXFxWnnzp2SpJYtW8rlst+te8FSWVmp4uJitWzZUs2aOfaGDgBAPRzzXuOCCy7QBRdcEHCeMUYzZ87UPffcoxEjRkiS/vrXv6pDhw765z//qd/+9rcNSyupY8eOkuQrAU4XERGhLl26OLoIAQCOXVC/Nm7evFlFRUV+z7uPjY1V//799eGHHwYsAOXl5Sov//kRvWVlZbWuw+VyKTExUQkJCTpw4EDwwttUZGSkIiJseSkHAMBCQS0ARUVFkqQOHTr4Te/QoYNv3pGmTZum+++//5jX5Xa7Oe8NAEA9Wf7VcfLkySotLfX9FBQUWB0JAIAmL6gFoOr8/I4dO/ym79ixwzfvSB6PRzExMX4/AACgcQW1AKSkpKhjx45asmSJb1pZWZn++9//6uyzzw7mqgAAQAMc8zUAP/30k/Lz832/b968WR9//LHatm2rLl26KDs7Ww8++KBOOOEE322AnTp10sUXXxzM3AAAoAGOuQB89NFHGjJkiO/3iRMnSpIyMzM1Z84cTZo0SXv27NG4ceNUUlKigQMHavHixTyqFgCAMOIyYTaW7LE8yxgAAPzsWPahlt8FAAAAQo8CAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAAwW9AHi9Xk2ZMkUpKSlq0aKFjj/+eD3wwAMyxgR7VQAAoJ6aBfsNp0+frieeeEJz585V9+7d9dFHH2nMmDGKjY3VLbfcEuzVAQCAegh6Afjggw80YsQIXXjhhZKkbt266eWXX9aqVauCvSoAAFBPQT8FMGDAAC1ZskRfffWVJGn9+vV6//33dcEFFwRcvry8XGVlZX4/AACgcQX9CMBdd92lsrIynXzyyXK73fJ6vZo6dapGjRoVcPlp06bp/vvvD3YMAABQi6AfAXj11Vf10ksvad68eVq7dq3mzp2rRx99VHPnzg24/OTJk1VaWur7KSgoCHYkAABwBJcJ8uX5ycnJuuuuuzRhwgTftAcffFB/+9vf9OWXXx719WVlZYqNjVVpaaliYmKCGQ0AgCbtWPahQT8CsHfvXkVE+L+t2+1WZWVlsFcFAADqKejXAAwfPlxTp05Vly5d1L17d61bt04zZszQ2LFjg70qAABQT0E/BbB7925NmTJFCxYs0M6dO9WpUyeNHDlS9957ryIjI4/6ek4BAABQP8eyDw16AWgoCgAAAPVj6TUAAAAg/FEAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwoGZWBwAAwGmM16gkr0QVhRWKTIxUXFqcXG5XSDNQAAAACKHi3GLlZ+WrfGu5b5onyaPUnFTFp8eHLAenAAAACJHi3GJtyNjgt/OXpPJt5dqQsUHFucUhy0IBAAAgBIzXKD8rXzKBZh76R352vow30ALBRwEAACAESvJKqn3z92Ok8oJyleSVhCQPBQAAgBCoKKwI6nINRQEAACAEIhMjg7pcQ1EAAAAIgbi0OHmSPFJNd/u5JE+yR3FpcSHJQwEAACAEXG6XUnNS//fLkTMP/SN1ZmrIxgOgAAAAECLx6fHqPr+7PJ09ftM9SR51n989pOMAMBAQAAAhFJ8er/Yj2jMSIAAATuNyu9RmcBtLM3AKAAAAB6IAAADgQBQAAAAciAIAAIADNUoB2LZtm6666iq1a9dOLVq0UM+ePfXRRx81xqoAAEA9BP0ugB9//FHnnHOOhgwZokWLFik+Pl6bNm1SmzbWXu0IAAB+FvQCMH36dCUnJ+uFF17wTUtJSQn2agAAQAME/RTAv/71L/Xt21eXXXaZEhISdPrpp+uZZ54J9moAAEADBL0AfPPNN3riiSd0wgkn6O2339b48eN1yy23aO7cuQGXLy8vV1lZmd8PAABoXC5jjAnmG0ZGRqpv37764IMPfNNuueUWrV69Wh9++GG15X//+9/r/vvvrza9tLRUMTExwYwGAECTVlZWptjY2DrtQ4N+BCAxMVGnnnqq37RTTjlFW7ZsCbj85MmTVVpa6vspKCgIdiQAAHCEoF8EeM4552jjxo1+07766it17do14PIej0cejyfgPAAA0DiCfgTg1ltv1cqVK/XQQw8pPz9f8+bN09NPP60JEyYEe1UAAKCegl4A+vXrpwULFujll19Wjx499MADD2jmzJkaNWpUsFcFAADqKegXATbUsVzAAAAAfmbpRYAAACD8UQAAAHAgCgAAAA5EAQAAwIEoAAAAOBAFAAAAB6IAAADgQBQAAAAciAIAAIADUQAAAHCgoD8NEACAUPB6vcrLy1NhYaESExOVlpYmt9ttdSzboAAAAGwnNzdXWVlZ2rp1q29aUlKScnJylJ6ebmEy++AUAADAVnJzc5WRkeG385ekbdu2KSMjQ7m5uRYlsxcKAADANrxer7KyshToQbZV07Kzs+X1ekMdzXYoAAAA28jLy6v2zf9wxhgVFBQoLy8vhKnsiQIAALCNwsLCoC7nZBQAAIBtJCYmBnU5J6MAAABsIy0tTUlJSXK5XAHnu1wuJScnKy0tLcTJ7IcCAACwDbfbrZycHEmqVgKqfp85cybjAdQBBQAAYCvp6emaP3++Onfu7Dc9KSlJ8+fPZxyAOnKZQPdSWKisrEyxsbEqLS1VTEyM1XEAAGGKkQCrO5Z9KCMBAgBsye12a/DgwVbHsC1OAQAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwoEYvAA8//LBcLpeys7Mbe1UAAKCOGrUArF69Wk899ZROO+20xlwNAAA4Ro1WAH766SeNGjVKzzzzjNq0adNYqwEAAPXQaAVgwoQJuvDCCzV06NDGWgUAAKinZo3xpq+88orWrl2r1atXH3XZ8vJylZeX+34vKytrjEgAAOAwQT8CUFBQoKysLL300kuKioo66vLTpk1TbGys7yc5OTnYkQAAwBFcxhgTzDf85z//qUsuuURut9s3zev1yuVyKSIiQuXl5X7zAh0BSE5OVmlpqWJiYoIZDQCAJq2srEyxsbF12ocG/RTAeeedp08//dRv2pgxY3TyySfrzjvv9Nv5S5LH45HH4wl2DAAAUIugF4Do6Gj16NHDb1qrVq3Url27atMBAIA1GAkQAAAHapS7AI60bNmyUKwGAADUEUcAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADhQM6sDAABQH8ZrVJJXoorCCkUmRiouLU4ut8vqWLZBAQAA2E5xbrHys/JVvrXcN82T5FFqTqri0+MtTGYfnAIAANhKcW6xNmRs8Nv5S1L5tnJtyNig4txii5LZCwUAAGAbxmuUn5UvmUAzD/0jPztfxhtoARyOAgAAsI2SvJJq3/z9GKm8oFwleSUhy2RXFAAAgG1UFFYEdTknowAAAGwjMjEyqMs5GQUAAGAbcWlx8iR5pJru9nNJnmSP4tLiQhnLligAAADbcLldSs1J/d8vR8489I/UmamMB1AHFAAAgK3Ep8er+/zu8nT2+E33JHnUfX53xgGoIwYCAgDYTnx6vNqPaM9IgA1AAQAA2JLL7VKbwW2sjmFbnAIAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA3EbIAA4mNfrVV5engoLC5WYmKi0tDS53W6rYyEEKAAA4FC5ubnKysrS1q1bfdOSkpKUk5Oj9PR0C5MhFDgFAAAOlJubq4yMDL+dvyRt27ZNGRkZys3NtSgZQoUCAAAO4/V6lZWVJWNMtXlV07Kzs+X1ekMdDSFEAQAAh8nLy6v2zf9wxhgVFBQoLy8vhKkQahQAAHCYwsLCoC4He6IAAIDDJCYmBnU52BMFAAAcJi0tTUlJSXK5Aj861+VyKTk5WWlpaSFOhlCiAACAw7jdbuXk5EhStRJQ9fvMmTMZD6CJowAAgAOlp6dr/vz56ty5s9/0pKQkzZ8/n3EAHMBlAt0HYqGysjLFxsaqtLRUMTExVscBgCaNkQCblmPZhzISIAA4mNvt1uDBg62OAQtwCgAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAcKegGYNm2a+vXrp+joaCUkJOjiiy/Wxo0bg70aAADQAEEvAMuXL9eECRO0cuVK/fvf/9aBAwc0bNgw7dmzJ9irAgAA9eQyxpjGXEFxcbESEhK0fPlynXvuuUddvqysTLGxsSotLVVMTExjRgMAoEk5ln1os8YOU1paKklq27ZtwPnl5eUqLy/3/V5WVtbYkQAAcLxGvQiwsrJS2dnZOuecc9SjR4+Ay0ybNk2xsbG+n+Tk5MaMBAAA1MinAMaPH69Fixbp/fffV1JSUsBlAh0BSE5O5hQAAADHKCxOAdx0001644039N5779W485ckj8cjj8fTWDEAAEAAQS8AxhjdfPPNWrBggZYtW6aUlJRgrwIAADRQ0AvAhAkTNG/ePL3++uuKjo5WUVGRJCk2NlYtWrQI9uoAAEA9BP0aAJfLFXD6Cy+8oNGjRx/19dwGCABA/Vh6DUAjDysAAACCgGcBAADgQBQAAAAciAIAAIADNfpQwACA8GW8RiV5JaoorFBkYqTi0uLkcge+mBtNCwUAAByqOLdY+Vn5Kt/682isniSPUnNSFZ8eb2EyhAKnAADAgYpzi7UhY4Pfzl+SyreVa0PGBhXnFluUDKFCAQAAhzFeo/ysfCnQXdv/m5afnS/j5bbupowCAAAOU5JXUu2bvx8jlReUqySvJGSZEHoUAABwmIrCiqAuB3uiAACAw0QmRgZ1OdgTBQAAHCYuLU6eJI9U091+LsmT7FFcWlwoYyHEKAAA4DAut0upOan/++XImYf+kTozlfEAmjgKAAA4UHx6vLrP7y5PZ4/fdE+SR93nd2ccAAdgICAAaCCv16u8vDwVFhYqMTFRaWlpcrvdVsc6qvj0eLUf0Z6RAB2KAgAADZCbm6usrCxt3brVNy0pKUk5OTlKT0+3MFnduNwutRncxuoYsACnAACgnnJzc5WRkeG385ekbdu2KSMjQ7m5uRYlA46OAgAA9eD1epWVlSVjqo+WVzUtOztbXq831NGAOqEAAEA95OXlVfvmfzhjjAoKCpSXlxfCVEDdUQAAoB4KCwuDuhwQahQAAKiHxMTEoC4HhBoFAADqIS0tTUlJSXK5At8y53K5lJycrLS0tBAnA+qGAgAA9eB2u5WTkyNJ1UpA1e8zZ860xXgAcCYKAADUU3p6uubPn6/OnTv7TU9KStL8+fNtMQ4AnMtlAt3DYqGysjLFxsaqtLRUMTExVscBgKOy60iAaHqOZR/KSIAA0EBut1uDBw+2OgZwTDgFAACAA1EAAABwIAoAAAAORAEAAMCBKAAAADgQBQAAAAeiAAAA4EAUAAAAHIgCAACAAzESIAA0kPEaleSVqKKwQpGJkYpLi5PLHfgpgUC4oAAAQAMU5xYrPytf5VvLfdM8SR6l5qQqPj3ewmRA7TgFAAD1VJxbrA0ZG/x2/pJUvq1cGzI2qDi32KJkwNFRAACgHozXKD8rXwr0PNX/TcvPzpfxhtUDVwEfCgAA1ENJXkm1b/5+jFReUK6SvJKQZQKOBdcAAAgLXq9XeXl5KiwsVGJiotLS0uR2u62OVaOKwoqgLgeEGgUAgOVyc3OVlZWlrVu3+qYlJSUpJydH6enpFiarWWRiZFCXA0KNUwAALJWbm6uMjAy/nb8kbdu2TRkZGcrNzbUoWe3i0uLkSfJINd3t55I8yR7FpcWFMhZQZxQAAJbxer3KysqSMdUvlKualp2dLa/XG+poR+Vyu5Sak/q/X46ceegfqTNTGQ8AYYsCAMAyeXl51b75H84Yo4KCAuXl5YUwVd3Fp8er+/zu8nT2+E33JHnUfX53xgFAWOMaAACWKSwsDOpyVohPj1f7Ee0ZCRC2QwEAYJnExMSgLmcVl9ulNoPbWB0DOCacAgBgmbS0NCUlJcnlCvxt2eVyKTk5WWlpaSFOBjR9FAAAlnG73crJyZGkaiWg6veZM2eG9XgAgF1RAABYKj09XfPnz1fnzp39piclJWn+/PlhOw4AYHcuE+j+GwuVlZUpNjZWpaWliomJsToOgBCx20iAQDg6ln0oFwECCAsRilAv9dIpOkWRilQEByiBRkUBAGC54txi5Wfl+z1cx5PkUWpOKvfSA42EAgA0IXY8jF6cW6wNGRuqPVa3fFu5NmRsYEAdoJFwjA1oInJzc9WtWzcNGTJEV155pYYMGaJu3bqF7Vj6kmS8RvlZ+dV2/odmHvpHfna+jDesLlUCmgQKANAE2PWBOiV5JX6H/asxUnlBuUrySkKWCXAKCgBgc3Z+oE5FYUVQlwNQdxQAwObs/ECdyMTIoC4HoO64CBCwucMflBOhCPVUT7VTO+3SLn2qT1WpymrLhYu4tDh5kjwq31Ye+DoA16G7AeLS4kIdDWjyKACAzVU9KCdNabpJNylBCb55O7VTszRLecoLywfquNwupeakHroLwCX/EvC/kYFTZ6byZD2gETASIHAEu91K5/V6ldEhQ7fsukWS5NLPO8tKVcoll/7S7i/6x45/hO3fI+A4AMkepc5kHADgWDASIFBPubm5ysrK8junnpSUpJycnLAdkz5CEbpJN0ny3/lXzatUpSZoQliPrBefHq/2I9qrJK9EFYUVikyMVFxaHN/8gUZEAQD+p+pWOpdxqZd6+c6jf7b1M2VkZITtg2lK8krk3lXzN/sIRUi7Di0Xzs+sd7ldYZ0PaGooAGgUdjyMnpWVpYFmYMDz6I+bx5Wdna0RI0aE3d+DW+kA1Ef4HhOEvF6vli1bppdfflnLli0Ly/u4A8nNzVVK1xRlD8nWs1c+q+wh2UrpmhK2g9FIh26lS9maovt1v+Llf865vdrr9/q9uhV041Y6AE0GRwDCVG5urrJvyVbbbW19h6J/6PyDZv55Zlgehq6Sm5urnEtz9Kge9f8WvW2nci7NkV5TWOYv3FZYp/Pohdu4lQ5A09DkC8DBioN6f/b7+vHrH9Xm+DYaeONANYsM77+2XXeiXq9XL457Ub/X76vNq/oW/ZdxfwnLw+iJxbXfIhehCHVQB6k4RIGOAbfSAaiPRjsF8Pjjj6tbt26KiopS//79tWrVqsZaVY0WTlqo11u+Lt0qtZnVRrpVer3l61o4aWHIs9TV4TvRmg5F/23c38LydEDesjyN3DVSUuBv0UZGV+y6QnnLwu8w+knxJwV1uVCLT49X9/nd5ens8ZvuSfLwND0AATVKAfj73/+uiRMn6r777tPatWvVq1cvnX/++dq5c2djrC6ghZMWqvUfW6utt63f9DbeNmr9x9ZhWwLsvBPdtWyXEpRQLXeVqm/Ru5btCnGyo4vqHBXU5awQnx6vs749S72W9tIp805Rr6W9dNbms9j5AwioUQrAjBkzdP3112vMmDE69dRT9eSTT6ply5Z6/vnnG2N11RysOKiKGYeueK5pJ1oxo0IHKw6GJM+xsPNOtJ3aBXW5UKo6j24CnkSXjIw8yeF/Hr3qVroOIzuozeA2HPYHUKOgF4CKigqtWbNGQ4cO/XklEREaOnSoPvzww2rLl5eXq6yszO+nod6f/b7aedvVuhNt522n92e/3+B1BZudd6I9B/cM6nKhVHUe3eVyqdpm45JcLhfn0QE0KUEvAN9//728Xq86dOjgN71Dhw4qKiqqtvy0adMUGxvr+0lOTm5whh+//jGoy4WSnXeibQe3lbed1/fwmSNVqlLedl61Hdw24HyrcR4dgJNYfjn85MmTNXHiRN/vZWVlDS4BbY6v22hidV0ulKp2oq5droBDt1aqUqadCcudqMvt0mlPn6bPLv1Mlar0y181Jn3Pp3uG9bdohqQF4BRBPwLQvn17ud1u7dixw2/6jh071LFjx2rLezwexcTE+P001MAbB2qXe1et30R3uXdp4I0DG7yuYKvaibrkqpa/aid62tOnhe0OKT49Xj1e66GoJP+L5VoktVCP13rY4ls059EBOEHQC0BkZKT69OmjJUuW+KZVVlZqyZIlOvvss4O9uoCaRTZT5MTIWneikRMjw3Y8ALvvROPT43X2t2f7X43+LVejA0A4aZQ94MSJE5WZmam+ffvqzDPP1MyZM7Vnzx6NGTOmMVYX0PBHhmuhFqpiRoXaeX++YO5H94+KnBip4Y8MD1mW+rD7oWge7AIA4a1RCsAVV1yh4uJi3XvvvSoqKlLv3r21ePHiahcGNrbhjwzXwQf9RwIcceOIsP3mfyR2ogCAxuIyxgS+8dkiZWVlio2NVWlpaVCuBwAAwCmOZR/K0wABAHAgCgAAAA5EAQAAwIEoAAAAOBAFAAAAB6IAAADgQBQAAAAciAIAAIADUQAAAHAgCgAAAA5EAQAAwIEoAAAAOBAFAAAABwq75+JWPZywrKzM4iQAANhL1b6zLg/6DbsCsHv3bklScnKyxUkAALCn3bt3KzY2ttZlXKYuNSGEKisrtX37dkVHR8vlcgXlPcvKypScnKyCgoKjPh853JDdGmS3BtmtQXZrNEZ2Y4x2796tTp06KSKi9rP8YXcEICIiQklJSY3y3jExMbbbQKqQ3RpktwbZrUF2awQ7+9G++VfhIkAAAByIAgAAgAM5ogB4PB7dd9998ng8Vkc5ZmS3BtmtQXZrkN0aVmcPu4sAAQBA43PEEQAAAOCPAgAAgANRAAAAcCAKAAAADkQBAADAgcJuJMCG+MMf/lCn5e69995GTnLs3nvvvTotd+655zZykmNn58/dztntvM3YOfuWLVvqtFyXLl0aOcmxc7vddVrO6/U2cpJjZ+fs4brNNKnbACMiItSpUyclJCTU+CQkl8ultWvXhjjZ0UVERPiefVBb9nDcuO3+uds5u523GbtmP3xHVJX98OeWGGPCNntERIS6du2qzMxMnX766TUuN2LEiBCmqhs7Zw/XbaZJHQG44IIL9O6776pv374aO3asfvOb3xz1YQjhok2bNoqOjtbo0aN19dVXq3379lZHqjM7f+52zm7nbcbO2V0ul5KSkjR69GgNHz5czZrZ53+jq1at0nPPPaecnBylpKRo7NixGjVqlNq0aWN1tKOyc/aw3WZME7Nt2zbz0EMPmRNPPNF07NjRTJo0yXz55ZdWxzqq8vJy88orr5hhw4aZFi1amEsvvdS89dZbprKy0upodWLXz90Y+2a38zZj5+yFhYXm4YcfNieddJLp0KGDue2228znn39udaxjsm/fPvPiiy+aX/ziF6Zly5bmiiuuMO+8847VserEjtnDdZtpcgXgcMuXLzejR4820dHRZsCAAWbv3r1WR6qT7777ztx///3muOOOM507dzZ33323OXDggNWx6syun7sx9s1u523Gztnz8vLM2LFjTXR0tOnfv795+umnjdfrtTrWMfnmm2/MkCFDTEREhNm1a5fVcY6JHbOH0zbTpAvA3r17zdy5c82ZZ55pWrRoYUpLS62OdEzsuHEbY+/P3c7ZjbHvNmOMvbMXFRXZLntBQYF54IEHzPHHH28SExPNnXfeaZviZefsVcJhm7HHyc5j9OGHH+r6669Xx44d9Ze//EWZmZnavn27LZ4VXV5ernnz5mno0KHq0aOH2rdvrzfffFNt27a1OtpR2flzt3N2O28zds4uSR988IGuu+46nXjiifrpp5/0+OOPKy4uzupYNaqoqNDf//53DRs2TCeccILWrl2rmTNnqqCgQA8//HD4nJsOwM7ZDxdW24wltaORTJ8+3ZxyyikmPj7eZGdnm/Xr11sdqc7++9//mhtuuMHExcWZ3r17m5ycHNt8k7Dz527n7HbeZuycffv27b7zuQkJCebWW281n376qdWx6qRt27ama9eu5t577zWbNm0ypaWlAX/CkZ2zh+s20+RuA+zSpYt+85vfKDIyssblZsyYEcJUdVOVPTMzU3369KlxuYsuuiiEqeqmKXzuds5u523GjtmbN2+uzp07KzMzUxdddJGaN28ecLnTTjstxMmO7vA7XA6/Da2KCfNbGKvYLXu4bjNNqgAMHjw44IZxOJfLpXfffTdEiequLreehevGbefP3c7Z7bzNNJXsNY1lEK7Zly9fXqflBg0a1MhJjp2ds4frNtOkCgAANLbvvvuuTst17dq1kZPALsJ1m6EAAIADVVZWKj8/Xzt37lRlZaXfvHAcgvlwds4eTuxx2eQx8nq9mjNnjpYsWRJwAwnHw7mH27Rpk5YuXRoweziOSV/Fzp+7nbNL9t1mJHtnLykp0apVqwJmv+aaayxKdXQrV67UlVdeqe+++y4sDkUfCztnl8Jrm2mSRwBuuukmzZkzRxdeeKESExOrneN97LHHLEp2dM8884zGjx+v9u3bq2PHjn7Zw3VM+ip2/tztnN3O24ydsy9cuFCjRo3STz/9pJiYmGrZf/jhBwvT1a5379468cQTdf/99wfc3mNjYy1KdnR2zh5220zobzxofO3atTNvvvmm1THqpUuXLubhhx+2Oka92Plzt3N2O28zds5+wgknmKysLLNnzx6roxyzli1bmk2bNlkdo17snD3ctpkmORBQZGSkUlNTrY5RLz/++KMuu+wyq2PUi50/dztnt/M2Y+fs27Zt0y233KKWLVtaHeWY9e/fX/n5+VbHqBc7Zw+3baZJFoDbbrtNOTk5NT5mNJxddtlleuedd6yOUS92/tztnN3O24yds59//vn66KOPrI5RLzfffLNuu+02zZkzR2vWrNEnn3zi9xPO7Jw93LaZJnkNwCWXXKKlS5eqbdu26t69e7VBF3Jzcy1KdnTTpk3TjBkzdOGFF6pnz57Vst9yyy0WJTs6O3/uds5u523Gztmfe+45/eEPf9CYMWMCZg/HQYyqBBqHweVyhfVgOlXsnD3ctpkmWQDGjBlT6/wXXnghREmOXUpKSo3zXC6XvvnmmxCmOTZ2/tztnN3O24yds9c2mFG474iOdl96OI9hYOfs4bbNNMkCAAAAatckrwEAAAC1owAAAOBAFAAAAByIAgAAgAM1yWcBAABqdvDgQW3YsEFFRUWSpI4dO+rUU0+t8Tn14cTO2cNNkysA33//vZ5//nl9+OGHfhvIgAEDNHr0aMXHx1ucsO42b96s/Px8JSYmqkePHlbHqbM9e/bo1Vdf9WUfOXKk2rVrZ3WsOrFzdsl+28yqVauq/bd69tln68wzz7Q42dEVFRXpv//9r1/2/v37q2PHjhYnq1llZaXuvfdePf744yotLfWbFxsbq5tuukn3339/rberWcXO2auE3TZj0RDEjWLVqlWmTZs2pnPnziYzM9NMmjTJTJo0yWRmZpqkpCTTtm1bs3r1aqtjBjR+/Hize/duY4wxe/fuNZdeeqmJiIgwLpfLREREmCFDhvjmh5tTTjnF7Nq1yxhjzJYtW0y3bt1MbGys6devn2nbtq1JSEgw33zzjcUpA7NzdjtvMzt27DADBw40LpfLdO3a1Zx55pnmzDPPNF27djUul8sMHDjQ7Nixw+qYAf30009m1KhRxu12m2bNmpmEhASTkJBgmjVrZtxut7nqqqvCZqz3I91xxx0mPj7ePPnkk2bz5s1m7969Zu/evWbz5s3mqaeeMgkJCWbSpElWxwzIztnDdZtpUgWgf//+Zty4caaysrLavMrKSjNu3Dhz1llnWZDs6CIiInz/w5s8ebJJSkoy7777rtmzZ495//33zfHHH2/uuusui1MG5nK5fNlHjRplBgwYYEpKSowxxuzevdsMHTrUjBw50sqINbJzdjtvM5deeqk5++yzzZdffllt3pdffmkGDBhgMjIyLEh2dNdee6054YQTzOLFi83Bgwd90w8ePGjefvttc+KJJ5rrrrvOwoQ169Chg1m8eHGN8xcvXmwSEhJCmKju7Jw9XLeZJlUAoqKizBdffFHj/C+++MJERUWFMFHdHb4j6tGjh5k3b57f/Ndff92ceOKJVkQ7qsOzH3fcceadd97xm79ixQqTnJxsRbSjairZ7bbNtG7d2qxdu7bG+R999JFp3bp1CBPVXVxcnFmxYkWN899//30TFxcXwkR117JlS/PJJ5/UOH/9+vWmVatWIUxUd3bOHq7bTPieLKmHjh07atWqVTXOX7VqlTp06BDCRMem6tnQRUVFOu200/zm9erVSwUFBVbEqpOq7Pv371diYqLfvM6dO6u4uNiKWHXSFLLbbZvxeDwqKyurcf7u3bvl8XhCmKjuKisrFRkZWeP8yMhIVVZWhjBR3Q0ePFi33367vv/++2rzvv/+e915550aPHhw6IPVgZ2zh+s206QuArz99ts1btw4rVmzRuedd55vZ79jxw4tWbJEzzzzjB599FGLU9ZsypQpatmypSIiIrR9+3Z1797dN2/Xrl1q1aqVhelqd95556lZs2YqKyvTxo0b/S5A++6778L6Qjo7Z7frNnPFFVcoMzNTjz32mM477zzFxMRIksrKyrRkyRJNnDhRI0eOtDhlYL/5zW80btw4Pffcczr99NP95q1bt07jx4/X8OHDLUpXuyeffFK//vWvlZiYqJ49e/r9P/LTTz/VqaeeqjfeeMPilIHZOXu4bjNNqgBMmDBB7du312OPPabZs2f7HqzgdrvVp08fzZkzR5dffrnFKQM799xztXHjRknSqaeeWu2BF2+99Zbf/9zDyX333ef3e+vWrf1+X7hwodLS0kIZqc7snN3O28yMGTNUWVmp3/72tzp48KDv21FFRYWaNWuma6+9NmzL+qxZs3TllVeqT58+atOmjRISEiRJO3fuVElJic4//3zNmjXL4pSBJScna/369Xr77be1cuVK39XoZ555ph566CENGzYsbK+it3P2cN1mmuzDgA4cOOA7VNS+fXvb3yP6zTffKDIyUklJSVZHgU3YYZspKyvTmjVr/G6L6tOnj++IQDj74osv/HZEVbcwnnzyyRYnQ7gKt22myRYAAEBggcZfGDBggPr162dxsqOzc/ZwQwEII/v27dOaNWvUtm1bnXrqqX7z9u/fr1dffVXXXHONRelqt379ei1cuFBt27bV5Zdfrvbt2/vmlZWVKTs7W88//7yFCetn9uzZ+v7773XvvfdaHSUgO28zh7PbAEwVFRX65z//GXBHNGLEiFov+LLSzp07demll2rFihXq0qWL33n0LVu26JxzztFrr73mO0QdTuycXQrTbSbk9x0goI0bN/oGQYmIiDDnnnuu2b59u29+UVGRiYiIsDBhzd5++20TGRlpunfvbrp06WLatWtn3n33Xd/8cM5+NL/4xS9MSkqK1TECsvM2c+QATF27drXNAEybNm0yxx13nImKijKDBg0yl19+ubn88svNoEGDTFRUlElNTTWbNm2yOmZAdh5/wc7Zw3WboQCEiYsvvthceOGFpri42GzatMlceOGFJiUlxXz33XfGmPD+n/nZZ59t7r77bmPMoQGXpk+fblq3bm0WLVpkjAnv7HZm523GzgMwDR061IwYMcKUlpZWm1daWmpGjBhhhg0bZkGyo7Pz+At2zh6u2wwFIEwkJCT4DXJRWVlpbrjhBtOlSxfz9ddfh/X/zGNiYkx+fr7ftJdeesm0atXKLFy4MKyz25mdtxk7D8DUokUL8+mnn9Y4/5NPPjEtWrQIYaK6a9eunVm2bFmN85cuXWratWsXwkR1Z+fs4brNhOc9Ew60b98+NWv2812ZLpdLTzzxhIYPH65Bgwbpq6++sjBd7Twej0pKSvymXXnllXr22Wd1xRVXaMGCBdYEOwZbt27VTz/9VG36gQMH9N5771mQ6OjsvM1I9h2AKS4uTt9++22N87/99lvFxcWFLM+xqBp/YcGCBX4DMZWVlWnBggUaM2ZM2I6/YOfsYbvNhLxyIKB+/fqZv/71rwHnTZgwwcTFxYXtt7lf/vKX5o9//GPAefPmzTPNmzcP2+zbt283/fr1MxEREcbtdpurr77a7wE64fwt2s7bjMvlMj179jSnn366ad26tZk/f77f/OXLl5vOnTtblK52U6ZMMW3atDEzZsww69evN0VFRaaoqMisX7/ezJgxw7Rt29bcd999VscMaP/+/eaGG24wkZGRJiIiwkRFRZmoqCgTERFhIiMjzfjx483+/futjhmQnbOH6zbDXQBhYtq0acrLy9Nbb70VcP6NN96oJ598MiyHGF2wYIHee+89PfbYYwHnz5s3T88884yWLl0a4mRHl5mZqY0bN2rWrFkqKSnRXXfdJZfLpXfeeUdt2rTRjh07lJiYGJafu523mfvvv9/v97POOkvnn3++7/c77rhDW7du1csvvxzqaHUyffp05eTkqKioyHckwxijjh07Kjs7W5MmTbI4Ye3sPP6CXbOH4zZDAYCjde7cWQsWLPA9f768vFyXXXaZCgoKtGTJEh04cECdOnXyjSoJHG7z5s1+O6KUlBSLEzmPMca3Q7WDcNpmKABwtNatW2vdunU64YQTfNMOHjyoyy67TN98843+9re/qXfv3hQANBlNZeyIKpGRkVq/fr1OOeUUq6PYDgXAJr7++mtdf/31evfdd62OcswyMzNVUFAQltlPO+003Xfffbr00kv9pleVgLVr12rr1q0UANRZQUGB7rvvvrAc+Oqrr77SsGHDtGXLFrlcLg0cOFAvv/yyOnXqJOnQoDrhesRr4sSJAafn5OToqquu8g0cNWPGjFDGqrNZs2Zp1apV+vWvf63f/va3evHFFzVt2jRVVlYqPT1df/jDH/wu6g0F7gKwiZ9++knLly+3Oka9dO7cWV27drU6RkAXXHCBnn766WrTmzVrpn/84x/q3bt36EPB1n744QfNnTvX6hgB3XnnnerRo4d27typjRs3Kjo6WgMHDtSWLVusjnZUM2fO1NKlS7Vu3Tq/H2OMvvjiC61bt04ff/yx1TEDevDBB3X33Xdr7969uvXWWzV9+nTdeuutGjVqlDIzM/Xss8/qgQceCHkujgCEiT//+c+1zt+2bZseffTRsGzmdnbw4EHt3bu3xguIDh48qG3btoVtgUHo/etf/6p1/jfffKPbbrstLP9b7dChg/7zn/+oZ8+ekg6dP7/xxhv11ltvaenSpWrVqlXYHgF4+OGH9fTTT+vZZ5/VL37xC9/05s2ba/369dVOZ4ST1NRUPfLII0pPT9f69evVp08fzZ07V6NGjZJ06ELqSZMmadOmTSHNRQEIExEREUpMTKxxPOiKigoVFRWF5X+YgJNERETI5XKptv91ulyusPxvNSYmRv/973+rnS+/6aab9Prrr2vevHkaPHhwWGaXpNWrV+uqq67S8OHDNW3aNDVv3twWBaBly5b68ssv1aVLF0mHrltYt26d73Hd3333nU499VTt2bMnpLk4BRAmunbtqscee0ybN28O+PPmm29aHbFGa9eu1ebNm32/v/jiizrnnHOUnJysgQMH6pVXXrEwXcMUFBRo7NixVsdAGElMTFRubq4qKysD/qxdu9bqiDU6+eST9dFHH1WbPmvWLI0YMUIXXXSRBanqrl+/flqzZo2Ki4vVt29fffbZZ7a4A6Bjx476/PPPJUmbNm2S1+v1/S5JGzZssOQhRhSAMNGnTx+tWbOmxvlH+8ZhpTFjxujrr7+WJD377LP63e9+p759++r//u//1K9fP11//fVheUFUXYTz+VxYw87/rV5yySU1jq0wa9YsjRw5MmyzV2ndurXmzp2ryZMna+jQoWF7tOJwo0aN0jXXXKPrr79e559/viZNmqTbb79dTz75pJ566indcMMNuuSSS0Kei1MAYeLzzz/X3r171bdv34DzDxw4oO3bt4flueiWLVvqiy++UNeuXXXGGWdo/Pjxuv76633z582bp6lTp2rDhg0WpgzMzudzYY28vDzt2bNHv/rVrwLO37Nnjz766CMNGjQoxMmcZ+vWrVqzZo2GDh2qVq1aWR2nRpWVlXr44Yf14YcfasCAAbrrrrv097//XZMmTdLevXs1fPhwzZo1K+R/BwoAGqx9+/Z6++231adPH3Xo0EHvvPOOevXq5Zv/9ddfq2fPntq7d6+FKQOz8/lcAGgITgGgwS644AI98cQTkqRBgwZp/vz5fvNfffVVpaamWhHtqOx8PhcAGiK0ow6g3u6++24VFRWF5bn06dOn65xzztGgQYPUt29f/elPf9KyZct0yimnaOPGjVq5cmXYPhGw6nzuiBEjAs4P5/O5ANAQFACb2LZtmwoKCqyOEVCnTp20bt06Pfzww1q4cKGMMVq1apUKCgp0zjnnaMWKFTVe22C1O+64o9Zbb1JTU8PyIUYA0FBcAwAAgANxDQAAAA5EAQgjX3zxhV544QV9+eWXkqQvv/xS48eP19ixY8PyQToAAPviFECYWLx4sUaMGKHWrVtr7969WrBgga655hr16tVLlZWVWr58ud555x2/MbABAKgvCkCYGDBggH7xi1/owQcf1CuvvKIbb7xR48eP19SpUyVJkydP1po1a/TOO+9YnBQA0BRQAMJEbGys1qxZo9TUVFVWVsrj8WjVqlU6/fTTJUmfffaZhg4dqqKiIouTAgCaAq4BCCNVD7WIiIhQVFSUYmNjffOio6NVWlpqVTQAQBNDAQgT3bp183sW9Icffuh7dKQkbdmyRYmJiVZEAwA0QQwEFCbGjx/vN958jx49/OYvWrSICwABAEHDNQAAADgQpwAAAHAgCgAAAA5EAQAAwIEoAAAAOBAFAIAl5syZI5fLJZfLpW+//dbqOIDjcBsgUItly5ZpyJAh1aa73W7FxMQoNjZWycnJ6tOnjwYOHKjhw4crMjLSgqQAcGw4AgDUg9fr1Y8//qhvv/1WeXl5mjlzpjIyMpSUlKQHH3xQBw8etDoiANSKIwBAHY0fP1433nij7/effvpJP/74oz755BMtWbJE//nPf1RcXKwpU6Zo4cKFeuONNxQfH29hYgCoGQUAqKOEhIRqIzRK0gUXXKA777xTn3/+ua666iqtW7dOq1at0iWXXKJ3332XUwIAwhKnAIAgOfXUU7VixQrfExxXrFihxx9/3OJUABAYBQAIohYtWujFF1/0Pdnx0Ucf1YEDB/yW+fbbb31Xv8+ZM0eSlJubq1//+tfq1KmTmjVrpsGDB/uWHzx4sFwul9+0QH7/+9/73rc2Cxcu1K9+9SvFx8erZcuWOvHEE3XHHXf4HjXdrVs3uVwujR49+pj+7kf68ccfddddd+nkk09WixYtlJCQoKFDh+of//jHMb3Pt99+q1tvvVXdu3dXdHS0WrZsqRNOOEG/+93v9Omnnx719QsWLNDFF1+spKQkeTweRUdH67jjjlNaWpqmTJmiVatW1fr6pUuXKjMzU8cdd5xatmypmJgY9ezZU3fccYe2b99+TH8XIKwYADVaunSpkWQkmfvuu6/Orxs2bJjvdStWrPCbt3nzZt+8559/3lx99dW+36t+Bg0a5Ft+0KBB1aYFct999/leX5Mbb7yx2rqqfjp27GjWrl1runbtaiSZzMzMOv99j/T555+bTp061biuMWPGmBdeeMH3++bNmwO+z9y5c43H46nxfdxut3nooYcCvvbgwYPmsssuq/G1VT99+vQJ+Pp9+/aZ3/72t7W+tlWrVuZf//pXvT8nwEpcAwA0gqFDh+qdd96RJOXl5WnAgAEBl5s5c6Y++eQTpaWlafz48TrxxBNVUlLSKPfFP/LII5o9e7YkKTk5WXfddZf69u2r8vJyvf3225oxY4YyMjK0d+/eBq2nrKxM559/vu/b8RVXXKHMzEwlJCToq6++0owZM/TCCy/os88+q/V93nzzTY0ePVrGGLVu3Vq33Xabhg4dqmbNmumDDz7QtGnT9P333+vuu+9WXFycxo8f7/f6J554wne0YeDAgbruuut0/PHHq1WrVtq1a5c++eQTLV68WKWlpdXWbYxRRkaG3nzzTUnS8OHDdfnll+u4445TRESEVq1apT/96U/asmWLMjIytGLFCvXt27dBnxsQclY3ECCc1fcIwH/+8x/f68aOHes37/AjAJLMNddcYyorK2t8r2AcASgsLDRRUVFGkklNTTXFxcXVllmxYoWJjIz0vUd9jwDcfvvtvvcI9O28oqLC7wiJAhwBqKio8B1BaN26tVm3bl219/n2229NYmKikWRatmxZ7e+UlpZmJJn+/fubAwcO1Jh3165d1aY9/fTTRpJp3ry5WbRoUcDX/fDDD6Z79+5GkjnnnHNqfH8gXHENANAI2rVr5/vzjz/+WONycXFxmjVr1lHP2zfU3LlztX//fkmHjjq0b9++2jIDBgzQhAkTGrSeiooKPffcc5Kk0047TXfddVe1ZZo3b67nnntOzZs3r/F9FixY4DuCcM8996h3797Vlunatav++Mc/SpL27t2rF154wW9+1TUNAwYMULNmNR/sbNu2rd/vxhhNnz5dknTLLbfoV7/6VcDXtWnTxrf+FStWaNOmTTWuAwhHFACgEbRu3dr35927d9e43PDhwxUdHd3oef7zn/9Iktq3b68LLrigxuWuueaaBq1nzZo1vsKTmZlZY7FJSkrSsGHDanyfqrwul0tjx46tcbnLLrtMsbGxfq+pkpiYKOnQRY/ff/99nf8On3/+ub7++mtJUkZGRq3Lnnvuub4/f/jhh3VeBxAOKABAIzh8px8TE1Pjcqeddloo4vjOt/fu3VsRETX/Z9+zZ88GjVtw+FX5/fr1q3XZM888s8Z5VXlTUlJqHUwpMjLSd9vlkdcUZGZmSpLy8/OVmpqqsWPH6uWXX9bWrVtrzfXRRx/5/nz22Wf77qwI9HN40as64gDYBQUAaASHf+M88hDz4dq0aROKOL5v5UcbmdDtdtea92h++OEH358TEhJqXbZDhw5HfZ+jvYckdezYsdq6JWns2LG6++671axZM5WWluqFF17QlVdeqeTkZKWmpuq2227TN998U+39du7cedR1BtLQiyeBUOMuAKARrFu3zvfnk046qcbl3G53KOJYIhjXNTT0PaZOnapx48bppZde0pIlS7Ry5Urt3btXX3/9tWbMmKG//OUv+vOf/6wbbrjB9xqv1+v788KFC9WtW7c6rasuZQUIJxQAoBH8+9//9v154MCBDXqvqkP2lZWVtS63Z8+eGue1adNGRUVFKi4urvU9qh5yVF+HH9HYsWOHTjzxxBqX3bFjR43zqo5C1LZMlapD7zUduejatavuvvtu3X333Tpw4IBWr16tV199VU899ZT279+vG2+8Uf379/edSjj8As64uLiAwz8DTQGnAIAg++yzz7RkyRJJh+63b+j94VUXCR5tx/zVV1/VOK979+6SpI8//rjWIvHpp5+qvLy8HikP6dmzp+/Pq1evrnXZ2uZX7XQ3b95ca2k5cOCA72hLXXbUzZs314ABAzRz5kzNmzdP0qGr/ufPn+9bpqoISIeu7geaKgoAEET79u3TNddcI2OMJOn222+v9Ra0ukhJSZF0aAdf0x0F33//vd9RhyOdd955vuUWLVpU43J//etfG5BU6tOnj+8owIsvvuj7HI60bds230BJgQwdOlTSoZ3zkbf3HW7+/Pm+gXyqXlNXVZ+J5H/NxhlnnKGkpCRJ0tNPP+27fRJoaigAQJB8/vnnGjhwoO8b6aBBg6qNTlcfgwYNknToHvu//OUv1eYfOHBA1113nfbt21fje2RmZsrj8UiSsrOzA94W9+GHHzb44UUej0djxoyRdOhoQ9V98oc7ePCgrr/+elVUVNT4PhdffLE6deok6dB5/EBj/hcUFOj222+XJLVs2dK33ip/+9vfdPDgwRrXcXgBqSpZ0qFTLnfffbck6ZtvvtE111xT61GRsrIyzZo1q8b5QNiydBgiIMwdPhLg+PHjzaeffur7WblypVm0aJGZPn26GTZsmHG5XL5lzzrrrICj7RnjPxLgCy+8cNQM5eXlvvH5IyIizK233mry8vLM6tWrzZw5c8wZZ5xhXC6XOeuss2p9FsBDDz3km9+lSxcze/Zss2rVKpOXl2fuuece06JFC9OtWzcTHx9vJJnRo0fX6zMrKSkxSUlJvnWNHDnSLFq0yKxZs8a8/PLLpl+/fkaS6du3b63PAnjjjTd8n2l0dLT5wx/+YFasWGFWrlxpZsyYYRISEnyvnz17drXXSzIdOnQw48ePNy+++KL54IMPzNq1a82iRYvMxIkTTYsWLXwjDW7ZssXvtZWVleaSSy7xvf/xxx9vHnnkEbNs2TKzbt06s3z5cvPUU0+ZkSNHmlatWpl27drV67MCrEQBAGpxeAGoy098fLyZOnVqrUPPHmsBMMaYvLw806pVqxofiJOTk3PUhwFVVlaa3/3udzVmb9++vVm9erVJTk42kswNN9xQn4/MGGPMZ599Zjp27FjjukaPHl2nhwHNmTOn3g8Dqsu/r9jY2BqH+q2oqDDjx4/3K3Y1/aSkpNT7swKswikAoB4iIiIUGxurLl26KC0tTdnZ2Xrttde0detW373nwTRw4ECtWbNGV199tTp16qTmzZsrMTFRl156qd577z3dcsstR30Pl8ulJ598Uq+//rqGDRumtm3bKioqSqmpqbrlllu0bt069e3bV2VlZZLkG2GvPrp3764NGzZo0qRJOuGEE+TxeNS+fXsNGTJE8+bNq/W8/uEyMzP15ZdfKisrS6eccopatWqlFi1a6Pjjj9f111+vdevWafLkyQFf+9lnn2n69OkaPny4Tj31VLVr105ut1txcXE666yzdN9992njxo01DvXbvHlzzZ49W+vXr9fNN9+snj17KjY2Vm63W7Gxserdu7euvfZazZ8/X1988UW9PyvAKi5jarhKB4DjbN26VcnJyZKkZ599Vtdee63FiQA0Fo4AAPB5+eWXfX8+66yzLEwCoLFxBABwiD179qisrMz3kJwjrVu3ToMGDdLu3bvVp08fvzHxATQ9jAQIOERxcbFOOeUUXXzxxfrVr36lk046SR6PR9u3b9fixYv13HPPad++fXK5XJoxY4bVcQE0Mo4AAA7x7bff+t3vHkhkZKSeeeaZBj8WGED4owAADnHgwAEtWLBAixcv1urVq1VcXKwffvhBLVu2VLdu3TR06FDdfPPN6tq1q9VRAYQABQAAAAfiLgAAAByIAgAAgANRAAAAcCAKAAAADkQBAADAgSgAAAA4EAUAAAAHogAAAOBAFAAAABzo/wGMfuHJsEUFSwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the mean and standard deviation for each column\n",
    "fig, ((ax0)) = plt.subplots(nrows=1, ncols=1, sharey=True, figsize=(6, 6))\n",
    "\n",
    "means1 = np.mean(kd_2N, axis=0)\n",
    "std_devs1 = np.std(kd_2N, axis=0)\n",
    "\n",
    "# means11 = np.mean(fg_2N_ic, axis=0)\n",
    "# std_devs11 = np.std(fg_2N_ic, axis=0)\n",
    "\n",
    "means2 = np.mean(kd_4N, axis=0)\n",
    "std_devs2 = np.std(kd_4N, axis=0)\n",
    "\n",
    "my_xlabels = ['0 nM', '3.125 nM', '6.25 nM', '12.5 nM', '25 nM', '50 nM', '100 nM', '200 nM', '400 nM', '800 nM']\n",
    "    \n",
    "# Generate a plot with error bars\n",
    "x = np.arange(1, 11)  # X-axis points for each column\n",
    "\n",
    "ax0.set_title(r\"$k_d$\",fontsize = 20)\n",
    "ax0.plot(x,  kd_2N, 'ko', label='2N')\n",
    "ax0.plot(x,  kd_4N, 'mo', label='4N')\n",
    "ax0.legend()\n",
    "ax0.set_xticks(x, my_xlabels, rotation ='vertical')\n",
    "ax0.set_xlabel(\"Drug dose\",fontsize = 20)\n",
    "\n",
    "\n",
    "# ax1.set_title(r\"$f_g$\",fontsize = 20)\n",
    "# ax1.plot(x,  fg_2N_ic, 'ko', label='2N, using IC50')\n",
    "# ax1.plot(x,  fg_4N_ic, 'mo', label='4N, using IC50')\n",
    "# ax1.legend()\n",
    "# ax1.set_xticks(x, my_xlabels, rotation ='vertical')\n",
    "# ax1.set_xlabel(\"Drug dose\",fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8e1b6-1d1f-4229-bc91-90c8306c1939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
